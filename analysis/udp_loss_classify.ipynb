{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import portion as P\n",
    "import itertools as it\n",
    "import csv\n",
    "from pprint import pprint\n",
    "from pytictoc import TicToc\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import math\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# ******************************* User Settings *******************************\n",
    "database = \"/home/wmnlab/D/database/\"\n",
    "# date = \"2022-12-20\"\n",
    "dates = [\n",
    "         \"2023-02-04\", \n",
    "         \"2023-02-04#1\",\n",
    "         \"2023-02-04#2\",\n",
    "         ]\n",
    "devices = sorted([\n",
    "    # \"sm00\",\n",
    "    # \"sm01\",\n",
    "    # \"sm02\",\n",
    "    # \"sm03\",\n",
    "    # \"sm04\",\n",
    "    # \"sm05\",\n",
    "    # \"sm06\",\n",
    "    # \"sm07\",\n",
    "    # \"sm08\",\n",
    "    # \"qc00\",\n",
    "    \"qc01\",\n",
    "    \"qc02\",\n",
    "    \"qc03\",\n",
    "])\n",
    "exps = {  # experiment_name: (number_of_experiment_rounds, list_of_experiment_round)\n",
    "            # If the list is None, it will not list as directories.\n",
    "            # If the list is empty, it will list all directories in the current directory by default.\n",
    "            # If the number of experiment times != the length of existing directories of list, it would trigger warning and skip the directory.\n",
    "    \"_Bandlock_Udp_B3_B7_B8_RM500Q\": (2, []),\n",
    "    \"_Bandlock_Udp_all_RM500Q\": (2, []),\n",
    "    # \"tsync\": (1, None),\n",
    "    # \"_Bandlock_Udp\": (4, [\"#01\", \"#02\", \"#03\", \"#04\"]),\n",
    "    # \"_Bandlock_Udp\": (4, [\"#03\", \"#04\", \"#05\", \"#06\"]),\n",
    "    # \"_Bandlock_Udp\": (4, []),\n",
    "    # \"_Bandlock_Udp\": (6, []),\n",
    "    # \"_Bandlock_Udp_B1_B3\":  (6, []),\n",
    "    # \"_Bandlock_Udp_B3_B28\": (4, []),\n",
    "    # \"_Bandlock_Udp_B28_B1\": (4, []),\n",
    "    # \"_Bandlock_Udp_B1_B3\":  (4, []),\n",
    "    # \"_Bandlock_Udp_B3_B7\":  (4, []),\n",
    "    # \"_Bandlock_Udp_B7_B8\":  (4, []),\n",
    "    # \"_Bandlock_Udp_B8_B1\":  (4, []),\n",
    "}\n",
    "\n",
    "class Payload:\n",
    "    LENGTH = 250              # (Bytes)\n",
    "    TAG = \"000425d401df5e76\"  # 2 71828 3 1415926 (hex)            : 8-bytes\n",
    "    OFS_TIME = (16, 24)       # epoch time of 'yyyy/mm/dd hh:mm:ss': 4-bytes\n",
    "    OFS_USEC = (24, 32)       # microsecond (usec)                 : 4-bytes\n",
    "    OFS_SEQN = (32, 40)       # sequence number (start from 1)     : 4-bytes\n",
    "class ServerIP:\n",
    "    PUBLIC = \"140.112.20.183\"  # 2F    \n",
    "    PRIVATE = \"192.168.1.251\"  # 2F\n",
    "    # PRIVATE = \"192.168.1.248\"  # 2F previous\n",
    "\n",
    "# DATA_RATE = 1000e3  # bits-per-second\n",
    "# PKT_RATE = DATA_RATE / Payload.LENGTH / 8  # packets-per-second\n",
    "# print(\"packet_rate (pps):\", PKT_RATE, \"\\n\")\n",
    "# *****************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****************************** Utils Functions ******************************\n",
    "def makedir(dirpath, mode=0):  # mode=1: show message; mode=0: hide message\n",
    "    if os.path.isdir(dirpath):\n",
    "        if mode:\n",
    "            print(\"mkdir: cannot create directory '{}': directory has already existed.\".format(dirpath))\n",
    "        return\n",
    "    ### recursively make directory\n",
    "    _temp = []\n",
    "    while not os.path.isdir(dirpath):\n",
    "        _temp.append(dirpath)\n",
    "        dirpath = os.path.dirname(dirpath)\n",
    "    while _temp:\n",
    "        dirpath = _temp.pop()\n",
    "        print(\"mkdir\", dirpath)\n",
    "        os.mkdir(dirpath)\n",
    "\n",
    "def interp(x, y, ratio):\n",
    "    \"\"\"\n",
    "    Interpolation\n",
    "\n",
    "    Args:\n",
    "        x, y (datetime.datetime): x < y\n",
    "        ratio (float): a decimal numeral in a range [0, 1]; 0 means break at x, 1 means break at y.\n",
    "    Returns:\n",
    "        (datetime.datetime): breakpoint of interpolation\n",
    "    \"\"\"\n",
    "    return x + (y - x) * ratio\n",
    "\n",
    "def is_disjoint(set1, set2):\n",
    "    \"\"\"\n",
    "    Check if two sets are disjoint.\n",
    "    \"\"\"\n",
    "    return (set1 & set2).empty\n",
    "\n",
    "def pairwise_disjoint(set_list):\n",
    "    \"\"\"\n",
    "    Check if all sets in the list are pairwise disjoint.\n",
    "    \"\"\"\n",
    "    pair_list = list(it.combinations(set_list, 2))\n",
    "    for item in pair_list:\n",
    "        if not is_disjoint(item[0], item[1]):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def union_all(set_list):\n",
    "    \"\"\"\n",
    "    Union all sets in the list.\n",
    "    \"\"\"\n",
    "    _temp = P.empty()\n",
    "    for _set in set_list:\n",
    "        _temp = _temp | _set\n",
    "    return _temp\n",
    "\n",
    "def get_length(intervals):\n",
    "    \"\"\"\n",
    "    Get total length of a set of intervals.\n",
    "    \"\"\"\n",
    "    if intervals.empty:\n",
    "        return 0\n",
    "    sum = 0\n",
    "    for s in intervals:\n",
    "        sum += (s.upper - s.lower) / dt.timedelta(seconds=1)\n",
    "    return round(sum, 3)\n",
    "# *****************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handover_types_0   = \"lte_handover,SN_addition,SN_removal,endc_SN_change,endc_MN_change,endc_MNSN_change,lte2endc_MN_change,endc2lte_MN_change\".split(',')\n",
    "link_failure_types = \"scg_failure,radio_link_failure,nas_recovery\".split(',')\n",
    "handover_types_1   = \"SN_change_only,Intra_frequency,Inter_frequency\".split(',')\n",
    "handover_types_2   = \"SN_change_only,Intra_sector,Intra_eNB,Inter_eNB\".split(',')\n",
    "# print(handover_types_0)\n",
    "# print(link_failure_types)\n",
    "# print(handover_types_1)\n",
    "# print(handover_types_2)\n",
    "\n",
    "handover_types = [ *handover_types_0, *link_failure_types ]\n",
    "\n",
    "# ****************************** Classify Function ******************************\n",
    "def get_loss_classify(hodf, lodf, rxdf, types=handover_types, sstime=(dt.datetime.min, dt.datetime.max), secs=(1, 3), ratio=0.5):\n",
    "    \"\"\"\n",
    "    Get intervals of each event type.\n",
    "\n",
    "    Args:\n",
    "        hodf (pandas.Dataframe): dataframe import from \"./data/diag_log_ho-info.csv\"\n",
    "        rxdf (pandas.Dataframe): dataframe import from \"./data/udp_dnlk_latency.csv\" or \"./data/udp_uplk_latency.csv\"\n",
    "        lodf (pandas.Dataframe): dataframe import from \"./data/udp_dnlk_loss_timestamp.csv\" or \"./data/udp_uplk_loss_timestamp.csv\"\n",
    "        types (list): names of event types.\n",
    "        setime (tuple): (start time, end time) of an experiment.\n",
    "        secs (tuple): size of window to specify (in seconds); the former is for successful ho, the latter is for failed ho.\n",
    "        ratio (float): a decimal numeral in a range [0, 1], to decide the breakpoint of interpolation when overlap occurs; set ratio to None if no need for overlap checking.\n",
    "\n",
    "    Returns:\n",
    "        before_event_intervals (portion.interval.Interval)\n",
    "        during_events_intervals (portion.interval.Interval)\n",
    "        after_events_intervals (portion.interval.Interval)\n",
    "    \"\"\"\n",
    "    def trim(timestamp):\n",
    "        \"\"\"\n",
    "        Avoid an interval exceeds the boundary of experiment stard/end time.\n",
    "        \"\"\"\n",
    "        return min(max(timestamp, sstime[0]), sstime[1])\n",
    "    \n",
    "    column_names = []\n",
    "    for type_name in types:\n",
    "        column_names += [\"before_{}\".format(type_name), \"during_{}\".format(type_name), \"after_{}\".format(type_name)]\n",
    "    column_names += [\"unstable\", \"stable\", \"overall\"]\n",
    "\n",
    "    event_intvl = dict.fromkeys(column_names, P.empty())\n",
    "    event_occur = dict.fromkeys(column_names, 0)\n",
    "    event_dur = dict.fromkeys(column_names, \"\")\n",
    "    loss_num = dict.fromkeys(column_names, \"\")\n",
    "    pkt_num = dict.fromkeys(column_names, \"\")\n",
    "    t_loss_num = dict.fromkeys(column_names, 0)\n",
    "    t_pkt_num = dict.fromkeys(column_names, 0)\n",
    "\n",
    "    j, k = 0, 0\n",
    "    lots = lodf[\"Timestamp\"].array\n",
    "    rxts = rxdf[\"Timestamp\"].array\n",
    "    anomaly_check = []\n",
    "    overlaps_num = 0\n",
    "    included_num = 0\n",
    "\n",
    "    print(secs[0], secs[1], \"seconds\") if secs[0] != secs[1] else print(secs[0], \"seconds\")\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    for i in range(len(hodf)):\n",
    "        _now = hodf.iloc[i]\n",
    "        _prior = hodf.iloc[i-1] if i != 0 else None\n",
    "        _post = hodf.iloc[i+1] if i != len(hodf)-1 else None\n",
    "        if i != 0 and _now[\"handoff_state\"] == \"trigger\" and _prior[\"handoff_state\"] == \"start\":\n",
    "            print(i, _now[\"Timestamp\"], \"'{}' is included during '{}'.\".format(_now[\"handoff_type\"], _prior[\"handoff_type\"]))\n",
    "            included_num += 1\n",
    "            anomaly_check.append((_now[\"Timestamp\"], _now[\"handoff_type\"], _prior[\"handoff_type\"]))\n",
    "            continue\n",
    "        if _now[\"handoff_state\"] != \"end\":  # \"start\" or \"trigger\"\n",
    "            ### prior\n",
    "            _start = _now\n",
    "            C = _start[\"Timestamp\"] - dt.timedelta(seconds=secs[0]) if _start[\"handoff_state\"] == \"start\" else _start[\"Timestamp\"] - dt.timedelta(seconds=secs[1])\n",
    "            D = _start[\"Timestamp\"]\n",
    "            prior_interval = P.closedopen(trim(C), trim(D))\n",
    "            if ratio != None and i != 0:\n",
    "                A = _prior[\"Timestamp\"]\n",
    "                B = _prior[\"Timestamp\"] + dt.timedelta(seconds=secs[0]) if _prior[\"handoff_state\"] == \"end\" else _prior[\"Timestamp\"] + dt.timedelta(seconds=secs[1])\n",
    "                if P.openclosed(A, B).overlaps(prior_interval):\n",
    "                    print(i, _now[\"Timestamp\"], \"'{}' overlaps with the previous '{}' for {} seconds.\".format(_start[\"handoff_type\"], _prior[\"handoff_type\"], get_length(P.openclosed(A, B) & prior_interval)))\n",
    "                    overlaps_num += 1\n",
    "                    # print(get_length(P.openclosed(A, B)))\n",
    "                    bkp = interp(C, B, ratio)\n",
    "                    bkp = max(bkp, A)  # to avoid the breakpoint overlaps the previous event's duration\n",
    "                    # bkp = min(max(bkp, A), D)  # 我不侵犯到其他任何人，代表其他人也不會侵犯到我！\n",
    "                    ## blindly set as open inverval is fine, but may sometimes miss one point.\n",
    "                    prior_interval = P.closedopen(trim(bkp), trim(D))\n",
    "                    if A in prior_interval:\n",
    "                        prior_interval = P.open(trim(bkp), trim(D))\n",
    "        if _now[\"handoff_state\"] != \"start\":  # \"end\" or \"trigger\"\n",
    "            ### middle\n",
    "            middle_interval = P.closed(trim(_start[\"Timestamp\"]), trim(_now[\"Timestamp\"]))\n",
    "            ### post\n",
    "            C = _now[\"Timestamp\"]\n",
    "            D = _now[\"Timestamp\"] + dt.timedelta(seconds=secs[0]) if _now[\"handoff_state\"] == \"end\" else _now[\"Timestamp\"] + dt.timedelta(seconds=secs[1])\n",
    "            post_interval = P.openclosed(trim(C), trim(D))\n",
    "            if ratio != None and i != len(hodf)-1:\n",
    "                A = _post[\"Timestamp\"] - dt.timedelta(seconds=secs[0]) if _post[\"handoff_state\"] == \"start\" else _post[\"Timestamp\"] - dt.timedelta(seconds=secs[1])\n",
    "                B = _post[\"Timestamp\"]\n",
    "                if P.closedopen(A, B).overlaps(post_interval):\n",
    "                    print(i, _now[\"Timestamp\"], \"'{}' overlaps with the following '{}' for {} seconds.\".format(_now[\"handoff_type\"], _post[\"handoff_type\"], get_length(P.closedopen(A, B) & post_interval)))\n",
    "                    overlaps_num += 1\n",
    "                    # print(get_length(P.closedopen(A, B)))\n",
    "                    bkp = interp(A, D, ratio)\n",
    "                    bkp = min(bkp, B)  # to avoid the breakpoint overlaps the following event's duration\n",
    "                    # bkp = max(min(bkp, B), C)  # 我不侵犯到其他任何人，代表其他人也不會侵犯到我！\n",
    "                    post_interval = P.open(trim(C), trim(bkp))\n",
    "            \n",
    "            type_name = _now[\"handoff_type\"]\n",
    "            ## before_event_intervals\n",
    "            event_name = \"before_{}\".format(type_name)\n",
    "            event_intvl[event_name] = event_intvl[event_name] | prior_interval\n",
    "            ## during_event_intervals\n",
    "            event_name = \"during_{}\".format(type_name)\n",
    "            event_intvl[event_name] = event_intvl[event_name] | middle_interval\n",
    "            ## after_event_intervals\n",
    "            event_name = \"after_{}\".format(type_name)\n",
    "            event_intvl[event_name] = event_intvl[event_name] | post_interval\n",
    "\n",
    "            _lotmp = dict.fromkeys([\"before\", \"during\", \"after\"], 0)\n",
    "            _rxtmp = dict.fromkeys([\"before\", \"during\", \"after\"], 0)\n",
    "            while j < len(lots):\n",
    "                if lots[j] in prior_interval:\n",
    "                    _lotmp[\"before\"] += 1\n",
    "                elif lots[j] in middle_interval:\n",
    "                    _lotmp[\"during\"] += 1\n",
    "                elif lots[j] in post_interval:\n",
    "                    _lotmp[\"after\"] += 1\n",
    "                elif lots[j] >= max(prior_interval.upper, middle_interval.upper, post_interval.upper):\n",
    "                    break\n",
    "                j += 1\n",
    "            while k < len(rxts):\n",
    "                if rxts[k] in prior_interval:\n",
    "                    _rxtmp[\"before\"] += 1\n",
    "                elif rxts[k] in middle_interval:\n",
    "                    _rxtmp[\"during\"] += 1\n",
    "                elif rxts[k] in post_interval:\n",
    "                    _rxtmp[\"after\"] += 1\n",
    "                elif rxts[k] >= max(prior_interval.upper, middle_interval.upper, post_interval.upper):\n",
    "                    break\n",
    "                k += 1\n",
    "\n",
    "            for prefix in [\"before\", \"during\", \"after\"]:\n",
    "                event_name = \"{}_{}\".format(prefix, type_name)\n",
    "                event_occur[event_name] += 1\n",
    "                loss_num[event_name] = '@'.join([loss_num[event_name], str(_lotmp[prefix])]) if loss_num[event_name] else str(_lotmp[prefix])\n",
    "                pkt_num[event_name] = '@'.join([pkt_num[event_name], str(_lotmp[prefix]+_rxtmp[prefix])]) if pkt_num[event_name] else str(_lotmp[prefix]+_rxtmp[prefix])\n",
    "                t_loss_num[event_name] += _lotmp[prefix]\n",
    "                t_pkt_num[event_name] += (_lotmp[prefix]+_rxtmp[prefix])\n",
    "            \n",
    "            event_dur[\"before_{}\".format(type_name)] = '@'.join([event_dur[\"before_{}\".format(type_name)], str(get_length(prior_interval))]) if event_dur[\"before_{}\".format(type_name)] else str(get_length(prior_interval))\n",
    "            event_dur[\"during_{}\".format(type_name)] = '@'.join([event_dur[\"during_{}\".format(type_name)], str(get_length(middle_interval))]) if event_dur[\"during_{}\".format(type_name)] else str(get_length(middle_interval))\n",
    "            event_dur[\"after_{}\".format(type_name)] = '@'.join([event_dur[\"after_{}\".format(type_name)], str(get_length(post_interval))]) if event_dur[\"after_{}\".format(type_name)] else str(get_length(post_interval))\n",
    "    \n",
    "    t_duration = dict(zip(column_names, [get_length(item) for item in event_intvl.values()]))\n",
    "\n",
    "    t_ho_num = sum(hodf[\"handoff_state\"] != \"end\")\n",
    "    overlaps_num = overlaps_num // 2\n",
    "    overlaps_ratio = round(overlaps_num / (t_ho_num + 1e-9) * 100, 2)\n",
    "    print(overlaps_num, included_num, t_ho_num)\n",
    "    print(\"overlaps ratio: {}(%)\".format(overlaps_ratio))\n",
    "\n",
    "    ## overall\n",
    "    t_duration[\"overall\"] = (sstime[1] - sstime[0]).total_seconds() if sstime[1] > sstime[0] else 0\n",
    "    t_loss_num[\"overall\"] = len(lots)\n",
    "    t_pkt_num[\"overall\"] = len(lots) + len(rxts)\n",
    "    ## unstable\n",
    "    t_duration[\"unstable\"] = sum([t_duration[key] for key in column_names[:-3]])\n",
    "    t_loss_num[\"unstable\"] = sum([t_loss_num[key] for key in column_names[:-3]])\n",
    "    t_pkt_num[\"unstable\"] = sum([t_pkt_num[key] for key in column_names[:-3]])\n",
    "    ## stable\n",
    "    t_duration[\"stable\"] = t_duration[\"overall\"] - t_duration[\"unstable\"]\n",
    "    t_loss_num[\"stable\"] = t_loss_num[\"overall\"] - t_loss_num[\"unstable\"]\n",
    "    t_pkt_num[\"stable\"] = t_pkt_num[\"overall\"] - t_pkt_num[\"unstable\"] \n",
    "    \n",
    "    lodict = {  \"event_occur\" : event_occur,\n",
    "                \"event_dur\" : event_dur,\n",
    "                \"loss_num\" : loss_num,\n",
    "                \"pkt_num\" : pkt_num,\n",
    "                \"t_duration\" : t_duration,\n",
    "                \"t_loss_num\" : t_loss_num,\n",
    "                \"t_pkt_num\" : t_pkt_num,\n",
    "                # \"event_intvl\" : event_intvl,\n",
    "            }\n",
    "    \n",
    "    # return event_intvl, event_occur, event_dur, loss_num, pkt_num, t_duration, t_loss_num, t_pkt_num, anomaly_check\n",
    "    return lodict, event_intvl, anomaly_check\n",
    "# *****************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hodf = pd.read_csv(\"/home/wmnlab/D/database/2022-12-20/_Bandlock_Udp_B1_B3/sm05/#01/data/diag_log_ho-info.csv\")\n",
    "lodf = pd.read_csv(\"/home/wmnlab/D/database/2022-12-20/_Bandlock_Udp_B1_B3/sm05/#01/data/udp_dnlk_loss_timestamp.csv\")\n",
    "rxdf = pd.read_csv(\"/home/wmnlab/D/database/2022-12-20/_Bandlock_Udp_B1_B3/sm05/#01/data/udp_dnlk_latency.csv\")\n",
    "hodf[\"Timestamp\"] = pd.to_datetime(hodf[\"Timestamp\"])\n",
    "lodf[\"Timestamp\"] = pd.to_datetime(lodf[\"Timestamp\"])\n",
    "rxdf[\"Timestamp\"] = pd.to_datetime(rxdf[\"Timestamp\"])\n",
    "\n",
    "# event_intvl, event_occur, event_dur, loss_num, pkt_num, t_duration, t_loss_num, t_pkt_num, anomaly_check = get_loss_classify(hodf, lodf, rxdf, secs=(1, 3))\n",
    "# print(\"event_intvl\")\n",
    "# pprint(event_intvl, sort_dicts=False)\n",
    "# print(\"event_occur\")\n",
    "# pprint(event_occur, sort_dicts=False)\n",
    "# print(\"event_dur\")\n",
    "# pprint(event_dur, sort_dicts=False)\n",
    "# print(\"loss_num\")\n",
    "# pprint(loss_num, sort_dicts=False)\n",
    "# print(\"pkt_num\")\n",
    "# pprint(pkt_num, sort_dicts=False)\n",
    "# print(\"t_duration\")\n",
    "# pprint(t_duration, sort_dicts=False)\n",
    "# print(\"t_loss_num\")\n",
    "# pprint(t_loss_num, sort_dicts=False)\n",
    "# print(\"t_pkt_num\")\n",
    "# pprint(t_pkt_num, sort_dicts=False)\n",
    "\n",
    "lodict, event_intvl, anomaly_check = get_loss_classify(hodf, lodf, rxdf, secs=(1, 3))\n",
    "# pprint(lodict, sort_dicts=False)\n",
    "df = pd.DataFrame.from_dict(lodict)\n",
    "# print(df)\n",
    "\n",
    "dirpath = \"/home/wmnlab/D/database/2022-12-20/_Bandlock_Udp_B1_B3/sm05/#01/statistics/classify-bilateral/dnlk-loss\"\n",
    "makedir(dirpath)\n",
    "filename = \"dnlk_loss_classify_1_3.csv\"\n",
    "filepath = os.path.join(dirpath, filename)\n",
    "df.to_csv(filepath)\n",
    "\n",
    "df = pd.read_csv(filepath, index_col=0)\n",
    "# print(df)\n",
    "_lodict = df.to_dict()\n",
    "pprint(_lodict, sort_dicts=False)\n",
    "\n",
    "# ss = [0] + [i/10 for i in range(1, 10)] + list(range(1, 11))\n",
    "# for s in ss:\n",
    "#     _, _, _, _, _, _, _, _, _ = get_loss_classify(hodf, lodf, rxdf, secs=(s, s))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************** Auxiliary Functions ****************************\n",
    "def get_sst(df_dlloss, df_dlrecv, df_ulloss, df_ulrecv):\n",
    "    dlloss_0 = df_dlloss[\"Timestamp\"].iloc[0] if len(df_dlloss) else pd.Timestamp.max\n",
    "    dlrecv_0 = df_dlrecv[\"Timestamp\"].iloc[0] if len(df_dlrecv) else pd.Timestamp.max\n",
    "    dlstart = min(dlloss_0, dlrecv_0)\n",
    "    ulloss_0 = df_ulloss[\"Timestamp\"].iloc[0] if len(df_ulloss) else pd.Timestamp.max\n",
    "    ulrecv_0 = df_ulrecv[\"Timestamp\"].iloc[0] if len(df_ulrecv) else pd.Timestamp.max\n",
    "    ulstart = min(ulloss_0, ulrecv_0)\n",
    "    start = max(dlstart, ulstart)\n",
    "\n",
    "    dlloss_l = df_dlloss[\"Timestamp\"].iloc[-1] if len(df_dlloss) else pd.Timestamp.min\n",
    "    dlrecv_l = df_dlrecv[\"Timestamp\"].iloc[-1] if len(df_dlrecv) else pd.Timestamp.min\n",
    "    dlstop = max(dlloss_l, dlrecv_l)\n",
    "    ulloss_l = df_ulloss[\"Timestamp\"].iloc[-1] if len(df_ulloss) else pd.Timestamp.min\n",
    "    ulrecv_l = df_ulrecv[\"Timestamp\"].iloc[-1] if len(df_ulrecv) else pd.Timestamp.min\n",
    "    ulstop = max(ulloss_l, ulrecv_l)\n",
    "    stop = min(dlstop, ulstop)\n",
    "    \n",
    "    if start == pd.Timestamp.max or stop == pd.Timestamp.min:\n",
    "        expr_time = 0\n",
    "    else:\n",
    "        expr_time = (stop - start).total_seconds()\n",
    "    return start, stop, expr_time\n",
    "\n",
    "def remove_head_tail(hodf, start_time, stop_time):\n",
    "    if len(hodf):\n",
    "        # print(dt.datetime.max)\n",
    "        # print(dt.datetime.min)\n",
    "        # print(pd.Timestamp.max)\n",
    "        # print(pd.Timestamp.min)\n",
    "        start_indices = hodf.index[hodf['Timestamp'] >= start_time]\n",
    "        end_indices = hodf.index[hodf['Timestamp'] <= stop_time]\n",
    "        if len(start_indices) and len(end_indices):\n",
    "            start_index = start_indices[0]\n",
    "            end_index = end_indices[-1]\n",
    "        else:\n",
    "            hodf = hodf.iloc[0:0]\n",
    "        try:\n",
    "            if hodf.loc[start_index, 'handoff_state'] == 'end':\n",
    "                hodf.loc[start_index - 1, 'Timestamp'] = start_time\n",
    "                start_index -= 1\n",
    "            if hodf.loc[end_index, 'handoff_state'] == 'start':\n",
    "                hodf.loc[end_index + 1, 'Timestamp'] = stop_time\n",
    "                end_index += 1\n",
    "            hodf = hodf.iloc[start_index : end_index + 1]\n",
    "        except:\n",
    "            pass\n",
    "        hodf = hodf.reset_index(drop=True)\n",
    "    return hodf\n",
    "\n",
    "def handoff_statistics(hodf, expr_time, fout1, fout2):\n",
    "    hodf.to_csv(fout2, index=False)\n",
    "    hodf = hodf[hodf['handoff_state'] != 'end']\n",
    "    event_names = \"lte_handover,SN_addition,SN_removal,endc_SN_change,endc_MN_change,endc_MNSN_change,lte2endc_MN_change,endc2lte_MN_change,scg_failure,radio_link_failure,nas_recovery\".split(',')\n",
    "    ss = [0] * len(event_names)\n",
    "    for i, item in enumerate(event_names):\n",
    "        ss[i] = sum(hodf['handoff_type'] == item)\n",
    "    ss.append(sum(ss[:8]))\n",
    "    ss.append(sum(ss[8:11]))\n",
    "    ss.append(sum(ss[:11]))\n",
    "    ss.append(expr_time)\n",
    "    event_names += [\"succ_handoff\", \"fail_handoff\", \"overall_handoff\", \"experiment_time(sec)\"]\n",
    "    with open(fout1, \"w\", newline='') as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow(event_names)\n",
    "        writer.writerow(ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************** Main Function ****************************\n",
    "def main():\n",
    "    df_hndoff = pd.read_csv(os.path.join(source_dir, \"diag_log_ho-info.csv\"))\n",
    "    # df_hndoff = pd.read_csv(os.path.join(source_dir, \"diag_log_ho-info_new.csv\"))\n",
    "    # df_dlloss = pd.read_csv(os.path.join(source_dir, \"udp_dnlk_loss_timestamp.csv\"))\n",
    "    # df_dlrecv = pd.read_csv(os.path.join(source_dir, \"udp_dnlk_latency.csv\"))\n",
    "    # df_ulloss = pd.read_csv(os.path.join(source_dir, \"udp_uplk_loss_timestamp.csv\"))\n",
    "    # df_ulrecv = pd.read_csv(os.path.join(source_dir, \"udp_uplk_latency.csv\"))\n",
    "    \n",
    "    df_dlloss = pd.read_csv(os.path.join(source_dir, \"udp_dnlk_loss_latency.csv\"))\n",
    "    df_dlloss = df_dlloss[df_dlloss['lost'] == True]\n",
    "    df_dlrecv = pd.read_csv(os.path.join(source_dir, \"udp_dnlk_loss_latency.csv\"))\n",
    "    df_dlrecv = df_dlrecv[df_dlrecv['lost'] == False]\n",
    "    df_ulloss = pd.read_csv(os.path.join(source_dir, \"udp_uplk_loss_latency.csv\"))\n",
    "    df_ulloss = df_ulloss[df_ulloss['lost'] == True]\n",
    "    df_ulrecv = pd.read_csv(os.path.join(source_dir, \"udp_uplk_loss_latency.csv\"))\n",
    "    df_ulrecv = df_ulrecv[df_ulrecv['lost'] == False]\n",
    "\n",
    "    df_hndoff[\"Timestamp\"] = pd.to_datetime(df_hndoff[\"Timestamp\"])\n",
    "    df_dlloss[\"Timestamp\"] = pd.to_datetime(df_dlloss[\"Timestamp\"])\n",
    "    df_dlrecv[\"Timestamp\"] = pd.to_datetime(df_dlrecv[\"Timestamp\"])\n",
    "    df_ulloss[\"Timestamp\"] = pd.to_datetime(df_ulloss[\"Timestamp\"])\n",
    "    df_ulrecv[\"Timestamp\"] = pd.to_datetime(df_ulrecv[\"Timestamp\"])\n",
    "\n",
    "    start, stop, expr_time = get_sst(df_dlloss, df_dlrecv, df_ulloss, df_ulrecv)\n",
    "    print(start, stop , expr_time)\n",
    "    df_hndoff = remove_head_tail(df_hndoff, start, stop)\n",
    "    fout1 = os.path.join(target_dir, \"diag_log_ho-statistics_new.csv\")\n",
    "    fout2 = os.path.join(source_dir, \"diag_log_ho-info_new.csv\")\n",
    "    handoff_statistics(df_hndoff, expr_time, fout1, fout2)\n",
    "\n",
    "    ss = [0] + [i/10 for i in range(1, 10)] + list(range(1, 11))\n",
    "\n",
    "    dirpath = os.path.join(target_dir, \"classify-bilateral\", \"uplk-loss\")\n",
    "    makedir(dirpath)\n",
    "    # for s in ss:\n",
    "    #     fout = os.path.join(dirpath, \"uplk_loss_classify_{}.csv\".format(s))\n",
    "    #     lodict, event_intvl, anomaly_check = get_loss_classify(df_hndoff, df_ulloss, df_ulrecv, secs=(s, s), sstime=(start, stop))\n",
    "    #     df = pd.DataFrame.from_dict(lodict)\n",
    "    #     df.to_csv(fout)\n",
    "    fout = os.path.join(dirpath, \"uplk_loss_classify_1_3.csv\")\n",
    "    lodict, event_intvl, anomaly_check = get_loss_classify(df_hndoff, df_ulloss, df_ulrecv, secs=(1, 3), sstime=(start, stop))\n",
    "    df = pd.DataFrame.from_dict(lodict)\n",
    "    df.to_csv(fout)\n",
    "    # fout = os.path.join(dirpath, \"uplk_loss_classify_1.csv\")\n",
    "    # lodict, event_intvl, anomaly_check = get_loss_classify(df_hndoff, df_ulloss, df_ulrecv, secs=(1, 1), sstime=(start, stop))\n",
    "    # df = pd.DataFrame.from_dict(lodict)\n",
    "    # df.to_csv(fout)\n",
    "\n",
    "    dirpath = os.path.join(target_dir, \"classify-bilateral\", \"dnlk-loss\")\n",
    "    makedir(dirpath)\n",
    "    # for s in ss:\n",
    "    #     fout = os.path.join(dirpath, \"dnlk_loss_classify_{}.csv\".format(s))\n",
    "    #     lodict, event_intvl, anomaly_check = get_loss_classify(df_hndoff, df_dlloss, df_dlrecv, secs=(s, s), sstime=(start, stop))\n",
    "    #     df = pd.DataFrame.from_dict(lodict)\n",
    "    #     df.to_csv(fout)\n",
    "    fout = os.path.join(dirpath, \"dnlk_loss_classify_1_3.csv\")\n",
    "    lodict, event_intvl, anomaly_check = get_loss_classify(df_hndoff, df_dlloss, df_dlrecv, secs=(1, 3), sstime=(start, stop))\n",
    "    df = pd.DataFrame.from_dict(lodict)\n",
    "    df.to_csv(fout)\n",
    "    # fout = os.path.join(dirpath, \"dnlk_loss_classify_1.csv\")\n",
    "    # lodict, event_intvl, anomaly_check = get_loss_classify(df_hndoff, df_dlloss, df_dlrecv, secs=(1, 1), sstime=(start, stop))\n",
    "    # df = pd.DataFrame.from_dict(lodict)\n",
    "    # df.to_csv(fout)\n",
    "\n",
    "    # fout = os.path.join(target_dir, \"classify\", \"uplk_loss_classify_type1.csv\")\n",
    "    # loss_classify_type1(df_hndoff.copy(), df_ulloss.copy(), df_ulrecv.copy(), fout, start, stop, overlap=False, ratio=0.5)\n",
    "    # fout = os.path.join(target_dir, \"classify\", \"dnlk_loss_classify_type1.csv\")\n",
    "    # loss_classify_type1(df_hndoff.copy(), df_dlloss.copy(), df_dlrecv.copy(), fout, start, stop, overlap=False, ratio=0.5)\n",
    "\n",
    "    # fout = os.path.join(target_dir, \"classify\", \"uplk_loss_classify_type2.csv\")\n",
    "    # loss_classify_type2(df_hndoff.copy(), df_ulloss.copy(), df_ulrecv.copy(), fout, start, stop, overlap=False, ratio=0.5)\n",
    "    # fout = os.path.join(target_dir, \"classify\", \"dnlk_loss_classify_type2.csv\")\n",
    "    # loss_classify_type2(df_hndoff.copy(), df_dlloss.copy(), df_dlrecv.copy(), fout, start, stop, overlap=False, ratio=0.5)\n",
    "# *****************************************************************************\n",
    "\n",
    "# ******************************* Check Files *********************************\n",
    "for date in dates:\n",
    "    for expr, (times, traces) in exps.items():\n",
    "        print(os.path.join(database, date, expr))\n",
    "        for dev in devices:\n",
    "            if not os.path.isdir(os.path.join(database, date, expr, dev)):\n",
    "                print(\"|___ {} does not exist.\".format(os.path.join(database, date, expr, dev)))\n",
    "                continue\n",
    "            \n",
    "            print(\"|___\", os.path.join(database, date, expr, dev))\n",
    "            if traces == None:\n",
    "                # print(os.path.join(database, date, expr, dev))\n",
    "                continue\n",
    "            elif len(traces) == 0:\n",
    "                traces = sorted(os.listdir(os.path.join(database, date, expr, dev)))\n",
    "            \n",
    "            print(\"|    \", times)\n",
    "            traces = [trace for trace in traces if os.path.isdir(os.path.join(database, date, expr, dev, trace))]\n",
    "            if len(traces) != times:\n",
    "                print(\"***************************************************************************************\")\n",
    "                print(\"Warning: the number of traces does not match the specified number of experiment times.\")\n",
    "                print(\"***************************************************************************************\")\n",
    "            for trace in traces:\n",
    "                print(\"|    |___\", os.path.join(database, date, expr, dev, trace))\n",
    "        print()\n",
    "# *****************************************************************************\n",
    "\n",
    "# ******************************** Processing *********************************\n",
    "t = TicToc()  # create instance of class\n",
    "t.tic()       # Start timer\n",
    "err_handles = []\n",
    "for date in dates:\n",
    "    for expr, (times, traces) in exps.items():\n",
    "        for dev in devices:\n",
    "            if not os.path.isdir(os.path.join(database, date, expr, dev)):\n",
    "                print(\"{} does not exist.\\n\".format(os.path.join(database, date, expr, dev)))\n",
    "                continue\n",
    "\n",
    "            if traces == None:\n",
    "                print(\"------------------------------------------\")\n",
    "                print(date, expr, dev)\n",
    "                print(\"------------------------------------------\")\n",
    "                source_dir = os.path.join(database, date, expr, dev)\n",
    "                target_dir = os.path.join(database, date, expr, dev)\n",
    "                makedir(target_dir)\n",
    "                filenames = os.listdir(source_dir)\n",
    "                main()\n",
    "                continue\n",
    "            elif len(traces) == 0:\n",
    "                traces = sorted(os.listdir(os.path.join(database, date, expr, dev)))\n",
    "            \n",
    "            traces = [trace for trace in traces if os.path.isdir(os.path.join(database, date, expr, dev, trace))]\n",
    "            print(traces)\n",
    "            for trace in traces:\n",
    "                print(\"------------------------------------------\")\n",
    "                print(date, expr, dev, trace)\n",
    "                print(\"------------------------------------------\")\n",
    "                source_dir = os.path.join(database, date, expr, dev, trace, \"data\")\n",
    "                target_dir = os.path.join(database, date, expr, dev, trace, \"statistics\")\n",
    "                makedir(target_dir)\n",
    "                main()\n",
    "t.toc()  # Time elapsed since t.tic()\n",
    "# *****************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figdir = os.path.join(\"/home/wmnlab/Desktop/Jackbedford/wmnl-handoff-research/analysis/2022-12-27/figure\", \"4\")\n",
    "makedir(os.path.join(figdir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHT 8+3 types: UL Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = \"/home/wmnlab/D/database/\"\n",
    "exps = { \"2022-12-20\": \n",
    "            {  # experiment_name: (number_of_experiment_rounds, list_of_experiment_round)\n",
    "                # \"_Bandlock_Udp_B1_B3\":  (1, [\"#01\",]),\n",
    "                # \"_Bandlock_Udp_B1_B3\":  (2, [\"#01\", \"#02\",]),\n",
    "                \"_Bandlock_Udp_B1_B3\":  (6, [\"#01\", \"#02\", \"#03\", \"#04\", \"#05\", \"#06\"]),\n",
    "            },\n",
    "        \"2022-12-22\":\n",
    "            {\n",
    "                \"_Bandlock_Udp_B1_B3\":  (3, [\"#02\", \"#03\", \"#04\",]),\n",
    "                \"_Bandlock_Udp_B3_B7\":  (4, [\"#01\", \"#02\", \"#03\", \"#04\",]),\n",
    "                \"_Bandlock_Udp_B7_B8\":  (4, [\"#01\", \"#02\", \"#03\", \"#04\",]),\n",
    "                \"_Bandlock_Udp_B8_B1\":  (4, [\"#01\", \"#02\", \"#03\", \"#04\",]),\n",
    "            },\n",
    "}\n",
    "# devices = [\"sm05\",]\n",
    "# setnames = [\"all\",]\n",
    "devices = [\"sm05\", \"sm06\", \"sm07\", \"sm08\"]\n",
    "setnames = [\"all\", \"b1\", \"b3\", \"b1b3\"]\n",
    "\n",
    "handover_types_0 = \"lte_handover,SN_addition,SN_removal,endc_SN_change,endc_MN_change,endc_MNSN_change,lte2endc_MN_change,endc2lte_MN_change\".split(',')\n",
    "handover_types_1 = \"SN_change_only,Intra_frequency,Inter_frequency\".split(',')\n",
    "handover_types_2 = \"SN_change_only,Intra_sector,Intra_eNB,Inter_eNB\".split(',')\n",
    "handover_fail_types = \"scg_failure,radio_link_failure,nas_recovery\".split(',')\n",
    "handover_types = handover_types_0 + handover_fail_types\n",
    "\n",
    "first_time = 1\n",
    "lodict = {}\n",
    "type_names = []\n",
    "for date, _exps in exps.items():\n",
    "    for expr, (times, traces) in _exps.items():\n",
    "        for k, (dev, stg) in enumerate(zip(devices, setnames)):\n",
    "            # print('@'.join([stg, dev]))\n",
    "            # print(\"----------------------\")\n",
    "            for trace in traces:\n",
    "                \"\"\"\"\"\"\n",
    "                source_dir = os.path.join(database, date, expr, dev, trace, \"statistics\", \"classify-bilateral\")\n",
    "                df = pd.read_csv(os.path.join(source_dir, \"uplk-loss\", \"uplk_loss_classify_1_3.csv\"), index_col=0)\n",
    "                # df = pd.read_csv(os.path.join(source_dir, \"dnlk-loss\", \"dnlk_loss_classify_1_3.csv\"), index_col=0)\n",
    "                # df = pd.read_csv(os.path.join(source_dir, \"uplk-loss\", \"uplk_loss_classify_1.csv\"), index_col=0)\n",
    "                \"\"\"\"\"\"\n",
    "                _lodict = df.to_dict()\n",
    "                if first_time:\n",
    "                    lodict = _lodict\n",
    "                    first_time = 0\n",
    "                    type_names = list(lodict[\"event_occur\"].keys())\n",
    "                    # print(type_names)\n",
    "                    for key in type_names:\n",
    "                        lodict[\"event_dur\"][key] = [float(item) for item in lodict[\"event_dur\"][key].split('@')] if type(lodict[\"event_dur\"][key]) == str else []\n",
    "                        lodict[\"loss_num\"][key] = [int(item) for item in lodict[\"loss_num\"][key].split('@')] if type(lodict[\"loss_num\"][key]) == str else []\n",
    "                        lodict[\"pkt_num\"][key] = [int(item) for item in lodict[\"pkt_num\"][key].split('@')] if type(lodict[\"pkt_num\"][key]) == str else []\n",
    "                    continue\n",
    "                for key in type_names:\n",
    "                    lodict[\"event_occur\"][key] += _lodict[\"event_occur\"][key]\n",
    "                    lodict[\"event_dur\"][key] += [float(item) for item in _lodict[\"event_dur\"][key].split('@')] if type(_lodict[\"event_dur\"][key]) == str else []\n",
    "                    lodict[\"loss_num\"][key] += [int(item) for item in _lodict[\"loss_num\"][key].split('@')] if type(_lodict[\"loss_num\"][key]) == str else []\n",
    "                    lodict[\"pkt_num\"][key] += [int(item) for item in _lodict[\"pkt_num\"][key].split('@')] if type(_lodict[\"pkt_num\"][key]) == str else []\n",
    "                    lodict[\"t_duration\"][key] += _lodict[\"t_duration\"][key]\n",
    "                    lodict[\"t_loss_num\"][key] += _lodict[\"t_loss_num\"][key]\n",
    "                    lodict[\"t_pkt_num\"][key] += _lodict[\"t_pkt_num\"][key]\n",
    "\n",
    "# pprint(lodict, sort_dicts=False, width=100, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = []\n",
    "for type_name in handover_types:\n",
    "    column_names += [\"before_{}\".format(type_name), \"during_{}\".format(type_name), \"after_{}\".format(type_name)]\n",
    "column_names += [\"unstable\", \"stable\", \"overall\"]\n",
    "    \n",
    "handover_types_0\n",
    "handover_fail_types\n",
    "\n",
    "data = {\"Occurrence\":{},\n",
    "        \"Lost packet number\":{},\n",
    "        \"Proportion (%)\":{},\n",
    "        \"Avg loss rate (%)\":{},\n",
    "        \"Avg loss per event\":{}\n",
    "        }\n",
    "\n",
    "sum1 = 0\n",
    "sum2 = 0\n",
    "occ = 0\n",
    "for key in handover_types_0:\n",
    "    _sum1 = 0\n",
    "    _sum2 = 0\n",
    "    for key2 in [\"before\", \"during\", \"after\"]:\n",
    "        _sum1 += lodict[\"t_loss_num\"][\"{}_{}\".format(key2, key)]\n",
    "        sum1 += lodict[\"t_loss_num\"][\"{}_{}\".format(key2, key)]\n",
    "        _sum2 += lodict[\"t_pkt_num\"][\"{}_{}\".format(key2, key)]\n",
    "        sum2 += lodict[\"t_pkt_num\"][\"{}_{}\".format(key2, key)]\n",
    "    _occ = lodict[\"event_occur\"][\"{}_{}\".format(key2, key)]\n",
    "    occ += lodict[\"event_occur\"][\"{}_{}\".format(key2, key)]\n",
    "    data[\"Occurrence\"][key] = _occ\n",
    "    data[\"Lost packet number\"][key] = _sum1\n",
    "    data[\"Proportion (%)\"][key] = round(_sum1 / (lodict[\"t_loss_num\"][\"overall\"] + 1e-9) * 100, 1)\n",
    "    data[\"Avg loss rate (%)\"][key] = round(_sum1 / (_sum2 + 1e-9) * 100, 3) if _sum2 > 0 else '-'\n",
    "    data[\"Avg loss per event\"][key] = round(_sum1 / (_occ + 1e-9), 3) if _occ > 0 else '-'\n",
    "    # print(key, _sum1, round(_sum1 / 21803 *100, 3), round(_sum1 / (_sum2 + 1e-9) *100, 3), round(_sum1 / (_occ + 1e-9), 3))\n",
    "data[\"Occurrence\"][\"success\"] = occ\n",
    "data[\"Lost packet number\"][\"success\"] = sum1\n",
    "data[\"Proportion (%)\"][\"success\"] = round(sum1 / (lodict[\"t_loss_num\"][\"overall\"] + 1e-9) * 100, 1)\n",
    "data[\"Avg loss rate (%)\"][\"success\"] = round(sum1 / (sum2 + 1e-9) * 100, 3) if sum2 > 0 else '-'\n",
    "data[\"Avg loss per event\"][\"success\"] = round(sum1 / (occ + 1e-9), 3) if occ > 0 else '-'\n",
    "\n",
    "sum1 = 0\n",
    "sum2 = 0\n",
    "occ = 0\n",
    "for key in handover_fail_types:\n",
    "    _sum1 = 0\n",
    "    _sum2 = 0\n",
    "    for key2 in [\"before\", \"during\", \"after\"]:\n",
    "        _sum1 += lodict[\"t_loss_num\"][\"{}_{}\".format(key2, key)]\n",
    "        sum1 += lodict[\"t_loss_num\"][\"{}_{}\".format(key2, key)]\n",
    "        _sum2 += lodict[\"t_pkt_num\"][\"{}_{}\".format(key2, key)]\n",
    "        sum2 += lodict[\"t_pkt_num\"][\"{}_{}\".format(key2, key)]\n",
    "    _occ = lodict[\"event_occur\"][\"{}_{}\".format(key2, key)]\n",
    "    occ += lodict[\"event_occur\"][\"{}_{}\".format(key2, key)]\n",
    "    data[\"Occurrence\"][key] = _occ\n",
    "    data[\"Lost packet number\"][key] = _sum1\n",
    "    data[\"Proportion (%)\"][key] = round(_sum1 / (lodict[\"t_loss_num\"][\"overall\"] + 1e-9) * 100, 1)\n",
    "    data[\"Avg loss rate (%)\"][key] = round(_sum1 / (_sum2 + 1e-9) * 100, 3) if _sum2 > 0 else '-'\n",
    "    data[\"Avg loss per event\"][key] = round(_sum1 / (_occ + 1e-9), 3) if _occ > 0 else '-'\n",
    "    # print(key, _sum1, round(_sum1 / 21803 *100, 3), round(_sum1 / (_sum2 + 1e-9) *100, 3), round(_sum1 / (_occ + 1e-9), 3))\n",
    "data[\"Occurrence\"][\"fail\"] = occ\n",
    "data[\"Lost packet number\"][\"fail\"] = sum1\n",
    "data[\"Proportion (%)\"][\"fail\"] = round(sum1 / (lodict[\"t_loss_num\"][\"overall\"] + 1e-9) * 100, 3)\n",
    "data[\"Avg loss rate (%)\"][\"fail\"] = round(sum1 / (sum2 + 1e-9) * 100, 1) if sum2 > 0 else '-'\n",
    "data[\"Avg loss per event\"][\"fail\"] = round(sum1 / (occ + 1e-9), 3) if occ > 0 else '-'\n",
    "\n",
    "for key in [\"stable\", \"overall\"]:\n",
    "    _sum1 = lodict[\"t_loss_num\"][key]\n",
    "    _sum2 = lodict[\"t_pkt_num\"][key]\n",
    "    data[\"Occurrence\"][key] = '-'\n",
    "    data[\"Lost packet number\"][key] = _sum1\n",
    "    data[\"Proportion (%)\"][key] = round(_sum1 / (lodict[\"t_loss_num\"][\"overall\"] + 1e-9) * 100, 1)\n",
    "    data[\"Avg loss rate (%)\"][key] = round(_sum1 / (_sum2 + 1e-9) * 100, 3) if _sum2 > 0 else '-'\n",
    "    data[\"Avg loss per event\"][key] = '-'\n",
    "    # print(key, _sum1, round(_sum1 / 21803 *100, 3), round(_sum1 / (_sum2 + 1e-9) *100, 3))\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "display(df)\n",
    "\n",
    "row_names = handover_types_0 + [\"success\"] + handover_fail_types + [\"fail\"] + [\"stable\", \"overall\"]\n",
    "for row in row_names:\n",
    "    print(*list(df.loc[row]), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ** SUCC\n",
    "# fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True)\n",
    "# fig, ax = plt.subplots(nrows=2, ncols=1, sharex=True)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(11.5, 6), sharex=True)\n",
    "rel = [\"before\", \"during\", \"after\"]\n",
    "_never_occurs = set()\n",
    "\n",
    "fig.suptitle(\"UL Packet Loss Rate Per Event (x=1s): Successful HO\")\n",
    "\n",
    "labels = handover_types_0\n",
    "handles = [None]*len(labels)\n",
    "for i, _rel in enumerate(rel):\n",
    "    ax[i].set_title(_rel.title())\n",
    "    ax[i].set_xlabel(\"loss rate (%)\")\n",
    "    for j, type_name in enumerate(handover_types_0):\n",
    "        # data = lodict[\"loss_num\"][\"after_lte_handover\"]\n",
    "        # event_name = \"before_{}\".format(type_name)\n",
    "        event_name = \"{}_{}\".format(_rel, type_name)\n",
    "        data1 = lodict[\"loss_num\"][event_name]\n",
    "        data2 = lodict[\"pkt_num\"][event_name]\n",
    "        data = [round(numer / (denom + 1e-9) * 100, 3) for numer, denom in zip(data1, data2)]\n",
    "        if len(data) == 0:\n",
    "            _never_occurs.add(type_name)\n",
    "            continue\n",
    "        # print(data)\n",
    "        count, bins_count = np.histogram(data, bins=5000)\n",
    "        # print(count, bins_count)\n",
    "        pdf = count / sum(count)\n",
    "        cdf = np.cumsum(pdf)\n",
    "        # ax[i].plot(bins_count[1:], cdf, label=event_name)\n",
    "        handles[j], = ax[i].plot(bins_count[1:], cdf)\n",
    "axbox = ax[1].get_position()\n",
    "# print(axbox)\n",
    "labels = [item for key, item in zip(handles, labels) if key != None]\n",
    "handles = [item for item in handles if item != None]\n",
    "fig.legend(\n",
    "    handles=handles, labels=labels,\n",
    "    loc='lower center', bbox_to_anchor=[0, axbox.y0-0.2,1,1], ncol=2)\n",
    "fig.savefig(os.path.join(figdir, \"ulloss_HO.png\"), bbox_inches='tight')\n",
    "fig.show()\n",
    "print(\"Never Occurs:\", list(_never_occurs))\n",
    "\n",
    "### ** FAIL\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(11.5, 6), sharex=True)\n",
    "rel = [\"before\", \"after\"]\n",
    "_never_occurs = set()\n",
    "\n",
    "fig.suptitle(\"UL Packet Loss Rate Per Event (x=3s): Failed HO\")\n",
    "\n",
    "labels = handover_fail_types\n",
    "handles = [None]*len(labels)\n",
    "for i, _rel in enumerate(rel):\n",
    "    ax[i].set_title(_rel.title())\n",
    "    ax[i].set_xlabel(\"loss rate (%)\")\n",
    "    for j, type_name in enumerate(handover_fail_types):\n",
    "        # data = lodict[\"loss_num\"][\"after_lte_handover\"]\n",
    "        # event_name = \"before_{}\".format(type_name)\n",
    "        event_name = \"{}_{}\".format(_rel, type_name)\n",
    "        data1 = lodict[\"loss_num\"][event_name]\n",
    "        data2 = lodict[\"pkt_num\"][event_name]\n",
    "        data = [round(numer / (denom + 1e-9) * 100, 3) for numer, denom in zip(data1, data2)]\n",
    "        if len(data) == 0:\n",
    "            _never_occurs.add(type_name)\n",
    "            continue\n",
    "        # print(data)\n",
    "        count, bins_count = np.histogram(data, bins=5000)\n",
    "        # print(count, bins_count)\n",
    "        pdf = count / sum(count)\n",
    "        cdf = np.cumsum(pdf)\n",
    "        # ax[i].plot(bins_count[1:], cdf, label=event_name)\n",
    "        handles[j], = ax[i].plot(bins_count[1:], cdf)\n",
    "axbox = ax[1].get_position()\n",
    "# print(axbox)\n",
    "labels = [item for key, item in zip(handles, labels) if key != None]\n",
    "handles = [item for item in handles if item != None]\n",
    "fig.legend(\n",
    "    handles=handles, labels=labels,\n",
    "    loc='lower center', bbox_to_anchor=[0, axbox.y0-0.2,1,1], ncol=2)\n",
    "fig.savefig(os.path.join(figdir, \"ulloss_Fail.png\"), bbox_inches='tight')\n",
    "fig.show()\n",
    "print(\"Never Occurs:\", list(_never_occurs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHT 8+3 types: DL Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = \"/home/wmnlab/D/database/\"\n",
    "exps = { \"2022-12-20\": \n",
    "            {  # experiment_name: (number_of_experiment_rounds, list_of_experiment_round)\n",
    "                # \"_Bandlock_Udp_B1_B3\":  (1, [\"#01\",]),\n",
    "                # \"_Bandlock_Udp_B1_B3\":  (2, [\"#01\", \"#02\",]),\n",
    "                \"_Bandlock_Udp_B1_B3\":  (6, [\"#01\", \"#02\", \"#03\", \"#04\", \"#05\", \"#06\"]),\n",
    "            },\n",
    "        \"2022-12-22\":\n",
    "            {\n",
    "                \"_Bandlock_Udp_B1_B3\":  (3, [\"#02\", \"#03\", \"#04\",]),\n",
    "                \"_Bandlock_Udp_B3_B7\":  (4, [\"#01\", \"#02\", \"#03\", \"#04\",]),\n",
    "                \"_Bandlock_Udp_B7_B8\":  (4, [\"#01\", \"#02\", \"#03\", \"#04\",]),\n",
    "                \"_Bandlock_Udp_B8_B1\":  (4, [\"#01\", \"#02\", \"#03\", \"#04\",]),\n",
    "            },\n",
    "}\n",
    "# devices = [\"sm05\",]\n",
    "# setnames = [\"all\",]\n",
    "devices = [\"sm05\", \"sm06\", \"sm07\", \"sm08\"]\n",
    "setnames = [\"all\", \"b1\", \"b3\", \"b1b3\"]\n",
    "\n",
    "handover_types_0 = \"lte_handover,SN_addition,SN_removal,endc_SN_change,endc_MN_change,endc_MNSN_change,lte2endc_MN_change,endc2lte_MN_change\".split(',')\n",
    "handover_types_1 = \"SN_change_only,Intra_frequency,Inter_frequency\".split(',')\n",
    "handover_types_2 = \"SN_change_only,Intra_sector,Intra_eNB,Inter_eNB\".split(',')\n",
    "handover_fail_types = \"scg_failure,radio_link_failure,nas_recovery\".split(',')\n",
    "handover_types = handover_types_0 + handover_fail_types\n",
    "\n",
    "first_time = 1\n",
    "lodict = {}\n",
    "type_names = []\n",
    "for date, _exps in exps.items():\n",
    "    for expr, (times, traces) in _exps.items():\n",
    "        for k, (dev, stg) in enumerate(zip(devices, setnames)):\n",
    "            # print('@'.join([stg, dev]))\n",
    "            # print(\"----------------------\")\n",
    "            for trace in traces:\n",
    "                \"\"\"\"\"\"\n",
    "                source_dir = os.path.join(database, date, expr, dev, trace, \"statistics\", \"classify-bilateral\")\n",
    "                # df = pd.read_csv(os.path.join(source_dir, \"uplk-loss\", \"uplk_loss_classify_1_3.csv\"), index_col=0)\n",
    "                df = pd.read_csv(os.path.join(source_dir, \"dnlk-loss\", \"dnlk_loss_classify_1_3.csv\"), index_col=0)\n",
    "                # df = pd.read_csv(os.path.join(source_dir, \"uplk-loss\", \"uplk_loss_classify_1.csv\"), index_col=0)\n",
    "                \"\"\"\"\"\"\n",
    "                _lodict = df.to_dict()\n",
    "                if first_time:\n",
    "                    lodict = _lodict\n",
    "                    first_time = 0\n",
    "                    type_names = list(lodict[\"event_occur\"].keys())\n",
    "                    # print(type_names)\n",
    "                    for key in type_names:\n",
    "                        lodict[\"event_dur\"][key] = [float(item) for item in lodict[\"event_dur\"][key].split('@')] if type(lodict[\"event_dur\"][key]) == str else []\n",
    "                        lodict[\"loss_num\"][key] = [int(item) for item in lodict[\"loss_num\"][key].split('@')] if type(lodict[\"loss_num\"][key]) == str else []\n",
    "                        lodict[\"pkt_num\"][key] = [int(item) for item in lodict[\"pkt_num\"][key].split('@')] if type(lodict[\"pkt_num\"][key]) == str else []\n",
    "                    continue\n",
    "                for key in type_names:\n",
    "                    lodict[\"event_occur\"][key] += _lodict[\"event_occur\"][key]\n",
    "                    lodict[\"event_dur\"][key] += [float(item) for item in _lodict[\"event_dur\"][key].split('@')] if type(_lodict[\"event_dur\"][key]) == str else []\n",
    "                    lodict[\"loss_num\"][key] += [int(item) for item in _lodict[\"loss_num\"][key].split('@')] if type(_lodict[\"loss_num\"][key]) == str else []\n",
    "                    lodict[\"pkt_num\"][key] += [int(item) for item in _lodict[\"pkt_num\"][key].split('@')] if type(_lodict[\"pkt_num\"][key]) == str else []\n",
    "                    lodict[\"t_duration\"][key] += _lodict[\"t_duration\"][key]\n",
    "                    lodict[\"t_loss_num\"][key] += _lodict[\"t_loss_num\"][key]\n",
    "                    lodict[\"t_pkt_num\"][key] += _lodict[\"t_pkt_num\"][key]\n",
    "\n",
    "# pprint(lodict, sort_dicts=False, width=100, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = []\n",
    "for type_name in handover_types:\n",
    "    column_names += [\"before_{}\".format(type_name), \"during_{}\".format(type_name), \"after_{}\".format(type_name)]\n",
    "column_names += [\"unstable\", \"stable\", \"overall\"]\n",
    "    \n",
    "handover_types_0\n",
    "handover_fail_types\n",
    "\n",
    "data = {\"Occurrence\":{},\n",
    "        \"Lost packet number\":{},\n",
    "        \"Proportion (%)\":{},\n",
    "        \"Avg loss rate (%)\":{},\n",
    "        \"Avg loss per event\":{}\n",
    "        }\n",
    "\n",
    "sum1 = 0\n",
    "sum2 = 0\n",
    "occ = 0\n",
    "for key in handover_types_0:\n",
    "    _sum1 = 0\n",
    "    _sum2 = 0\n",
    "    for key2 in [\"before\", \"during\", \"after\"]:\n",
    "        _sum1 += lodict[\"t_loss_num\"][\"{}_{}\".format(key2, key)]\n",
    "        sum1 += lodict[\"t_loss_num\"][\"{}_{}\".format(key2, key)]\n",
    "        _sum2 += lodict[\"t_pkt_num\"][\"{}_{}\".format(key2, key)]\n",
    "        sum2 += lodict[\"t_pkt_num\"][\"{}_{}\".format(key2, key)]\n",
    "    _occ = lodict[\"event_occur\"][\"{}_{}\".format(key2, key)]\n",
    "    occ += lodict[\"event_occur\"][\"{}_{}\".format(key2, key)]\n",
    "    data[\"Occurrence\"][key] = _occ\n",
    "    data[\"Lost packet number\"][key] = _sum1\n",
    "    data[\"Proportion (%)\"][key] = round(_sum1 / (lodict[\"t_loss_num\"][\"overall\"] + 1e-9) * 100, 1)\n",
    "    data[\"Avg loss rate (%)\"][key] = round(_sum1 / (_sum2 + 1e-9) * 100, 3) if _sum2 > 0 else '-'\n",
    "    data[\"Avg loss per event\"][key] = round(_sum1 / (_occ + 1e-9), 3) if _occ > 0 else '-'\n",
    "    # print(key, _sum1, round(_sum1 / 21803 *100, 3), round(_sum1 / (_sum2 + 1e-9) *100, 3), round(_sum1 / (_occ + 1e-9), 3))\n",
    "data[\"Occurrence\"][\"success\"] = occ\n",
    "data[\"Lost packet number\"][\"success\"] = sum1\n",
    "data[\"Proportion (%)\"][\"success\"] = round(sum1 / (lodict[\"t_loss_num\"][\"overall\"] + 1e-9) * 100, 1)\n",
    "data[\"Avg loss rate (%)\"][\"success\"] = round(sum1 / (sum2 + 1e-9) * 100, 3) if sum2 > 0 else '-'\n",
    "data[\"Avg loss per event\"][\"success\"] = round(sum1 / (occ + 1e-9), 3) if occ > 0 else '-'\n",
    "\n",
    "sum1 = 0\n",
    "sum2 = 0\n",
    "occ = 0\n",
    "for key in handover_fail_types:\n",
    "    _sum1 = 0\n",
    "    _sum2 = 0\n",
    "    for key2 in [\"before\", \"during\", \"after\"]:\n",
    "        _sum1 += lodict[\"t_loss_num\"][\"{}_{}\".format(key2, key)]\n",
    "        sum1 += lodict[\"t_loss_num\"][\"{}_{}\".format(key2, key)]\n",
    "        _sum2 += lodict[\"t_pkt_num\"][\"{}_{}\".format(key2, key)]\n",
    "        sum2 += lodict[\"t_pkt_num\"][\"{}_{}\".format(key2, key)]\n",
    "    _occ = lodict[\"event_occur\"][\"{}_{}\".format(key2, key)]\n",
    "    occ += lodict[\"event_occur\"][\"{}_{}\".format(key2, key)]\n",
    "    data[\"Occurrence\"][key] = _occ\n",
    "    data[\"Lost packet number\"][key] = _sum1\n",
    "    data[\"Proportion (%)\"][key] = round(_sum1 / (lodict[\"t_loss_num\"][\"overall\"] + 1e-9) * 100, 1)\n",
    "    data[\"Avg loss rate (%)\"][key] = round(_sum1 / (_sum2 + 1e-9) * 100, 3) if _sum2 > 0 else '-'\n",
    "    data[\"Avg loss per event\"][key] = round(_sum1 / (_occ + 1e-9), 3) if _occ > 0 else '-'\n",
    "    # print(key, _sum1, round(_sum1 / 21803 *100, 3), round(_sum1 / (_sum2 + 1e-9) *100, 3), round(_sum1 / (_occ + 1e-9), 3))\n",
    "data[\"Occurrence\"][\"fail\"] = occ\n",
    "data[\"Lost packet number\"][\"fail\"] = sum1\n",
    "data[\"Proportion (%)\"][\"fail\"] = round(sum1 / (lodict[\"t_loss_num\"][\"overall\"] + 1e-9) * 100, 3)\n",
    "data[\"Avg loss rate (%)\"][\"fail\"] = round(sum1 / (sum2 + 1e-9) * 100, 1) if sum2 > 0 else '-'\n",
    "data[\"Avg loss per event\"][\"fail\"] = round(sum1 / (occ + 1e-9), 3) if occ > 0 else '-'\n",
    "\n",
    "for key in [\"stable\", \"overall\"]:\n",
    "    _sum1 = lodict[\"t_loss_num\"][key]\n",
    "    _sum2 = lodict[\"t_pkt_num\"][key]\n",
    "    data[\"Occurrence\"][key] = '-'\n",
    "    data[\"Lost packet number\"][key] = _sum1\n",
    "    data[\"Proportion (%)\"][key] = round(_sum1 / (lodict[\"t_loss_num\"][\"overall\"] + 1e-9) * 100, 1)\n",
    "    data[\"Avg loss rate (%)\"][key] = round(_sum1 / (_sum2 + 1e-9) * 100, 3) if _sum2 > 0 else '-'\n",
    "    data[\"Avg loss per event\"][key] = '-'\n",
    "    # print(key, _sum1, round(_sum1 / 21803 *100, 3), round(_sum1 / (_sum2 + 1e-9) *100, 3))\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "display(df)\n",
    "\n",
    "row_names = handover_types_0 + [\"success\"] + handover_fail_types + [\"fail\"] + [\"stable\", \"overall\"]\n",
    "for row in row_names:\n",
    "    print(*list(df.loc[row]), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ** SUCC\n",
    "# fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True)\n",
    "# fig, ax = plt.subplots(nrows=2, ncols=1, sharex=True)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(11.5, 6), sharex=True)\n",
    "rel = [\"before\", \"during\", \"after\"]\n",
    "_never_occurs = set()\n",
    "\n",
    "fig.suptitle(\"DL Packet Loss Rate Per Event (x=1s): Successful HO\")\n",
    "\n",
    "labels = handover_types_0\n",
    "handles = [None]*len(labels)\n",
    "for i, _rel in enumerate(rel):\n",
    "    ax[i].set_title(_rel.title())\n",
    "    ax[i].set_xlabel(\"loss rate (%)\")\n",
    "    for j, type_name in enumerate(handover_types_0):\n",
    "        # data = lodict[\"loss_num\"][\"after_lte_handover\"]\n",
    "        # event_name = \"before_{}\".format(type_name)\n",
    "        event_name = \"{}_{}\".format(_rel, type_name)\n",
    "        data1 = lodict[\"loss_num\"][event_name]\n",
    "        data2 = lodict[\"pkt_num\"][event_name]\n",
    "        data = [round(numer / (denom + 1e-9) * 100, 3) for numer, denom in zip(data1, data2)]\n",
    "        if len(data) == 0:\n",
    "            _never_occurs.add(type_name)\n",
    "            continue\n",
    "        # print(data)\n",
    "        count, bins_count = np.histogram(data, bins=5000)\n",
    "        # print(count, bins_count)\n",
    "        pdf = count / sum(count)\n",
    "        cdf = np.cumsum(pdf)\n",
    "        # ax[i].plot(bins_count[1:], cdf, label=event_name)\n",
    "        handles[j], = ax[i].plot(bins_count[1:], cdf)\n",
    "axbox = ax[1].get_position()\n",
    "# print(axbox)\n",
    "labels = [item for key, item in zip(handles, labels) if key != None]\n",
    "handles = [item for item in handles if item != None]\n",
    "fig.legend(\n",
    "    handles=handles, labels=labels,\n",
    "    loc='lower center', bbox_to_anchor=[0, axbox.y0-0.2,1,1], ncol=2)\n",
    "fig.savefig(os.path.join(figdir, \"dlloss_HO.png\"), bbox_inches='tight')\n",
    "fig.show()\n",
    "print(\"Never Occurs:\", list(_never_occurs))\n",
    "\n",
    "### ** FAIL\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(11.5, 6), sharex=True)\n",
    "rel = [\"before\", \"after\"]\n",
    "_never_occurs = set()\n",
    "\n",
    "fig.suptitle(\"DL Packet Loss Rate Per Event (x=3s): Failed HO\")\n",
    "\n",
    "labels = handover_fail_types\n",
    "handles = [None]*len(labels)\n",
    "for i, _rel in enumerate(rel):\n",
    "    ax[i].set_title(_rel.title())\n",
    "    ax[i].set_xlabel(\"loss rate (%)\")\n",
    "    for j, type_name in enumerate(handover_fail_types):\n",
    "        # data = lodict[\"loss_num\"][\"after_lte_handover\"]\n",
    "        # event_name = \"before_{}\".format(type_name)\n",
    "        event_name = \"{}_{}\".format(_rel, type_name)\n",
    "        data1 = lodict[\"loss_num\"][event_name]\n",
    "        data2 = lodict[\"pkt_num\"][event_name]\n",
    "        data = [round(numer / (denom + 1e-9) * 100, 3) for numer, denom in zip(data1, data2)]\n",
    "        if len(data) == 0:\n",
    "            _never_occurs.add(type_name)\n",
    "            continue\n",
    "        # print(data)\n",
    "        count, bins_count = np.histogram(data, bins=5000)\n",
    "        # print(count, bins_count)\n",
    "        pdf = count / sum(count)\n",
    "        cdf = np.cumsum(pdf)\n",
    "        # ax[i].plot(bins_count[1:], cdf, label=event_name)\n",
    "        handles[j], = ax[i].plot(bins_count[1:], cdf)\n",
    "axbox = ax[1].get_position()\n",
    "# print(axbox)\n",
    "labels = [item for key, item in zip(handles, labels) if key != None]\n",
    "handles = [item for item in handles if item != None]\n",
    "fig.legend(\n",
    "    handles=handles, labels=labels,\n",
    "    loc='lower center', bbox_to_anchor=[0, axbox.y0-0.2,1,1], ncol=2)\n",
    "fig.savefig(os.path.join(figdir, \"dlloss_Fail.png\"), bbox_inches='tight')\n",
    "fig.show()\n",
    "print(\"Never Occurs:\", list(_never_occurs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHT 8+3 types / 4 schemes: UL Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = \"/home/wmnlab/D/database/\"\n",
    "exps = {\n",
    "        \"2022-12-20\": \n",
    "            {  # experiment_name: (number_of_experiment_rounds, list_of_experiment_round)\n",
    "                \"_Bandlock_Udp_B1_B3\":  (1, [\"#02\",]),\n",
    "                # \"_Bandlock_Udp_B1_B3\":  (2, [\"#01\", \"#02\",]),\n",
    "                # \"_Bandlock_Udp_B1_B3\":  (6, [\"#01\", \"#02\", \"#03\", \"#04\", \"#05\", \"#06\"]),\n",
    "                # \"_Bandlock_Udp_B1_B3\":  (5, [\"#01\", \"#03\", \"#04\", \"#05\", \"#06\"]),\n",
    "            },\n",
    "        # \"2022-12-22\":\n",
    "        #     {\n",
    "        #         # \"_Bandlock_Udp_B1_B3\":  (3, [\"#02\", \"#03\", \"#04\",]),\n",
    "        #         # \"_Bandlock_Udp_B3_B7\":  (4, [\"#01\", \"#02\", \"#03\", \"#04\",]),\n",
    "        #         # \"_Bandlock_Udp_B7_B8\":  (4, [\"#01\", \"#02\", \"#03\", \"#04\",]),\n",
    "        #         \"_Bandlock_Udp_B8_B1\":  (4, [\"#01\", \"#02\", \"#03\", \"#04\",]),\n",
    "        #     },\n",
    "}\n",
    "# devices = [\"sm05\",]\n",
    "# setnames = [\"all\",]\n",
    "devices = [\"sm05\", \"sm06\", \"sm07\", \"sm08\"]\n",
    "setnames = [\"all\", \"b1\", \"b3\", \"b1b3\"]\n",
    "# setnames = [\"all\", \"b3\", \"b7\", \"b3b7\"]\n",
    "# setnames = [\"all\", \"b7\", \"b8\", \"b7b8\"]\n",
    "# setnames = [\"all\", \"b8\", \"b1\", \"b8b1\"]\n",
    "\n",
    "handover_types_0 = \"lte_handover,SN_addition,SN_removal,endc_SN_change,endc_MN_change,endc_MNSN_change,lte2endc_MN_change,endc2lte_MN_change\".split(',')\n",
    "handover_types_1 = \"SN_change_only,Intra_frequency,Inter_frequency\".split(',')\n",
    "handover_types_2 = \"SN_change_only,Intra_sector,Intra_eNB,Inter_eNB\".split(',')\n",
    "handover_fail_types = \"scg_failure,radio_link_failure,nas_recovery\".split(',')\n",
    "handover_types = handover_types_0 + handover_fail_types\n",
    "\n",
    "# first_time = 1\n",
    "lodicts = [{}, {}, {}, {}]\n",
    "lodict = {}\n",
    "type_names = []\n",
    "for date, _exps in exps.items():\n",
    "    for expr, (times, traces) in _exps.items():\n",
    "        for k, (dev, stg) in enumerate(zip(devices, setnames)):\n",
    "            # lodict = lodicts[k]\n",
    "            # print(lodict)\n",
    "            print('@'.join([stg, dev]))\n",
    "            print(\"----------------------\")\n",
    "            first_time = 1\n",
    "            for trace in traces:\n",
    "                \"\"\"\"\"\"\n",
    "                source_dir = os.path.join(database, date, expr, dev, trace, \"statistics\", \"classify-bilateral\")\n",
    "                df = pd.read_csv(os.path.join(source_dir, \"uplk-loss\", \"uplk_loss_classify_1_3.csv\"), index_col=0)\n",
    "                # df = pd.read_csv(os.path.join(source_dir, \"dnlk-loss\", \"dnlk_loss_classify_1_3.csv\"), index_col=0)\n",
    "                # df = pd.read_csv(os.path.join(source_dir, \"uplk-loss\", \"uplk_loss_classify_1.csv\"), index_col=0)\n",
    "                \"\"\"\"\"\"\n",
    "                _lodict = df.to_dict()\n",
    "                if first_time:\n",
    "                    # print(lodicts[k])\n",
    "                    lodicts[k] = _lodict\n",
    "                    first_time = 0\n",
    "                    type_names = list(lodicts[k][\"event_occur\"].keys())\n",
    "                    # print(type_names)\n",
    "                    for key in type_names:\n",
    "                        lodicts[k][\"event_dur\"][key] = [float(item) for item in lodicts[k][\"event_dur\"][key].split('@')] if type(lodicts[k][\"event_dur\"][key]) == str else []\n",
    "                        lodicts[k][\"loss_num\"][key] = [int(item) for item in lodicts[k][\"loss_num\"][key].split('@')] if type(lodicts[k][\"loss_num\"][key]) == str else []\n",
    "                        lodicts[k][\"pkt_num\"][key] = [int(item) for item in lodicts[k][\"pkt_num\"][key].split('@')] if type(lodicts[k][\"pkt_num\"][key]) == str else []\n",
    "                    continue\n",
    "                for key in type_names:\n",
    "                    lodicts[k][\"event_occur\"][key] += _lodict[\"event_occur\"][key]\n",
    "                    lodicts[k][\"event_dur\"][key] += [float(item) for item in _lodict[\"event_dur\"][key].split('@')] if type(_lodict[\"event_dur\"][key]) == str else []\n",
    "                    lodicts[k][\"loss_num\"][key] += [int(item) for item in _lodict[\"loss_num\"][key].split('@')] if type(_lodict[\"loss_num\"][key]) == str else []\n",
    "                    lodicts[k][\"pkt_num\"][key] += [int(item) for item in _lodict[\"pkt_num\"][key].split('@')] if type(_lodict[\"pkt_num\"][key]) == str else []\n",
    "                    lodicts[k][\"t_duration\"][key] += _lodict[\"t_duration\"][key]\n",
    "                    lodicts[k][\"t_loss_num\"][key] += _lodict[\"t_loss_num\"][key]\n",
    "                    lodicts[k][\"t_pkt_num\"][key] += _lodict[\"t_pkt_num\"][key]\n",
    "\n",
    "# pprint(lodicts, sort_dicts=False, width=100, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = []\n",
    "for type_name in handover_types:\n",
    "    column_names += [\"before_{}\".format(type_name), \"during_{}\".format(type_name), \"after_{}\".format(type_name)]\n",
    "column_names += [\"unstable\", \"stable\", \"overall\"]\n",
    "handover_types_0\n",
    "handover_fail_types\n",
    "\n",
    "for k, (dev, stg) in enumerate(zip(devices, setnames)):\n",
    "    print('@'.join([stg, dev]))\n",
    "    data = {\"Occurrence\":{},\n",
    "            \"Lost packet number\":{},\n",
    "            \"Proportion (%)\":{},\n",
    "            \"Avg loss rate (%)\":{},\n",
    "            \"Avg loss per event\":{}\n",
    "            }\n",
    "\n",
    "    sum1 = 0\n",
    "    sum2 = 0\n",
    "    occ = 0\n",
    "    for key in handover_types_0:\n",
    "        _sum1 = 0\n",
    "        _sum2 = 0\n",
    "        for key2 in [\"before\", \"during\", \"after\"]:\n",
    "            _sum1 += lodicts[k][\"t_loss_num\"][\"{}_{}\".format(key2, key)]\n",
    "            sum1 += lodicts[k][\"t_loss_num\"][\"{}_{}\".format(key2, key)]\n",
    "            _sum2 += lodicts[k][\"t_pkt_num\"][\"{}_{}\".format(key2, key)]\n",
    "            sum2 += lodicts[k][\"t_pkt_num\"][\"{}_{}\".format(key2, key)]\n",
    "        _occ = lodicts[k][\"event_occur\"][\"{}_{}\".format(key2, key)]\n",
    "        occ += lodicts[k][\"event_occur\"][\"{}_{}\".format(key2, key)]\n",
    "        data[\"Occurrence\"][key] = _occ\n",
    "        data[\"Lost packet number\"][key] = _sum1\n",
    "        data[\"Proportion (%)\"][key] = round(_sum1 / (lodicts[k][\"t_loss_num\"][\"overall\"] + 1e-9) * 100, 1)\n",
    "        data[\"Avg loss rate (%)\"][key] = round(_sum1 / (_sum2 + 1e-9) * 100, 3) if _sum2 > 0 else '-'\n",
    "        data[\"Avg loss per event\"][key] = round(_sum1 / (_occ + 1e-9), 3) if _occ > 0 else '-'\n",
    "        # print(key, _sum1, round(_sum1 / 21803 *100, 3), round(_sum1 / (_sum2 + 1e-9) *100, 3), round(_sum1 / (_occ + 1e-9), 3))\n",
    "    data[\"Occurrence\"][\"success\"] = occ\n",
    "    data[\"Lost packet number\"][\"success\"] = sum1\n",
    "    data[\"Proportion (%)\"][\"success\"] = round(sum1 / (lodicts[k][\"t_loss_num\"][\"overall\"] + 1e-9) * 100, 1)\n",
    "    data[\"Avg loss rate (%)\"][\"success\"] = round(sum1 / (sum2 + 1e-9) * 100, 3) if sum2 > 0 else '-'\n",
    "    data[\"Avg loss per event\"][\"success\"] = round(sum1 / (occ + 1e-9), 3) if occ > 0 else '-'\n",
    "\n",
    "    sum1 = 0\n",
    "    sum2 = 0\n",
    "    occ = 0\n",
    "    for key in handover_fail_types:\n",
    "        _sum1 = 0\n",
    "        _sum2 = 0\n",
    "        for key2 in [\"before\", \"during\", \"after\"]:\n",
    "            _sum1 += lodicts[k][\"t_loss_num\"][\"{}_{}\".format(key2, key)]\n",
    "            sum1 += lodicts[k][\"t_loss_num\"][\"{}_{}\".format(key2, key)]\n",
    "            _sum2 += lodicts[k][\"t_pkt_num\"][\"{}_{}\".format(key2, key)]\n",
    "            sum2 += lodicts[k][\"t_pkt_num\"][\"{}_{}\".format(key2, key)]\n",
    "        _occ = lodicts[k][\"event_occur\"][\"{}_{}\".format(key2, key)]\n",
    "        occ += lodicts[k][\"event_occur\"][\"{}_{}\".format(key2, key)]\n",
    "        data[\"Occurrence\"][key] = _occ\n",
    "        data[\"Lost packet number\"][key] = _sum1\n",
    "        data[\"Proportion (%)\"][key] = round(_sum1 / (lodicts[k][\"t_loss_num\"][\"overall\"] + 1e-9) * 100, 1)\n",
    "        data[\"Avg loss rate (%)\"][key] = round(_sum1 / (_sum2 + 1e-9) * 100, 3) if _sum2 > 0 else '-'\n",
    "        data[\"Avg loss per event\"][key] = round(_sum1 / (_occ + 1e-9), 3) if _occ > 0 else '-'\n",
    "        # print(key, _sum1, round(_sum1 / 21803 *100, 3), round(_sum1 / (_sum2 + 1e-9) *100, 3), round(_sum1 / (_occ + 1e-9), 3))\n",
    "    data[\"Occurrence\"][\"fail\"] = occ\n",
    "    data[\"Lost packet number\"][\"fail\"] = sum1\n",
    "    data[\"Proportion (%)\"][\"fail\"] = round(sum1 / (lodicts[k][\"t_loss_num\"][\"overall\"] + 1e-9) * 100, 3)\n",
    "    data[\"Avg loss rate (%)\"][\"fail\"] = round(sum1 / (sum2 + 1e-9) * 100, 1) if sum2 > 0 else '-'\n",
    "    data[\"Avg loss per event\"][\"fail\"] = round(sum1 / (occ + 1e-9), 3) if occ > 0 else '-'\n",
    "\n",
    "    for key in [\"stable\", \"overall\"]:\n",
    "        _sum1 = lodicts[k][\"t_loss_num\"][key]\n",
    "        _sum2 = lodicts[k][\"t_pkt_num\"][key]\n",
    "        data[\"Occurrence\"][key] = '-'\n",
    "        data[\"Lost packet number\"][key] = _sum1\n",
    "        data[\"Proportion (%)\"][key] = round(_sum1 / (lodicts[k][\"t_loss_num\"][\"overall\"] + 1e-9) * 100, 1)\n",
    "        data[\"Avg loss rate (%)\"][key] = round(_sum1 / (_sum2 + 1e-9) * 100, 3) if _sum2 > 0 else '-'\n",
    "        data[\"Avg loss per event\"][key] = '-'\n",
    "        # print(key, _sum1, round(_sum1 / 21803 *100, 3), round(_sum1 / (_sum2 + 1e-9) *100, 3))\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    display(df)\n",
    "    if k == 0:\n",
    "        dfs = df\n",
    "    else:\n",
    "        dfs = pd.concat([dfs, df], axis=1)\n",
    "\n",
    "display(dfs)\n",
    "row_names = handover_types_0 + [\"success\"] + handover_fail_types + [\"fail\"] + [\"stable\", \"overall\"]\n",
    "for row in row_names:\n",
    "    print(*list(dfs.loc[row]), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ** SUCC\n",
    "# fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True)\n",
    "# fig, ax = plt.subplots(nrows=2, ncols=1, sharex=True)\n",
    "fig, ax = plt.subplots(nrows=4, ncols=3, figsize=(11.5, 8), sharex=True)\n",
    "rel = [\"before\", \"during\", \"after\"]\n",
    "_never_occurs = set()\n",
    "\n",
    "fig.suptitle(\"UL Packet Loss Rate Per Event (x=1s): Successful HO\")\n",
    "\n",
    "labels = handover_types_0\n",
    "handles = [None]*len(labels)\n",
    "# cmap = plt.cm.get_cmap('gist_rainbow', len(labels))\n",
    "# colors = [mcolors.to_hex(cmap(i)) for i in range(cmap.N)]\n",
    "# colors = list(mcolors.TABLEAU_COLORS.keys())\n",
    "colors = ['tab:blue', 'tab:purple', 'tab:brown', 'tab:orange', 'tab:green', 'tab:pink', 'tab:red', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "for k, (dev, stg) in enumerate(zip(devices, setnames)):\n",
    "    for i, _rel in enumerate(rel):\n",
    "        ax[k, i].set_title(\"{} {}\".format(stg, _rel).title())\n",
    "        ax[k, i].set_xlabel(\"loss rate (%)\")\n",
    "        for j, type_name in enumerate(handover_types_0):\n",
    "            # data = lodicts[k][\"loss_num\"][\"after_lte_handover\"]\n",
    "            # event_name = \"before_{}\".format(type_name)\n",
    "            event_name = \"{}_{}\".format(_rel, type_name)\n",
    "            data1 = lodicts[k][\"loss_num\"][event_name]\n",
    "            data2 = lodicts[k][\"pkt_num\"][event_name]\n",
    "            data = [round(numer / (denom + 1e-9) * 100, 3) for numer, denom in zip(data1, data2)]\n",
    "            if len(data) == 0:\n",
    "                _never_occurs.add(type_name)\n",
    "                continue\n",
    "            # print(data)\n",
    "            count, bins_count = np.histogram(data, bins=5000)\n",
    "            # print(count, bins_count)\n",
    "            pdf = count / sum(count)\n",
    "            cdf = np.cumsum(pdf)\n",
    "            # ax[k, i].plot(bins_count[1:], cdf, label=event_name)\n",
    "            handles[j], = ax[k, i].plot(bins_count[1:], cdf, c=colors[j])\n",
    "axbox = ax[3, 1].get_position()\n",
    "# print(axbox)\n",
    "labels = [item for key, item in zip(handles, labels) if key != None]\n",
    "handles = [item for item in handles if item != None]\n",
    "fig.legend(\n",
    "    handles=handles, labels=labels,\n",
    "    loc='lower center', bbox_to_anchor=[0, axbox.y0-0.17,1,1], ncol=2)\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(figdir, \"ulloss_HO_{}.png\".format(setnames[3])), bbox_inches='tight')\n",
    "fig.show()\n",
    "print(\"Never Occurs:\", list(_never_occurs))\n",
    "\n",
    "### ** FAIL\n",
    "fig, ax = plt.subplots(nrows=4, ncols=2, figsize=(8.5, 8), sharex=True)\n",
    "rel = [\"before\", \"after\"]\n",
    "_never_occurs = set()\n",
    "\n",
    "fig.suptitle(\"UL Packet Loss Rate Per Event (x=3s): Failed HO\")\n",
    "\n",
    "labels = handover_fail_types\n",
    "handles = [None]*len(labels)\n",
    "# cmap = plt.cm.get_cmap('gist_rainbow', len(labels))\n",
    "# colors = [mcolors.to_hex(cmap(i)) for i in range(cmap.N)]\n",
    "# colors = list(mcolors.TABLEAU_COLORS.keys())\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "for k, (dev, stg) in enumerate(zip(devices, setnames)):\n",
    "    for i, _rel in enumerate(rel):\n",
    "        ax[k, i].set_title(\"{} {}\".format(stg, _rel).title())\n",
    "        ax[k, i].set_xlabel(\"loss rate (%)\")\n",
    "        for j, type_name in enumerate(handover_fail_types):\n",
    "            # data = lodicts[k][\"loss_num\"][\"after_lte_handover\"]\n",
    "            # event_name = \"before_{}\".format(type_name)\n",
    "            event_name = \"{}_{}\".format(_rel, type_name)\n",
    "            data1 = lodicts[k][\"loss_num\"][event_name]\n",
    "            data2 = lodicts[k][\"pkt_num\"][event_name]\n",
    "            data = [round(numer / (denom + 1e-9) * 100, 3) for numer, denom in zip(data1, data2)]\n",
    "            if len(data) == 0:\n",
    "                _never_occurs.add(type_name)\n",
    "                continue\n",
    "            # print(data)\n",
    "            count, bins_count = np.histogram(data, bins=5000)\n",
    "            # print(count, bins_count)\n",
    "            pdf = count / sum(count)\n",
    "            cdf = np.cumsum(pdf)\n",
    "            # ax[k, i].plot(bins_count[1:], cdf, label=event_name)\n",
    "            handles[j], = ax[k, i].plot(bins_count[1:], cdf, c=colors[j])\n",
    "axbox = ax[3, 1].get_position()\n",
    "# print(axbox)\n",
    "labels = [item for key, item in zip(handles, labels) if key != None]\n",
    "handles = [item for item in handles if item != None]\n",
    "fig.legend(\n",
    "    handles=handles, labels=labels,\n",
    "    loc='lower center', bbox_to_anchor=[0, axbox.y0-0.17,1,1], ncol=2)\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(figdir, \"ulloss_Fail_{}.png\".format(setnames[3])), bbox_inches='tight')\n",
    "fig.show()\n",
    "print(\"Never Occurs:\", list(_never_occurs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHT 8+3 types / 4 schemes: DL Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = \"/home/wmnlab/D/database/\"\n",
    "exps = {\n",
    "        \"2022-12-20\": \n",
    "            {  # experiment_name: (number_of_experiment_rounds, list_of_experiment_round)\n",
    "                # \"_Bandlock_Udp_B1_B3\":  (1, [\"#01\",]),\n",
    "                # \"_Bandlock_Udp_B1_B3\":  (2, [\"#01\", \"#02\",]),\n",
    "                # \"_Bandlock_Udp_B1_B3\":  (6, [\"#01\", \"#02\", \"#03\", \"#04\", \"#05\", \"#06\"]),\n",
    "                \"_Bandlock_Udp_B1_B3\":  (5, [\"#01\", \"#03\", \"#04\", \"#05\", \"#06\"]),\n",
    "            },\n",
    "        # \"2022-12-22\":\n",
    "        #     {\n",
    "        #         # \"_Bandlock_Udp_B1_B3\":  (3, [\"#02\", \"#03\", \"#04\",]),\n",
    "        #         # \"_Bandlock_Udp_B3_B7\":  (4, [\"#01\", \"#02\", \"#03\", \"#04\",]),\n",
    "        #         # \"_Bandlock_Udp_B7_B8\":  (4, [\"#01\", \"#02\", \"#03\", \"#04\",]),\n",
    "        #         \"_Bandlock_Udp_B8_B1\":  (4, [\"#01\", \"#02\", \"#03\", \"#04\",]),\n",
    "        #     },\n",
    "}\n",
    "# devices = [\"sm05\",]\n",
    "# setnames = [\"all\",]\n",
    "devices = [\"sm05\", \"sm06\", \"sm07\", \"sm08\"]\n",
    "# setnames = [\"all\", \"b1\", \"b3\", \"b1b3\"]\n",
    "# setnames = [\"all\", \"b3\", \"b7\", \"b3b7\"]\n",
    "# setnames = [\"all\", \"b7\", \"b8\", \"b7b8\"]\n",
    "setnames = [\"all\", \"b8\", \"b1\", \"b8b1\"]\n",
    "\n",
    "handover_types_0 = \"lte_handover,SN_addition,SN_removal,endc_SN_change,endc_MN_change,endc_MNSN_change,lte2endc_MN_change,endc2lte_MN_change\".split(',')\n",
    "handover_types_1 = \"SN_change_only,Intra_frequency,Inter_frequency\".split(',')\n",
    "handover_types_2 = \"SN_change_only,Intra_sector,Intra_eNB,Inter_eNB\".split(',')\n",
    "handover_fail_types = \"scg_failure,radio_link_failure,nas_recovery\".split(',')\n",
    "handover_types = handover_types_0 + handover_fail_types\n",
    "\n",
    "# first_time = 1\n",
    "lodicts = [{}, {}, {}, {}]\n",
    "lodict = {}\n",
    "type_names = []\n",
    "for date, _exps in exps.items():\n",
    "    for expr, (times, traces) in _exps.items():\n",
    "        for k, (dev, stg) in enumerate(zip(devices, setnames)):\n",
    "            # lodict = lodicts[k]\n",
    "            # print(lodict)\n",
    "            print('@'.join([stg, dev]))\n",
    "            print(\"----------------------\")\n",
    "            first_time = 1\n",
    "            for trace in traces:\n",
    "                \"\"\"\"\"\"\n",
    "                source_dir = os.path.join(database, date, expr, dev, trace, \"statistics\", \"classify-bilateral\")\n",
    "                # df = pd.read_csv(os.path.join(source_dir, \"uplk-loss\", \"uplk_loss_classify_1_3.csv\"), index_col=0)\n",
    "                df = pd.read_csv(os.path.join(source_dir, \"dnlk-loss\", \"dnlk_loss_classify_1_3.csv\"), index_col=0)\n",
    "                # df = pd.read_csv(os.path.join(source_dir, \"uplk-loss\", \"uplk_loss_classify_1.csv\"), index_col=0)\n",
    "                \"\"\"\"\"\"\n",
    "                _lodict = df.to_dict()\n",
    "                if first_time:\n",
    "                    # print(lodicts[k])\n",
    "                    lodicts[k] = _lodict\n",
    "                    first_time = 0\n",
    "                    type_names = list(lodicts[k][\"event_occur\"].keys())\n",
    "                    # print(type_names)\n",
    "                    for key in type_names:\n",
    "                        lodicts[k][\"event_dur\"][key] = [float(item) for item in lodicts[k][\"event_dur\"][key].split('@')] if type(lodicts[k][\"event_dur\"][key]) == str else []\n",
    "                        lodicts[k][\"loss_num\"][key] = [int(item) for item in lodicts[k][\"loss_num\"][key].split('@')] if type(lodicts[k][\"loss_num\"][key]) == str else []\n",
    "                        lodicts[k][\"pkt_num\"][key] = [int(item) for item in lodicts[k][\"pkt_num\"][key].split('@')] if type(lodicts[k][\"pkt_num\"][key]) == str else []\n",
    "                    continue\n",
    "                for key in type_names:\n",
    "                    lodicts[k][\"event_occur\"][key] += _lodict[\"event_occur\"][key]\n",
    "                    lodicts[k][\"event_dur\"][key] += [float(item) for item in _lodict[\"event_dur\"][key].split('@')] if type(_lodict[\"event_dur\"][key]) == str else []\n",
    "                    lodicts[k][\"loss_num\"][key] += [int(item) for item in _lodict[\"loss_num\"][key].split('@')] if type(_lodict[\"loss_num\"][key]) == str else []\n",
    "                    lodicts[k][\"pkt_num\"][key] += [int(item) for item in _lodict[\"pkt_num\"][key].split('@')] if type(_lodict[\"pkt_num\"][key]) == str else []\n",
    "                    lodicts[k][\"t_duration\"][key] += _lodict[\"t_duration\"][key]\n",
    "                    lodicts[k][\"t_loss_num\"][key] += _lodict[\"t_loss_num\"][key]\n",
    "                    lodicts[k][\"t_pkt_num\"][key] += _lodict[\"t_pkt_num\"][key]\n",
    "\n",
    "# pprint(lodicts, sort_dicts=False, width=100, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = []\n",
    "for type_name in handover_types:\n",
    "    column_names += [\"before_{}\".format(type_name), \"during_{}\".format(type_name), \"after_{}\".format(type_name)]\n",
    "column_names += [\"unstable\", \"stable\", \"overall\"]\n",
    "handover_types_0\n",
    "handover_fail_types\n",
    "\n",
    "for k, (dev, stg) in enumerate(zip(devices, setnames)):\n",
    "    print('@'.join([stg, dev]))\n",
    "    data = {\"Occurrence\":{},\n",
    "            \"Lost packet number\":{},\n",
    "            \"Proportion (%)\":{},\n",
    "            \"Avg loss rate (%)\":{},\n",
    "            \"Avg loss per event\":{}\n",
    "            }\n",
    "\n",
    "    sum1 = 0\n",
    "    sum2 = 0\n",
    "    occ = 0\n",
    "    for key in handover_types_0:\n",
    "        _sum1 = 0\n",
    "        _sum2 = 0\n",
    "        for key2 in [\"before\", \"during\", \"after\"]:\n",
    "            _sum1 += lodicts[k][\"t_loss_num\"][\"{}_{}\".format(key2, key)]\n",
    "            sum1 += lodicts[k][\"t_loss_num\"][\"{}_{}\".format(key2, key)]\n",
    "            _sum2 += lodicts[k][\"t_pkt_num\"][\"{}_{}\".format(key2, key)]\n",
    "            sum2 += lodicts[k][\"t_pkt_num\"][\"{}_{}\".format(key2, key)]\n",
    "        _occ = lodicts[k][\"event_occur\"][\"{}_{}\".format(key2, key)]\n",
    "        occ += lodicts[k][\"event_occur\"][\"{}_{}\".format(key2, key)]\n",
    "        data[\"Occurrence\"][key] = _occ\n",
    "        data[\"Lost packet number\"][key] = _sum1\n",
    "        data[\"Proportion (%)\"][key] = round(_sum1 / (lodicts[k][\"t_loss_num\"][\"overall\"] + 1e-9) * 100, 1)\n",
    "        data[\"Avg loss rate (%)\"][key] = round(_sum1 / (_sum2 + 1e-9) * 100, 3) if _sum2 > 0 else '-'\n",
    "        data[\"Avg loss per event\"][key] = round(_sum1 / (_occ + 1e-9), 3) if _occ > 0 else '-'\n",
    "        # print(key, _sum1, round(_sum1 / 21803 *100, 3), round(_sum1 / (_sum2 + 1e-9) *100, 3), round(_sum1 / (_occ + 1e-9), 3))\n",
    "    data[\"Occurrence\"][\"success\"] = occ\n",
    "    data[\"Lost packet number\"][\"success\"] = sum1\n",
    "    data[\"Proportion (%)\"][\"success\"] = round(sum1 / (lodicts[k][\"t_loss_num\"][\"overall\"] + 1e-9) * 100, 1)\n",
    "    data[\"Avg loss rate (%)\"][\"success\"] = round(sum1 / (sum2 + 1e-9) * 100, 3) if sum2 > 0 else '-'\n",
    "    data[\"Avg loss per event\"][\"success\"] = round(sum1 / (occ + 1e-9), 3) if occ > 0 else '-'\n",
    "\n",
    "    sum1 = 0\n",
    "    sum2 = 0\n",
    "    occ = 0\n",
    "    for key in handover_fail_types:\n",
    "        _sum1 = 0\n",
    "        _sum2 = 0\n",
    "        for key2 in [\"before\", \"during\", \"after\"]:\n",
    "            _sum1 += lodicts[k][\"t_loss_num\"][\"{}_{}\".format(key2, key)]\n",
    "            sum1 += lodicts[k][\"t_loss_num\"][\"{}_{}\".format(key2, key)]\n",
    "            _sum2 += lodicts[k][\"t_pkt_num\"][\"{}_{}\".format(key2, key)]\n",
    "            sum2 += lodicts[k][\"t_pkt_num\"][\"{}_{}\".format(key2, key)]\n",
    "        _occ = lodicts[k][\"event_occur\"][\"{}_{}\".format(key2, key)]\n",
    "        occ += lodicts[k][\"event_occur\"][\"{}_{}\".format(key2, key)]\n",
    "        data[\"Occurrence\"][key] = _occ\n",
    "        data[\"Lost packet number\"][key] = _sum1\n",
    "        data[\"Proportion (%)\"][key] = round(_sum1 / (lodicts[k][\"t_loss_num\"][\"overall\"] + 1e-9) * 100, 1)\n",
    "        data[\"Avg loss rate (%)\"][key] = round(_sum1 / (_sum2 + 1e-9) * 100, 3) if _sum1 > 0 else '-'\n",
    "        data[\"Avg loss per event\"][key] = round(_sum1 / (_occ + 1e-9), 3) if _sum1 > 0 else '-'\n",
    "        # print(key, _sum1, round(_sum1 / 21803 *100, 3), round(_sum1 / (_sum2 + 1e-9) *100, 3), round(_sum1 / (_occ + 1e-9), 3))\n",
    "    data[\"Occurrence\"][\"fail\"] = occ\n",
    "    data[\"Lost packet number\"][\"fail\"] = sum1\n",
    "    data[\"Proportion (%)\"][\"fail\"] = round(sum1 / (lodicts[k][\"t_loss_num\"][\"overall\"] + 1e-9) * 100, 3)\n",
    "    data[\"Avg loss rate (%)\"][\"fail\"] = round(sum1 / (sum2 + 1e-9) * 100, 1) if sum2 > 0 else '-'\n",
    "    data[\"Avg loss per event\"][\"fail\"] = round(sum1 / (occ + 1e-9), 3) if occ > 0 else '-'\n",
    "\n",
    "    for key in [\"stable\", \"overall\"]:\n",
    "        _sum1 = lodicts[k][\"t_loss_num\"][key]\n",
    "        _sum2 = lodicts[k][\"t_pkt_num\"][key]\n",
    "        data[\"Occurrence\"][key] = '-'\n",
    "        data[\"Lost packet number\"][key] = _sum1\n",
    "        data[\"Proportion (%)\"][key] = round(_sum1 / (lodicts[k][\"t_loss_num\"][\"overall\"] + 1e-9) * 100, 1)\n",
    "        data[\"Avg loss rate (%)\"][key] = round(_sum1 / (_sum2 + 1e-9) * 100, 3) if _sum2 > 0 else '-'\n",
    "        data[\"Avg loss per event\"][key] = '-'\n",
    "        # print(key, _sum1, round(_sum1 / 21803 *100, 3), round(_sum1 / (_sum2 + 1e-9) *100, 3))\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    display(df)\n",
    "    if k == 0:\n",
    "        dfs = df\n",
    "    else:\n",
    "        dfs = pd.concat([dfs, df], axis=1)\n",
    "\n",
    "display(dfs)\n",
    "row_names = handover_types_0 + [\"success\"] + handover_fail_types + [\"fail\"] + [\"stable\", \"overall\"]\n",
    "for row in row_names:\n",
    "    print(*list(dfs.loc[row]), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ** SUCC\n",
    "# fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True)\n",
    "# fig, ax = plt.subplots(nrows=2, ncols=1, sharex=True)\n",
    "fig, ax = plt.subplots(nrows=4, ncols=3, figsize=(11.5, 8), sharex=True)\n",
    "rel = [\"before\", \"during\", \"after\"]\n",
    "_never_occurs = set()\n",
    "\n",
    "fig.suptitle(\"DL Packet Loss Rate Per Event (x=1s): Successful HO\")\n",
    "\n",
    "labels = handover_types_0\n",
    "handles = [None]*len(labels)\n",
    "# cmap = plt.cm.get_cmap('gist_rainbow', len(labels))\n",
    "# colors = [mcolors.to_hex(cmap(i)) for i in range(cmap.N)]\n",
    "# colors = list(mcolors.TABLEAU_COLORS.keys())\n",
    "colors = ['tab:blue', 'tab:purple', 'tab:brown', 'tab:orange', 'tab:green', 'tab:pink', 'tab:red', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "for k, (dev, stg) in enumerate(zip(devices, setnames)):\n",
    "    for i, _rel in enumerate(rel):\n",
    "        ax[k, i].set_title(\"{} {}\".format(stg, _rel).title())\n",
    "        ax[k, i].set_xlabel(\"loss rate (%)\")\n",
    "        for j, type_name in enumerate(handover_types_0):\n",
    "            # data = lodicts[k][\"loss_num\"][\"after_lte_handover\"]\n",
    "            # event_name = \"before_{}\".format(type_name)\n",
    "            event_name = \"{}_{}\".format(_rel, type_name)\n",
    "            data1 = lodicts[k][\"loss_num\"][event_name]\n",
    "            data2 = lodicts[k][\"pkt_num\"][event_name]\n",
    "            data = [round(numer / (denom + 1e-9) * 100, 3) for numer, denom in zip(data1, data2)]\n",
    "            if len(data) == 0:\n",
    "                _never_occurs.add(type_name)\n",
    "                continue\n",
    "            # print(data)\n",
    "            count, bins_count = np.histogram(data, bins=5000)\n",
    "            # print(count, bins_count)\n",
    "            pdf = count / sum(count)\n",
    "            cdf = np.cumsum(pdf)\n",
    "            # ax[k, i].plot(bins_count[1:], cdf, label=event_name)\n",
    "            handles[j], = ax[k, i].plot(bins_count[1:], cdf, c=colors[j])\n",
    "axbox = ax[3, 1].get_position()\n",
    "# print(axbox)\n",
    "labels = [item for key, item in zip(handles, labels) if key != None]\n",
    "handles = [item for item in handles if item != None]\n",
    "fig.legend(\n",
    "    handles=handles, labels=labels,\n",
    "    loc='lower center', bbox_to_anchor=[0, axbox.y0-0.17,1,1], ncol=2)\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(figdir, \"dlloss_HO_{}.png\".format(setnames[3])), bbox_inches='tight')\n",
    "fig.show()\n",
    "print(\"Never Occurs:\", list(_never_occurs))\n",
    "\n",
    "### ** FAIL\n",
    "fig, ax = plt.subplots(nrows=4, ncols=2, figsize=(8.5, 8), sharex=True)\n",
    "rel = [\"before\", \"after\"]\n",
    "_never_occurs = set()\n",
    "\n",
    "fig.suptitle(\"DL Packet Loss Rate Per Event (x=3s): Failed HO\")\n",
    "\n",
    "labels = handover_fail_types\n",
    "handles = [None]*len(labels)\n",
    "# cmap = plt.cm.get_cmap('gist_rainbow', len(labels))\n",
    "# colors = [mcolors.to_hex(cmap(i)) for i in range(cmap.N)]\n",
    "# colors = list(mcolors.TABLEAU_COLORS.keys())\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "for k, (dev, stg) in enumerate(zip(devices, setnames)):\n",
    "    for i, _rel in enumerate(rel):\n",
    "        ax[k, i].set_title(\"{} {}\".format(stg, _rel).title())\n",
    "        ax[k, i].set_xlabel(\"loss rate (%)\")\n",
    "        for j, type_name in enumerate(handover_fail_types):\n",
    "            # data = lodicts[k][\"loss_num\"][\"after_lte_handover\"]\n",
    "            # event_name = \"before_{}\".format(type_name)\n",
    "            event_name = \"{}_{}\".format(_rel, type_name)\n",
    "            data1 = lodicts[k][\"loss_num\"][event_name]\n",
    "            data2 = lodicts[k][\"pkt_num\"][event_name]\n",
    "            data = [round(numer / (denom + 1e-9) * 100, 3) for numer, denom in zip(data1, data2)]\n",
    "            if len(data) == 0:\n",
    "                _never_occurs.add(type_name)\n",
    "                continue\n",
    "            # print(data)\n",
    "            count, bins_count = np.histogram(data, bins=5000)\n",
    "            # print(count, bins_count)\n",
    "            pdf = count / sum(count)\n",
    "            cdf = np.cumsum(pdf)\n",
    "            # ax[k, i].plot(bins_count[1:], cdf, label=event_name)\n",
    "            handles[j], = ax[k, i].plot(bins_count[1:], cdf, c=colors[j])\n",
    "axbox = ax[3, 1].get_position()\n",
    "# print(axbox)\n",
    "labels = [item for key, item in zip(handles, labels) if key != None]\n",
    "handles = [item for item in handles if item != None]\n",
    "fig.legend(\n",
    "    handles=handles, labels=labels,\n",
    "    loc='lower center', bbox_to_anchor=[0, axbox.y0-0.17,1,1], ncol=2)\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(figdir, \"dlloss_Fail_{}.png\".format(setnames[3])), bbox_inches='tight')\n",
    "fig.show()\n",
    "print(\"Never Occurs:\", list(_never_occurs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
