{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from pprint import pprint\n",
    "import csv\n",
    "import itertools as it\n",
    "from prettytable import PrettyTable\n",
    "# from nanrms import *\n",
    "# from handover import *\n",
    "# from mask import *\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pdir = os.path.abspath(os.path.join(os.getcwd(), '..'))  # for jupyter-notebook\n",
    "sys.path.insert(1, pdir)\n",
    "from myutils import *\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'  # 提高 jupyter notebook 的圖形顯示解析度\n",
    "# plt.style.use('ggplot')\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "# pd.set_option('display.max_rows', 200)\n",
    "\n",
    "def set_data(df, mode='pcap', tz=0):\n",
    "    if mode == 'pcap':\n",
    "        df['seq'] = df['seq'].astype('Int32')\n",
    "        # df['rpkg'] = df['rpkg'].astype('Int8')\n",
    "        df['frame_id'] = df['frame_id'].astype('Int32')\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "        df['xmit_time'] = pd.to_datetime(df['xmit_time'])\n",
    "        df['arr_time'] = pd.to_datetime(df['arr_time'])\n",
    "        df['Timestamp_epoch'] = df['Timestamp_epoch'].astype('float32')\n",
    "        df['xmit_time_epoch'] = df['xmit_time_epoch'].astype('float32')\n",
    "        df['arr_time_epoch'] = df['arr_time_epoch'].astype('float32')\n",
    "        df['lost'] = df['lost'].astype('boolean')\n",
    "        df['excl'] = df['excl'].astype('boolean')\n",
    "        df['latency'] = df['latency'].astype('float32')\n",
    "    # E-UTRA & NR\n",
    "    def nr_serv_cel(row):\n",
    "        pos = row.serv_cel_pos\n",
    "        if pos == 255:\n",
    "            return 65535, -160, -50\n",
    "        else:\n",
    "            return row[f'PCI{pos}'], row[f'RSRP{pos}'], row[f'RSRQ{pos}']\n",
    "    if mode == 'eutra':\n",
    "        df = df.rename(columns={\n",
    "            'RSRP(dBm)': 'RSRP',\n",
    "            'RSRQ(dB)': 'RSRQ',\n",
    "            'Serving Cell Index': 'serv_cel_index',\n",
    "            'Number of Neighbor Cells': 'num_neigh_cels',\n",
    "            'Number of Detected Cells': 'num_cels',\n",
    "            }).reindex(['Timestamp','type_id','PCI','RSRP','RSRQ','serv_cel_index','EARFCN','NR_ARFCN',\n",
    "                        'num_cels','num_neigh_cels','serv_cel_pos','PCI0','RSRP0','RSRQ0',\n",
    "                        *df.columns.to_list()[df.columns.get_loc('PCI1'):]], axis=1)\n",
    "        df.loc[df['serv_cel_index'] == '(MI)Unknown', 'serv_cel_index'] = '3_SCell'\n",
    "        df['num_cels'] = df['num_neigh_cels'] + 1\n",
    "        ## set dtypes\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp']) + pd.Timedelta(hours=tz)\n",
    "        df['type_id'] = df['type_id'].astype('category')\n",
    "        df['serv_cel_index'] = df['serv_cel_index'].astype('category')\n",
    "        df['EARFCN'] = df['EARFCN'].astype('Int32')\n",
    "        df['NR_ARFCN'] = df['NR_ARFCN'].astype('Int32')\n",
    "        df['num_cels'] = df['num_cels'].astype('UInt8')\n",
    "        df['num_neigh_cels'] = df['num_neigh_cels'].astype('UInt8')\n",
    "        df['serv_cel_pos'] = df['serv_cel_pos'].astype('UInt8')\n",
    "        for tag in df.columns:\n",
    "            if tag.startswith('PCI'):\n",
    "                df[tag] = df[tag].astype('Int32')\n",
    "            if tag.startswith(('RSRP','RSRQ')):\n",
    "                df[tag] = df[tag].astype('float32')\n",
    "    if mode == 'nr':\n",
    "        if df.empty:\n",
    "            df = pd.DataFrame(columns=['Timestamp','type_id','PCI','RSRP','RSRQ','serv_cel_index','EARFCN','NR_ARFCN',\n",
    "                                       'num_cels','num_neigh_cels','serv_cel_pos','PCI0','RSRP0','RSRQ0'])\n",
    "        else:\n",
    "            df = df.rename(columns={\n",
    "                'Raster ARFCN': 'NR_ARFCN',\n",
    "                'Serving Cell Index': 'serv_cel_pos',\n",
    "                'Num Cells': 'num_cels',\n",
    "                }).reindex(['Timestamp','type_id','PCI','RSRP','RSRQ','serv_cel_index','EARFCN','NR_ARFCN',\n",
    "                            'num_cels','num_neigh_cels','serv_cel_pos','PCI0','RSRP0','RSRQ0',\n",
    "                            *df.columns.to_list()[df.columns.get_loc('PCI1'):]], axis=1)\n",
    "            df.loc[df['serv_cel_pos'] != 255, 'serv_cel_index'] = 'PSCell'\n",
    "            df[['PCI','RSRP','RSRQ']] = df.apply(nr_serv_cel, axis=1, result_type='expand')\n",
    "            df.loc[df['serv_cel_pos'] == 255, 'num_neigh_cels'] = df['num_cels']\n",
    "            df.loc[df['serv_cel_pos'] != 255, 'num_neigh_cels'] = df['num_cels'] - 1\n",
    "        ## set dtypes\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp']) + pd.Timedelta(hours=tz)\n",
    "        df['type_id'] = df['type_id'].astype('category')\n",
    "        df['serv_cel_index'] = df['serv_cel_index'].astype('category')\n",
    "        df['EARFCN'] = df['EARFCN'].astype('Int32')\n",
    "        df['NR_ARFCN'] = df['NR_ARFCN'].astype('Int32')\n",
    "        df['num_cels'] = df['num_cels'].astype('UInt8')\n",
    "        df['num_neigh_cels'] = df['num_neigh_cels'].astype('UInt8')\n",
    "        df['serv_cel_pos'] = df['serv_cel_pos'].astype('UInt8')\n",
    "        for tag in df.columns:\n",
    "            if tag.startswith('PCI'):\n",
    "                df[tag] = df[tag].astype('Int32')\n",
    "            if tag.startswith(('RSRP','RSRQ')):\n",
    "                df[tag] = df[tag].astype('float32')\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = '8_Schemes'\n",
    "datadir = \"/Users/jackbedford/Desktop/MOXA/Code/data\"\n",
    "exps = {\n",
    "    \"2023-05-15/_Bandlock_8_Schemes_Phone\": [\"#{:02d}\".format(i+1) for i in range(4)],\n",
    "}\n",
    "_devices = [\n",
    "    [\"sm00\", \"sm01\", \"sm02\", \"sm03\", \"sm04\", \"sm05\", \"sm06\", \"sm07\",],\n",
    "]\n",
    "_schemes = [\n",
    "    [\"All\", \"B3\", \"B7\", \"B8\", \"B3B7\", \"B3B8\", \"B7B8\", \"LTE\",],\n",
    "]\n",
    "\n",
    "path = './temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -1\n",
    "dfs_ul = []\n",
    "for (exp, traces), devices, schemes in zip(exps.items(), _devices, _schemes):\n",
    "# for date, traces in dates.items():\n",
    "    for trace in traces:\n",
    "        i += 1\n",
    "        dfs_ul.append([])\n",
    "        st, et = [], []\n",
    "        for j, (dev, schm) in enumerate(zip(devices, schemes)):\n",
    "            # data = os.path.join(datadir, date, exp, dev, trace, 'data', 'udp_uplk_loss_latency.csv')\n",
    "            data = os.path.join(datadir, exp, dev, trace, 'data', 'udp_uplk_loss_latency.csv')\n",
    "            print(data, os.path.isfile(data))\n",
    "            # print(i, j)\n",
    "            df = pd.read_csv(data)\n",
    "            df = set_data(df)\n",
    "            dfs_ul[i].append(df.copy())\n",
    "            st.append(df['seq'].array[0])\n",
    "            et.append(df['seq'].array[-1])\n",
    "        st, et = max(st), min(et)\n",
    "        for j, (dev, schm) in enumerate(zip(devices, schemes)):\n",
    "            dfs_ul[i][j] = dfs_ul[i][j][(dfs_ul[i][j]['seq'] >= st) & (dfs_ul[i][j]['seq'] <= et)].reset_index(drop=True)\n",
    "print(len(dfs_ul))\n",
    "\n",
    "i = -1\n",
    "dfs_dl = []\n",
    "for (exp, traces), devices, schemes in zip(exps.items(), _devices, _schemes):\n",
    "# for date, traces in dates.items():\n",
    "    for trace in traces:\n",
    "        i += 1\n",
    "        dfs_dl.append([])\n",
    "        st, et = [], []\n",
    "        for j, (dev, schm) in enumerate(zip(devices, schemes)):\n",
    "            # data = os.path.join(datadir, date, exp, dev, trace, 'data', 'udp_dnlk_loss_latency.csv')\n",
    "            data = os.path.join(datadir, exp, dev, trace, 'data', 'udp_dnlk_loss_latency.csv')\n",
    "            print(data, os.path.isfile(data))\n",
    "            # print(i, j)\n",
    "            df = pd.read_csv(data)\n",
    "            df = set_data(df)\n",
    "            dfs_dl[i].append(df.copy())\n",
    "            st.append(df['seq'].array[0])\n",
    "            et.append(df['seq'].array[-1])\n",
    "        st, et = max(st), min(et)\n",
    "        for j, (dev, schm) in enumerate(zip(devices, schemes)):\n",
    "            dfs_dl[i][j] = dfs_dl[i][j][(dfs_dl[i][j]['seq'] >= st) & (dfs_dl[i][j]['seq'] <= et)].reset_index(drop=True)\n",
    "print(len(dfs_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schms = ['B1', 'B3', 'B7', 'B8']\n",
    "schms = [f'{_schms[i]}+{_schms[j]}' for i in range(len(_schms)) for j in range(i, len(_schms))]\n",
    "\n",
    "xs = list(it.product(_schms, repeat=2))\n",
    "xs = ['+'.join([s[0], s[1]]) for s in xs]\n",
    "xs = np.reshape(xs, (len(_schms), len(_schms)))\n",
    "mtags = masked(xs.tolist(), mask(len(schemes), mode='upper'))\n",
    "mtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = list(it.product(schemes, repeat=2))\n",
    "print(xs)\n",
    "xs = ['+'.join([s[0], s[1]]) if s[0] != s[1] else s[0] for s in xs]\n",
    "# print(xs)\n",
    "xs = np.reshape(xs, (len(schemes), len(schemes)))\n",
    "# print(schemes)\n",
    "# print(xs)\n",
    "mtags = masked(xs.tolist(), mask(len(schemes), mode='upper'))\n",
    "\n",
    "xs = list(it.combinations(schemes, 2))\n",
    "tags = [*schemes, *['+'.join([s[0], s[1]]) for s in xs]]\n",
    "\n",
    "xs = list(it.combinations(range(len(schemes)), 2))\n",
    "xs = [*list(range(len(schemes))), *xs]\n",
    "\n",
    "# print(xs)\n",
    "# print(tags)\n",
    "# display(pd.DataFrame(mtags))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uplink PLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(dfs_ul)\n",
    "devices = _devices[0]\n",
    "\n",
    "table = pd.DataFrame(columns=tags)\n",
    "for i in range(N):\n",
    "    row = []\n",
    "    for x, tag in zip(xs, tags):\n",
    "        if type(x) is not tuple:\n",
    "            if not dfs_ul[i][x].empty:\n",
    "                df = dfs_ul[i][x].copy()\n",
    "                PLR = df.lost.mean() * 100\n",
    "            else:\n",
    "                PLR = np.nan\n",
    "        else:\n",
    "            if not (dfs_ul[i][x[0]].empty or dfs_ul[i][x[1]].empty):\n",
    "                df = pd.merge(dfs_ul[i][x[0]].copy(), dfs_ul[i][x[1]].copy(), on=['seq'], suffixes=('_m','_s')).copy()\n",
    "                PLR = (df.lost_m & df.lost_s).mean() * 100\n",
    "            else:\n",
    "                PLR = np.nan\n",
    "        row.append(PLR)\n",
    "    table = pd.concat([table, pd.DataFrame(row, index=table.columns).T])\n",
    "    ## draw heatmap for one trace\n",
    "    mat = fill_out_matrix(schemes, mtags, pd.DataFrame(row, index=table.columns).T.mean().to_dict())\n",
    "    ax = sns.heatmap(mat.T, annot=True, fmt='.2g', mask=mask(len(schemes)), annot_kws={\"size\": 6})\n",
    "    ax.set_title(f'Uplink PLR (%): {title}, Trace #{i+1}')\n",
    "    plt.savefig(os.path.join(path, f'{title}_ULPLR_{i+1}.png'), bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "mat = fill_out_matrix(schemes, mtags, table.mean().to_dict())\n",
    "ax = sns.heatmap(mat.T, annot=True, fmt='.2g', mask=mask(len(schemes)), annot_kws={\"size\": 6})\n",
    "ax.set_title(f'Uplink PLR (%): {title}')\n",
    "plt.savefig(os.path.join(path, f'{title}_ULPLR.png'), bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(schemes)\n",
    "print(mtags)\n",
    "print(table.mean().to_dict())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downlink PLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(dfs_dl)\n",
    "devices = _devices[0]\n",
    "\n",
    "table = pd.DataFrame(columns=tags)\n",
    "for i in range(N):\n",
    "    row = []\n",
    "    for x, tag in zip(xs, tags):\n",
    "        if type(x) is not tuple:\n",
    "            if not dfs_dl[i][x].empty:\n",
    "                df = dfs_dl[i][x].copy()\n",
    "                PLR = df.lost.mean() * 100\n",
    "            else:\n",
    "                PLR = np.nan\n",
    "        else:\n",
    "            if not (dfs_dl[i][x[0]].empty or dfs_dl[i][x[1]].empty):\n",
    "                df = pd.merge(dfs_dl[i][x[0]].copy(), dfs_dl[i][x[1]].copy(), on=['seq'], suffixes=('_m','_s')).copy()\n",
    "                PLR = (df.lost_m & df.lost_s).mean() * 100\n",
    "            else:\n",
    "                PLR = np.nan\n",
    "        row.append(PLR)\n",
    "    table = pd.concat([table, pd.DataFrame(row, index=table.columns).T])\n",
    "    ## draw heatmap for one trace\n",
    "    mat = fill_out_matrix(schemes, mtags, pd.DataFrame(row, index=table.columns).T.mean().to_dict())\n",
    "    ax = sns.heatmap(mat.T, annot=True, fmt='.2g', mask=mask(len(schemes)), annot_kws={\"size\": 6})\n",
    "    ax.set_title(f'Downlink PLR (%): {title}, Trace #{i+1}')\n",
    "    plt.savefig(os.path.join(path, f'{title}_DLPLR_{i+1}.png'), bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "mat = fill_out_matrix(schemes, mtags, table.mean().to_dict())\n",
    "ax = sns.heatmap(mat.T, annot=True, fmt='.2g', mask=mask(len(schemes)), annot_kws={\"size\": 6})\n",
    "ax.set_title(f'Downlink PLR (%): {title}')\n",
    "plt.savefig(os.path.join(path, f'{title}_DLPLR.png'), bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uplink ELR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(dfs_ul)\n",
    "devices = _devices[0]\n",
    "\n",
    "table = pd.DataFrame(columns=tags)\n",
    "for i in range(N):\n",
    "    row = []\n",
    "    for x, tag in zip(xs, tags):\n",
    "        if type(x) is not tuple:\n",
    "            if not dfs_ul[i][x].empty:\n",
    "                df = dfs_ul[i][x].copy()\n",
    "                ELR = df[~df.lost].excl.mean() * 100\n",
    "            else:\n",
    "                ELR = np.nan\n",
    "        else:\n",
    "            if not (dfs_ul[i][x[0]].empty or dfs_ul[i][x[1]].empty):\n",
    "                df = pd.merge(dfs_ul[i][x[0]].copy(), dfs_ul[i][x[1]].copy(), on=['seq'], suffixes=('_m','_s')).copy()\n",
    "                ELR = (df[~(df.lost_m & df.lost_s)].excl_m & df[~(df.lost_m & df.lost_s)].excl_s).mean() * 100\n",
    "            else:\n",
    "                ELR = np.nan\n",
    "        row.append(ELR)\n",
    "    table = pd.concat([table, pd.DataFrame(row, index=table.columns).T])\n",
    "    ## draw heatmap for one trace\n",
    "    mat = fill_out_matrix(schemes, mtags, pd.DataFrame(row, index=table.columns).T.mean().to_dict())\n",
    "    ax = sns.heatmap(mat.T, annot=True, fmt='.3g', mask=mask(len(schemes)), annot_kws={\"size\": 6})\n",
    "    ax.set_title(f'Uplink ELR (%): {title}, Trace #{i+1}')\n",
    "    plt.savefig(os.path.join(path, f'{title}_ULELR_{i+1}.png'), bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "mat = fill_out_matrix(schemes, mtags, table.mean().to_dict())\n",
    "ax = sns.heatmap(mat.T, annot=True, fmt='.3g', mask=mask(len(schemes)), annot_kws={\"size\": 6})\n",
    "ax.set_title(f'Uplink ELR (%): {title}')\n",
    "plt.savefig(os.path.join(path, f'{title}_ULELR.png'), bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downlink ELR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(dfs_dl)\n",
    "devices = _devices[0]\n",
    "\n",
    "table = pd.DataFrame(columns=tags)\n",
    "for i in range(N):\n",
    "    row = []\n",
    "    for x, tag in zip(xs, tags):\n",
    "        if type(x) is not tuple:\n",
    "            if not dfs_dl[i][x].empty:\n",
    "                df = dfs_dl[i][x].copy()\n",
    "                ELR = df[~df.lost].excl.mean() * 100\n",
    "            else:\n",
    "                ELR = np.nan\n",
    "        else:\n",
    "            if not (dfs_dl[i][x[0]].empty or dfs_dl[i][x[1]].empty):\n",
    "                df = pd.merge(dfs_dl[i][x[0]].copy(), dfs_dl[i][x[1]].copy(), on=['seq'], suffixes=('_m','_s')).copy()\n",
    "                ELR = (df[~(df.lost_m & df.lost_s)].excl_m & df[~(df.lost_m & df.lost_s)].excl_s).mean() * 100\n",
    "            else:\n",
    "                ELR = np.nan\n",
    "        row.append(ELR)\n",
    "    table = pd.concat([table, pd.DataFrame(row, index=table.columns).T])\n",
    "    ## draw heatmap for one trace\n",
    "    mat = fill_out_matrix(schemes, mtags, pd.DataFrame(row, index=table.columns).T.mean().to_dict())\n",
    "    ax = sns.heatmap(mat.T, annot=True, fmt='.3g', mask=mask(len(schemes)), annot_kws={\"size\": 6})\n",
    "    ax.set_title(f'Downlink ELR (%): {title}, Trace #{i+1}')\n",
    "    plt.savefig(os.path.join(path, f'{title}_DLELR_{i+1}.png'), bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "mat = fill_out_matrix(schemes, mtags, table.mean().to_dict())\n",
    "ax = sns.heatmap(mat.T, annot=True, fmt='.3g', mask=mask(len(schemes)), annot_kws={\"size\": 6})\n",
    "ax.set_title(f'Downlink ELR (%): {title}')\n",
    "plt.savefig(os.path.join(path, f'{title}_DLELR.png'), bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uplink Latency (except for lost & excl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(dfs_ul)\n",
    "devices = _devices[0]\n",
    "\n",
    "table = pd.DataFrame(columns=tags)\n",
    "for i in range(N):\n",
    "    row = []\n",
    "    for x, tag in zip(xs, tags):\n",
    "        if type(x) is not tuple:\n",
    "            if not dfs_ul[i][x].empty:\n",
    "                df = dfs_ul[i][x].copy()\n",
    "                latency = df[~df.excl].latency.mean() * 1000\n",
    "            else:\n",
    "                latency = np.nan\n",
    "        else:\n",
    "            if not (dfs_ul[i][x[0]].empty or dfs_ul[i][x[1]].empty):\n",
    "                df = pd.merge(dfs_ul[i][x[0]].copy(), dfs_ul[i][x[1]].copy(), on=['seq'], suffixes=('_m','_s')).copy()\n",
    "                latency = df.loc[~(df.excl_m & df.excl_s), ['latency_m','latency_s']].min(axis=1).mean() * 1000\n",
    "            else:\n",
    "                latency = np.nan\n",
    "        row.append(latency)\n",
    "    table = pd.concat([table, pd.DataFrame(row, index=table.columns).T])\n",
    "    ## draw heatmap for one trace\n",
    "    mat = fill_out_matrix(schemes, mtags, pd.DataFrame(row, index=table.columns).T.mean().to_dict())\n",
    "    ax = sns.heatmap(mat.T, annot=True, fmt='.3g', mask=mask(len(schemes)), annot_kws={\"size\": 6})\n",
    "    ax.set_title(f'Uplink Latency w/o excl (ms): {title}, Trace #{i+1}')\n",
    "    plt.savefig(os.path.join(path, f'{title}_ULLat1_{i+1}.png'), bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "mat = fill_out_matrix(schemes, mtags, table.mean().to_dict())\n",
    "ax = sns.heatmap(mat.T, annot=True, fmt='.3g', mask=mask(len(schemes)), annot_kws={\"size\": 6})\n",
    "ax.set_title(f'Uplink Latency w/o excl (ms): {title}')\n",
    "plt.savefig(os.path.join(path, f'{title}_ULLat1.png'), bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downlink Latency (except for lost & excl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(dfs_dl)\n",
    "devices = _devices[0]\n",
    "\n",
    "table = pd.DataFrame(columns=tags)\n",
    "for i in range(N):\n",
    "    row = []\n",
    "    for x, tag in zip(xs, tags):\n",
    "        if type(x) is not tuple:\n",
    "            if not dfs_dl[i][x].empty:\n",
    "                df = dfs_dl[i][x].copy()\n",
    "                latency = df[~df.excl].latency.mean() * 1000\n",
    "            else:\n",
    "                latency = np.nan\n",
    "        else:\n",
    "            if not (dfs_dl[i][x[0]].empty or dfs_dl[i][x[1]].empty):\n",
    "                df = pd.merge(dfs_dl[i][x[0]].copy(), dfs_dl[i][x[1]].copy(), on=['seq'], suffixes=('_m','_s')).copy()\n",
    "                latency = df.loc[~(df.excl_m & df.excl_s), ['latency_m','latency_s']].min(axis=1).mean() * 1000\n",
    "            else:\n",
    "                latency = np.nan\n",
    "        row.append(latency)\n",
    "    table = pd.concat([table, pd.DataFrame(row, index=table.columns).T])\n",
    "    ## draw heatmap for one trace\n",
    "    mat = fill_out_matrix(schemes, mtags, pd.DataFrame(row, index=table.columns).T.mean().to_dict())\n",
    "    ax = sns.heatmap(mat.T, annot=True, fmt='.3g', mask=mask(len(schemes)), annot_kws={\"size\": 6})\n",
    "    ax.set_title(f'Downlink Latency w/o excl (ms): {title}, Trace #{i+1}')\n",
    "    plt.savefig(os.path.join(path, f'{title}_DLLat1_{i+1}.png'), bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "mat = fill_out_matrix(schemes, mtags, table.mean().to_dict())\n",
    "ax = sns.heatmap(mat.T, annot=True, fmt='.3g', mask=mask(len(schemes)), annot_kws={\"size\": 6})\n",
    "ax.set_title(f'Downlink Latency w/o excl (ms): {title}')\n",
    "plt.savefig(os.path.join(path, f'{title}_DLLat1.png'), bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uplink Latency (except for lost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(dfs_ul)\n",
    "devices = _devices[0]\n",
    "\n",
    "table = pd.DataFrame(columns=tags)\n",
    "for i in range(N):\n",
    "    row = []\n",
    "    for x, tag in zip(xs, tags):\n",
    "        if type(x) is not tuple:\n",
    "            if not dfs_ul[i][x].empty:\n",
    "                df = dfs_ul[i][x].copy()\n",
    "                latency = df[~df.lost].latency.mean() * 1000\n",
    "            else:\n",
    "                latency = np.nan\n",
    "        else:\n",
    "            if not (dfs_ul[i][x[0]].empty or dfs_ul[i][x[1]].empty):\n",
    "                df = pd.merge(dfs_ul[i][x[0]].copy(), dfs_ul[i][x[1]].copy(), on=['seq'], suffixes=('_m','_s')).copy()\n",
    "                latency = df.loc[~(df.lost_m & df.lost_s), ['latency_m','latency_s']].min(axis=1).mean() * 1000\n",
    "            else:\n",
    "                latency = np.nan\n",
    "        row.append(latency)\n",
    "    table = pd.concat([table, pd.DataFrame(row, index=table.columns).T])\n",
    "    ## draw heatmap for one trace\n",
    "    mat = fill_out_matrix(schemes, mtags, pd.DataFrame(row, index=table.columns).T.mean().to_dict())\n",
    "    ax = sns.heatmap(mat.T, annot=True, fmt='.3g', mask=mask(len(schemes)), annot_kws={\"size\": 6})\n",
    "    ax.set_title(f'Uplink Latency (ms): {title}, Trace #{i+1}')\n",
    "    plt.savefig(os.path.join(path, f'{title}_ULLat2_{i+1}.png'), bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "mat = fill_out_matrix(schemes, mtags, table.mean().to_dict())\n",
    "ax = sns.heatmap(mat.T, annot=True, fmt='.3g', mask=mask(len(schemes)), annot_kws={\"size\": 6})\n",
    "ax.set_title(f'Uplink Latency (ms): {title}')\n",
    "plt.savefig(os.path.join(path, f'{title}_ULLat2.png'), bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downlink Latency (except for lost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(dfs_dl)\n",
    "devices = _devices[0]\n",
    "\n",
    "table = pd.DataFrame(columns=tags)\n",
    "for i in range(N):\n",
    "    row = []\n",
    "    for x, tag in zip(xs, tags):\n",
    "        if type(x) is not tuple:\n",
    "            if not dfs_dl[i][x].empty:\n",
    "                df = dfs_dl[i][x].copy()\n",
    "                latency = df[~df.lost].latency.mean() * 1000\n",
    "            else:\n",
    "                latency = np.nan\n",
    "        else:\n",
    "            if not (dfs_dl[i][x[0]].empty or dfs_dl[i][x[1]].empty):\n",
    "                df = pd.merge(dfs_dl[i][x[0]].copy(), dfs_dl[i][x[1]].copy(), on=['seq'], suffixes=('_m','_s')).copy()\n",
    "                latency = df.loc[~(df.lost_m & df.lost_s), ['latency_m','latency_s']].min(axis=1).mean() * 1000\n",
    "            else:\n",
    "                latency = np.nan\n",
    "        row.append(latency)\n",
    "    table = pd.concat([table, pd.DataFrame(row, index=table.columns).T])\n",
    "    ## draw heatmap for one trace\n",
    "    mat = fill_out_matrix(schemes, mtags, pd.DataFrame(row, index=table.columns).T.mean().to_dict())\n",
    "    ax = sns.heatmap(mat.T, annot=True, fmt='.3g', mask=mask(len(schemes)), annot_kws={\"size\": 6})\n",
    "    ax.set_title(f'Downlink Latency (ms): {title}, Trace #{i+1}')\n",
    "    plt.savefig(os.path.join(path, f'{title}_DLLat2_{i+1}.png'), bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "mat = fill_out_matrix(schemes, mtags, table.mean().to_dict())\n",
    "ax = sns.heatmap(mat.T, annot=True, fmt='.3g', mask=mask(len(schemes)), annot_kws={\"size\": 6})\n",
    "ax.set_title(f'Downlink Latency (ms): {title}')\n",
    "plt.savefig(os.path.join(path, f'{title}_DLLat2.png'), bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moxa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
