{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Basic Tools\n",
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv, json\n",
    "import math, random, ast, swifter\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import portion as P\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from pprint import pprint\n",
    "from collections import namedtuple\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Set plot style\n",
    "# plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: unify data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_data(df, mode='pcap', tz=0):\n",
    "    def nr_serv_cel(row):\n",
    "        pos = row.serv_cel_pos\n",
    "        if pos == 255:\n",
    "            return 65535, -160, -50\n",
    "        else:\n",
    "            return row[f'PCI{pos}'], row[f'RSRP{pos}'], row[f'RSRQ{pos}']\n",
    "    \n",
    "    if mode == 'pcap':\n",
    "        common_column_names = ['seq', 'rpkg', 'frame_id', 'Timestamp', 'lost', 'excl', 'latency', 'xmit_time', 'arr_time']\n",
    "        \n",
    "        if df.empty:\n",
    "            return pd.DataFrame(columns=common_column_names)\n",
    "        \n",
    "        date_columns = ['Timestamp', 'xmit_time', 'arr_time']\n",
    "        df[date_columns] = df[date_columns].apply(pd.to_datetime)\n",
    "        df[['seq', 'rpkg', 'frame_id']] = df[['seq', 'rpkg', 'frame_id']].astype('Int32')\n",
    "        df[['latency']] = df[['latency']].astype('float32')\n",
    "        df[['lost', 'excl']] = df[['lost', 'excl']].astype('boolean')\n",
    "\n",
    "    if mode in ['lte', 'nr']:\n",
    "        common_column_names = [\n",
    "            'Timestamp', 'type_id', 'PCI', 'RSRP', 'RSRQ', 'serv_cel_index', 'EARFCN', 'NR_ARFCN', \n",
    "            'num_cels', 'num_neigh_cels', 'serv_cel_pos', 'PCI0', 'RSRP0', 'RSRQ0',\n",
    "        ]\n",
    "        \n",
    "        if df.empty:\n",
    "            return pd.DataFrame(columns=common_column_names)\n",
    "        \n",
    "        if mode == 'lte':\n",
    "            columns_mapping = {\n",
    "                'RSRP(dBm)': 'RSRP',\n",
    "                'RSRQ(dB)': 'RSRQ',\n",
    "                'Serving Cell Index': 'serv_cel_index',\n",
    "                'Number of Neighbor Cells': 'num_neigh_cels',\n",
    "                'Number of Detected Cells': 'num_cels',\n",
    "            }\n",
    "            columns_order = [*common_column_names, *df.columns[df.columns.get_loc('PCI1'):].tolist()]\n",
    "            \n",
    "            df = df.rename(columns=columns_mapping).reindex(columns_order, axis=1)\n",
    "            df['serv_cel_index'] = np.where(df['serv_cel_index'] == '(MI)Unknown', '3_SCell', df['serv_cel_index'])\n",
    "            df['num_cels'] = df['num_neigh_cels'] + 1\n",
    "            df['type_id'] = 'LTE_PHY'\n",
    "\n",
    "        if mode == 'nr':\n",
    "            columns_mapping = {\n",
    "                'Raster ARFCN': 'NR_ARFCN',\n",
    "                'Serving Cell Index': 'serv_cel_pos',\n",
    "                'Num Cells': 'num_cels',\n",
    "            }\n",
    "            columns_order = [*common_column_names, *df.columns[df.columns.get_loc('PCI1'):].tolist()]\n",
    "            \n",
    "            df = df.rename(columns=columns_mapping).reindex(columns_order, axis=1)\n",
    "            df[['PCI', 'RSRP', 'RSRQ']] = df.apply(nr_serv_cel, axis=1, result_type='expand')\n",
    "            df['serv_cel_index'] = np.where(df['serv_cel_pos'] == 255, df['serv_cel_index'], 'PSCell')\n",
    "            df['num_neigh_cels'] = np.where(df['serv_cel_pos'] == 255, df['num_cels'], df['num_cels'] - 1)\n",
    "            df['type_id'] = '5G_NR_ML1'\n",
    "        \n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp']) + pd.Timedelta(hours=tz)\n",
    "        df[['type_id', 'serv_cel_index']] = df[['type_id', 'serv_cel_index']].astype('category')\n",
    "        df[['EARFCN', 'NR_ARFCN']] = df[['EARFCN', 'NR_ARFCN']].astype('Int32')\n",
    "        df[['num_cels', 'num_neigh_cels', 'serv_cel_pos']] = df[['num_cels', 'num_neigh_cels', 'serv_cel_pos']].astype('UInt8')\n",
    "\n",
    "        for tag in df.columns:\n",
    "            if tag.startswith('PCI'):\n",
    "                df[tag] = df[tag].astype('Int32')\n",
    "            if tag.startswith(('RSRP', 'RSRQ')):\n",
    "                df[tag] = df[tag].astype('float32')\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: parse handover event\n",
    "### Stage 1: rrc to handover dictionary\n",
    "\n",
    "- Sheng-Ru latest version (2023-09-25)\n",
    "    - 2023-10-27: add try ... except ... statement\n",
    "    - 2023-11-13: add NR ARFCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "triggered_events = {\n",
    "    'HO':     ['LTEH', 'ENBH', 'MCGH', 'MNBH', 'SCGM', 'SCGA', 'SCGR-I', 'SCGC-I', 'SCGR-II', 'SCGC-II', ],\n",
    "    'RLF':    ['MCGF', 'NASR', 'SCGF', ],\n",
    "    'Others': ['CXNS', 'CXNR', 'SCLA', ],\n",
    "}\n",
    "\n",
    "rlf_events = {\n",
    "    'MCGF': ['reconfigurationFailure (0)', 'handoverFailure (1)', 'otherFailure (2)', ],\n",
    "    'NASR': ['reconfigurationFailure (0)', 'handoverFailure (1)', 'otherFailure (2)', ],\n",
    "    'SCGF': ['t310-Expiry (0)', 'randomAccessProblem (1)', 'rlc-MaxNumRetx (2)', 'synchReconfigFailureSCG (3)', 'scg-ReconfigFailure (4)', 'srb3-IntegrityFailure (5)', 'other-r16 (6)', ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mi_ho(df, tz=8):\n",
    "\n",
    "    # df = pd.read_csv(f)\n",
    "    df[\"Timestamp\"] = df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x) + dt.timedelta(hours=tz))\n",
    "    nr_pci = 'O'\n",
    "    nr_arfcn = 0\n",
    "    scells = []\n",
    "\n",
    "    def NR_OTA(idx):\n",
    "\n",
    "        if df[\"type_id\"].iloc[idx] == \"5G_NR_RRC_OTA_Packet\": return True\n",
    "        else: return False\n",
    "    \n",
    "    def LTE_SERV_INFO(idx):\n",
    "\n",
    "        if df[\"type_id\"].iloc[idx] == \"LTE_RRC_Serv_Cell_Info\": return True\n",
    "        else: return False\n",
    "    \n",
    "\n",
    "    def find_1st_after(start_idx, target, look_after=1):\n",
    "        for j in range(start_idx, len(df)):\n",
    "            t_ = df[\"Timestamp\"].iloc[j]\n",
    "            if NR_OTA(j) or LTE_SERV_INFO(j):\n",
    "                continue\n",
    "            if (t_ - t).total_seconds() > look_after:\n",
    "                return None, None\n",
    "            if df[target].iloc[j] not in [0,'0'] and not np.isnan(df[target].iloc[j]):\n",
    "                return t_, j\n",
    "        return None, None\n",
    "    \n",
    "    def find_1st_before(start_idx, target, look_before=1):\n",
    "        for j in range(start_idx, -1, -1):\n",
    "            t_ = df[\"Timestamp\"].iloc[j]\n",
    "            if NR_OTA(j) or LTE_SERV_INFO(j):\n",
    "                continue\n",
    "            if (t - t_).total_seconds() > look_before:\n",
    "                return None, None\n",
    "            if df[target].iloc[j] not in [0,'0'] and not np.isnan(df[target].iloc[j]):\n",
    "                return t_, j\n",
    "        return None, None\n",
    "    \n",
    "    def find_1st_before_with_special_value(start_idx, target, target_value, look_before=1):\n",
    "        for j in range(start_idx, -1, -1):\n",
    "            t_ = df[\"Timestamp\"].iloc[j]\n",
    "            if NR_OTA(j) or LTE_SERV_INFO(j):\n",
    "                continue\n",
    "            if (t - t_).total_seconds() > look_before:\n",
    "                return None, None\n",
    "            if df[target].iloc[j] in [target_value] and not np.isnan(df[target].iloc[j]):\n",
    "                return t_, j\n",
    "        return None, None\n",
    "    \n",
    "    def find_in_D_exact(targets):\n",
    "\n",
    "        l = []\n",
    "        # In l : (second, ho_type)\n",
    "        for target in targets:\n",
    "            for ho in D[target]:\n",
    "                l.append(((t - ho.start).total_seconds(), target))\n",
    "\n",
    "        if len(l) != 0:\n",
    "            for x in l:\n",
    "                if (x[0]== 0):\n",
    "                    return x[1]\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def find_in_D_first_before(targets, look_before=1):\n",
    "\n",
    "        l = []\n",
    "        # In l : (second, ho_type)\n",
    "        for target in targets:\n",
    "            for ho in D[target]:\n",
    "                try:\n",
    "                    l.append(((t - ho.end).total_seconds(), target, ho))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        if len(l) != 0:\n",
    "            closest = min(filter(lambda x: x[0] > 0, l), key=lambda x: x[0])\n",
    "            if 0 <= closest[0] < look_before:\n",
    "                return closest[1], closest[2]\n",
    "        \n",
    "        return None, None\n",
    "    \n",
    "    HO = namedtuple('HO',['start', 'end', 'others', 'trans'], defaults=[None,None,'',''])\n",
    "    \n",
    "    D = {\n",
    "        'Conn_Rel':[], \n",
    "        'Conn_Req':[], # Setup\n",
    "        'LTE_HO': [], # LTE -> newLTE\n",
    "        'MN_HO': [], # LTE + NR -> newLTE + NR\n",
    "        'MN_HO_to_eNB': [], # LTE + NR -> newLTE\n",
    "        'SN_setup': [], # LTE -> LTE + NR => NR setup\n",
    "        'SN_Rel': [], # LTE + NR -> LTE\n",
    "        'SN_HO': [], # LTE + NR -> LTE + newNR  \n",
    "        'RLF_II': [],\n",
    "        'RLF_III': [],\n",
    "        'SCG_RLF': [],\n",
    "        'Add_SCell': [],\n",
    "        }\n",
    "\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        # Pass NR RRC packet. In NSA mode, LTE RRC packet include NR packet message.\n",
    "        if NR_OTA(i) or LTE_SERV_INFO(i):\n",
    "            continue\n",
    "\n",
    "        others = ''\n",
    "        t = df[\"Timestamp\"].iloc[i]\n",
    "\n",
    "        if df[\"rrcConnectionRelease\"].iloc[i] == 1:\n",
    "            D['Conn_Rel'].append(HO(start=t))\n",
    "            nr_pci = 'O'\n",
    "\n",
    "        if df[\"rrcConnectionRequest\"].iloc[i] == 1:\n",
    "            \n",
    "            # Define end of rrcConnectionRequest to be rrcConnectionReconfigurationComplete or securityModeComplete.\n",
    "            a = find_1st_after(i, 'rrcConnectionReconfigurationComplete',look_after=2)[0]\n",
    "            b = find_1st_after(i, 'securityModeComplete',look_after=2)[0]\n",
    "            if a is None and b is None: end = None\n",
    "            elif a is None and b is not None: end = b\n",
    "            elif a is not None and b is None: end = a \n",
    "            else: end = a if a > b else b\n",
    "            \n",
    "            serv_cell, serv_freq = df[\"PCI\"].iloc[i], int(df[\"Freq\"].iloc[i])\n",
    "            trans = f'? -> ({serv_cell}, {serv_freq})'\n",
    "            D['Conn_Req'].append(HO(start=t,end=end,trans=trans))\n",
    "            nr_pci = 'O'\n",
    "        \n",
    "        if df[\"lte-rrc.t304\"].iloc[i] == 1:\n",
    "            \n",
    "            end, _ = find_1st_after(i, 'rrcConnectionReconfigurationComplete')\n",
    "            serv_cell, target_cell = df[\"PCI\"].iloc[i], int(df['lte_targetPhysCellId'].iloc[i])\n",
    "            serv_freq, target_freq = int(df[\"Freq\"].iloc[i]), int(df['dl-CarrierFreq'].iloc[i])\n",
    "\n",
    "            if df[\"SCellToAddMod-r10\"].iloc[i] == 1:\n",
    "                n =len(str(df[\"SCellIndex-r10.1\"].iloc[i]).split('@'))\n",
    "                others += f' Set up {n} SCell.'\n",
    "            else:\n",
    "                scells = []\n",
    "            \n",
    "            if serv_freq != target_freq:\n",
    "                a,b = find_1st_before(i, \"rrcConnectionReestablishmentRequest\", 1)\n",
    "                others += \" Inter frequency HO.\"\n",
    "                if a is not None:\n",
    "                    others += \" Near after RLF.\"\n",
    "                \n",
    "            if df[\"nr-rrc.t304\"].iloc[i] == 1 and df[\"dualConnectivityPHR: setup (1)\"].iloc[i] == 1:\n",
    "                \n",
    "                if serv_cell == target_cell and serv_freq == target_freq:\n",
    "\n",
    "                    a, _ = find_1st_before(i, \"rrcConnectionReestablishmentRequest\", 2)\n",
    "                    \n",
    "                    if a is not None:\n",
    "\n",
    "                        ho_type, ho = find_in_D_first_before(['RLF_II', 'RLF_III'], 2)\n",
    "                        try:\n",
    "                            others += f' Near after RLF of trans: {ho.trans}.'\n",
    "                        except:\n",
    "                            others += f' Near after RLF.'\n",
    "\n",
    "                    else:\n",
    "                        \n",
    "                        ho_type, _ = find_in_D_first_before(['MN_HO_to_eNB', 'SN_Rel'], 2)\n",
    "                        if ho_type is not None:\n",
    "                            others += f' Near after {ho_type}.'\n",
    "                    orig_serv = (nr_pci, nr_arfcn) if nr_pci != 'O' else 'O'\n",
    "                    nr_pci = int(df['nr_physCellId'].iloc[i])\n",
    "                    nr_arfcn = int(df['absoluteFrequencySSB'].iloc[i])\n",
    "                    trans = f'({serv_cell}, {serv_freq}) | {orig_serv} -> ({nr_pci}, {nr_arfcn})'\n",
    "                    D['SN_setup'].append(HO(start=t, end=end, others=others, trans=trans))\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    nr_pci = int(df['nr_physCellId'].iloc[i])\n",
    "                    trans = f'({serv_cell}, {serv_freq}) -> ({target_cell}, {target_freq}) | ({nr_pci}, {nr_arfcn})'\n",
    "                    D['MN_HO'].append(HO(start=t, end=end, others=others, trans=trans))\n",
    "\n",
    "            else:\n",
    "                \n",
    "                if serv_cell == target_cell and serv_freq == target_freq:\n",
    "\n",
    "                    a, b = find_1st_before(i, \"scgFailureInformationNR-r15\")\n",
    "                    if a is not None:\n",
    "                        others += \" Caused by scg-failure.\"\n",
    "                    \n",
    "                    orig_serv = (nr_pci, nr_arfcn) if nr_pci != 'O' else 'O'\n",
    "                    nr_pci = 'O'\n",
    "                    trans = f'({serv_cell}, {serv_freq}) | {orig_serv} -> {nr_pci}'\n",
    "                    D['SN_Rel'].append(HO(start=t, end=end, others=others, trans=trans))\n",
    "                    \n",
    "                else:\n",
    "\n",
    "                    a, _ = find_1st_before(i,\"rrcConnectionSetup\",3)\n",
    "                    if a is not None:\n",
    "                        others += ' Near After connection setup.'\n",
    "                    if nr_pci == 'O':\n",
    "                        trans = f'({serv_cell}, {serv_freq}) -> ({target_cell}, {target_freq}) | {nr_pci}'\n",
    "                        D['LTE_HO'].append(HO(start=t, end=end, others=others, trans=trans))\n",
    "                    else:\n",
    "                        orig_serv = (nr_pci, nr_arfcn) if nr_pci != 'O' else 'O'\n",
    "                        nr_pci = 'O'\n",
    "                        trans = f'({serv_cell}, {serv_freq}) -> ({target_cell}, {target_freq}) | {orig_serv} -> {nr_pci}'\n",
    "                        D['MN_HO_to_eNB'].append(HO(start=t, end=end, others=others, trans=trans))\n",
    "\n",
    "\n",
    "        if df[\"nr-rrc.t304\"].iloc[i] == 1 and not df[\"dualConnectivityPHR: setup (1)\"].iloc[i] == 1:\n",
    "\n",
    "            end, _ = find_1st_after(i,'rrcConnectionReconfigurationComplete')\n",
    "        \n",
    "            serv_cell, serv_freq = df[\"PCI\"].iloc[i], int(df[\"Freq\"].iloc[i])\n",
    "            orig_serv = (nr_pci, nr_arfcn) if nr_pci != 'O' else 'O'\n",
    "            nr_pci = int(df['nr_physCellId'].iloc[i])\n",
    "            nr_arfcn = int(df['absoluteFrequencySSB'].iloc[i])\n",
    "            trans = f'({serv_cell}, {serv_freq}) | {orig_serv} -> ({nr_pci}, {nr_arfcn})'\n",
    "            D['SN_HO'].append(HO(start=t,end=end,trans=trans))\n",
    "\n",
    "\n",
    "        if df[\"rrcConnectionReestablishmentRequest\"].iloc[i] == 1:\n",
    "\n",
    "            end1, _ = find_1st_after(i, 'rrcConnectionReestablishmentComplete', look_after=1)\n",
    "            b, _ = find_1st_after(i, 'rrcConnectionReestablishmentReject', look_after=1)\n",
    "            end2, _ = find_1st_after(i, 'securityModeComplete',look_after=3)\n",
    "\n",
    "            others += ' ' + df[\"reestablishmentCause\"].iloc[i] + '.'\n",
    "            scells = []\n",
    "\n",
    "            c, _ = find_1st_before(i, 'scgFailureInformationNR-r15', 1)\n",
    "            if c != None:\n",
    "                others  += ' caused by scgfailure.'\n",
    "                \n",
    "            serv_cell, rlf_cell = df[\"PCI\"].iloc[i], int(df['physCellId.3'].iloc[i])\n",
    "            serv_freq = int(df['Freq'].iloc[i])\n",
    "            \n",
    "            # Type II & Type III\n",
    "            if end1 is not None: \n",
    "\n",
    "                orig_serv = (nr_pci, nr_arfcn) if nr_pci != 'O' else 'O'\n",
    "                nr_pci = 'O'\n",
    "                _, idx = find_1st_before_with_special_value(i, 'PCI', rlf_cell, look_before=10)\n",
    "                try:\n",
    "                    rlf_freq = int(df['Freq'].iloc[idx])\n",
    "                except:\n",
    "                    rlf_freq = 0\n",
    "                trans = f'({rlf_cell}, {rlf_freq}) -> ({serv_cell}, {serv_freq}) | {orig_serv} -> {nr_pci}'\n",
    "                D['RLF_II'].append(HO(start=t,end=end1,others=others,trans=trans))\n",
    "\n",
    "            elif b is not None and end2 is not None:\n",
    "                \n",
    "                orig_serv = (nr_pci, nr_arfcn) if nr_pci != 'O' else 'O'\n",
    "                nr_pci = 'O'\n",
    "                _, idx = find_1st_before_with_special_value(i, 'PCI', rlf_cell, look_before=10)\n",
    "                try:\n",
    "                    rlf_freq = int(df['Freq'].iloc[idx])\n",
    "                except:\n",
    "                    rlf_freq = 0\n",
    "\n",
    "                _, idx = find_1st_after(i, \"rrcConnectionRequest\", 2)\n",
    "                recon_cell, recon_freq = df['PCI'].iloc[idx], int(float(df['Freq'].iloc[idx]))\n",
    "                \n",
    "                trans = f'({rlf_cell}, {rlf_freq}) -> ({recon_cell}, {recon_freq}) | {orig_serv} -> {nr_pci}'\n",
    "                D['RLF_III'].append(HO(start=t,end=end2,others=others,trans=trans))\n",
    "                \n",
    "            else:\n",
    "                others+=' No end.'\n",
    "                D['RLF_II'].append(HO(start=t,others=others))\n",
    "                print('No end for RLF')\n",
    "\n",
    "        if df[\"scgFailureInformationNR-r15\"].iloc[i] == 1:\n",
    "\n",
    "            others += ' ' + df[\"failureType-r15\"].iloc[i] + '.'\n",
    "            a, idx1 = find_1st_after(i, \"rrcConnectionReestablishmentRequest\", look_after=1)\n",
    "            b, idx2 = find_1st_after(i, \"lte-rrc.t304\", look_after=10)\n",
    "\n",
    "            if a is not None:\n",
    "\n",
    "                end1, _ = find_1st_after(idx1, 'rrcConnectionReestablishmentComplete', look_after=1)\n",
    "                b, _ = find_1st_after(idx1, 'rrcConnectionReestablishmentReject', look_after=1)\n",
    "                end2 = find_1st_after(idx1, 'securityModeComplete',look_after=3)[0]\n",
    "\n",
    "                others += ' Result in rrcReestablishment.'\n",
    "                    \n",
    "                # Type II & Type III Result\n",
    "                if end1 is not None: \n",
    "                    D['SCG_RLF'].append(HO(start=t,end=end1,others=others))\n",
    "                elif b is not None and end2 is not None: \n",
    "                    D['SCG_RLF'].append(HO(start=t,end=end2,others=others))\n",
    "                else:\n",
    "                    others += ' No end.'\n",
    "                    D['SCG_RLF'].append(HO(start=t,others=others))\n",
    "                    print('No end for scg failure result in rrcReestablishment.')\n",
    "\n",
    "            elif b is not None:\n",
    "\n",
    "                end, _ = find_1st_after(idx2, 'rrcConnectionReconfigurationComplete')\n",
    "                serv_cell, target_cell = df[\"PCI\"].iloc[idx2], df['lte_targetPhysCellId'].iloc[idx2]\n",
    "                serv_freq, target_freq = int(df[\"Freq\"].iloc[idx2]), df['dl-CarrierFreq'].iloc[idx2]\n",
    "                others += ' Result in gNB release.'\n",
    "                # We do not change nr_pci here. Instead, we will change it at gNB_Rel event.\n",
    "                orig_serv = (nr_pci, nr_arfcn) if nr_pci != 'O' else 'O'\n",
    "                trans = f'({serv_cell}, {serv_freq}) | {orig_serv} -> O'\n",
    "                \n",
    "                if serv_cell == target_cell and serv_freq == target_freq:\n",
    "                    D['SCG_RLF'].append(HO(start=t,end=end,others=others,trans=trans))\n",
    "                else:\n",
    "                    others += ' Weird gNB release.'\n",
    "                    print('Weird for scg failure result in gNb Release.')\n",
    "                    D['SCG_RLF'].append(HO(start=t,end=end,others=others,trans=trans))                  \n",
    "\n",
    "            else:\n",
    "\n",
    "                print('No end for scg failure.')\n",
    "                others += ' No end.'\n",
    "                D['SCG_RLF'].append(HO(start=t,others=others))\n",
    "        \n",
    "        if df['SCellToAddMod-r10'].iloc[i] == 1 and df['physCellId-r10'].iloc[i] != 'nr or cqi report':\n",
    "\n",
    "            others = ''\n",
    "            pcis = str(df[\"physCellId-r10\"].iloc[i]).split('@')\n",
    "            freqs = str(df[\"dl-CarrierFreq-r10\"].iloc[i]).split('@')\n",
    "            orig_scells = scells\n",
    "            scells = [(int(float(pci)), int(float(freq))) for pci, freq in zip(pcis, freqs)]\n",
    "\n",
    "            others += f' Set up {len(scells)} SCell.'\n",
    "            trans = f'{orig_scells} -> {scells}'\n",
    "\n",
    "            end, _ = find_1st_after(i,'rrcConnectionReconfigurationComplete')\n",
    "            \n",
    "            a, _ = find_1st_before(i, \"rrcConnectionReestablishmentRequest\", 3)\n",
    "            if a is not None:\n",
    "                others += ' Near after RLF.'\n",
    "\n",
    "            a = find_in_D_exact(['LTE_HO', 'MN_HO', 'MN_HO_to_eNB', 'SN_setup', 'SN_Rel'])\n",
    "            if a is not None:\n",
    "                others += f' With {a}.'\n",
    "\n",
    "            D['Add_SCell'].append(HO(start=t,end=end,others=others, trans=trans))\n",
    "    \n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: parse into readable dataframe\n",
    "- 2023-11-21: add BSID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi_parse_handover(df, tz=8, radical=True, endfill=False):\n",
    "    \n",
    "    def parse_trans(item):\n",
    "        \n",
    "        chunk = item.split(' | ')\n",
    "        \n",
    "        if len(chunk) == 1:\n",
    "            s_src = np.nan\n",
    "            s_tgt = np.nan\n",
    "            if chunk[0] == '':\n",
    "                m_src = np.nan\n",
    "                m_tgt = np.nan\n",
    "            elif chunk[0][0] == '?':\n",
    "                m_src = np.nan\n",
    "                m_tgt = chunk[0].split(' -> ')[1]\n",
    "            else:\n",
    "                m_src = chunk[0].split(' -> ')[0]\n",
    "                m_tgt = chunk[0].split(' -> ')[1]\n",
    "        else:\n",
    "            if chunk[1] == 'O':\n",
    "                s_src = np.nan\n",
    "                s_tgt = np.nan\n",
    "            else:\n",
    "                chunk1 = chunk[1].split(' -> ')\n",
    "                if len(chunk1) == 1:\n",
    "                    s_src = chunk1[0]\n",
    "                    s_tgt = np.nan\n",
    "                else:\n",
    "                    s_src = chunk1[0] if chunk1[0] != 'O' else np.nan\n",
    "                    s_tgt = chunk1[1] if chunk1[1] != 'O' else np.nan\n",
    "                \n",
    "            chunk1 = chunk[0].split(' -> ')\n",
    "            if len(chunk1) == 1:\n",
    "                m_src = chunk1[0]\n",
    "                m_tgt = np.nan\n",
    "            else:\n",
    "                m_src = chunk1[0]\n",
    "                m_tgt = chunk1[1]\n",
    "                \n",
    "        return m_src, m_tgt, s_src, s_tgt\n",
    "    \n",
    "    key_mapping = {\n",
    "        'Conn_Rel': 'CXNR',\n",
    "        'Conn_Req': 'CXNS',\n",
    "        'LTE_HO': 'LTEH',\n",
    "        'MN_HO': 'MCGH',\n",
    "        'MN_HO_to_eNB': 'SCGR-II',\n",
    "        'SN_setup': 'SCGA',\n",
    "        'SN_Rel': 'SCGR-I',\n",
    "        'SN_HO': 'SCGM',\n",
    "        'RLF_II': 'MCGF',\n",
    "        'RLF_III': 'NASR',\n",
    "        'SCG_RLF': 'SCGF',\n",
    "        'Add_SCell': 'SCLA'\n",
    "    }\n",
    "    \n",
    "    D = parse_mi_ho(df, tz)\n",
    "    \n",
    "    # rename as acronym\n",
    "    new_D = {key_mapping.get(key, key): value for key, value in D.items()}\n",
    "    \n",
    "    if df.empty:\n",
    "        selected_cols = ['m_src1', 'm_tgt1', 's_src1', 's_tgt1', 'PCI', 'Cell Identity', 'eNB_ID', 'next_eNB_ID', 'Band ID', 'next_BID', 'DL frequency', 'UL frequency', 'DL bandwidth', 'UL bandwidth', 'TAC', 'MCC', 'MNC']\n",
    "        table = pd.DataFrame(columns=['type', 'start', 'end', 'others', 'm_src', 'm_tgt', 's_src', 's_tgt', 'category', 'inter-eNB', 'inter-gNB', 'inter-Freq', 'band_cng', 'inter-RAT', '4G_5G', 'cause', 'near_before_RLF', *selected_cols])\n",
    "        print('Empty RRC File!!!')\n",
    "        return table, new_D\n",
    "    \n",
    "    table = pd.DataFrame()\n",
    "    for key, lst in new_D.items():\n",
    "        table1 = pd.DataFrame(lst, index=[key]*len(lst)).reset_index(names='type')\n",
    "        table = pd.concat([table, table1], ignore_index=True)\n",
    "    \n",
    "    if table.empty:\n",
    "        selected_cols = ['m_src1', 'm_tgt1', 's_src1', 's_tgt1', 'PCI', 'Cell Identity', 'eNB_ID', 'next_eNB_ID', 'Band ID', 'next_BID', 'DL frequency', 'UL frequency', 'DL bandwidth', 'UL bandwidth', 'TAC', 'MCC', 'MNC']\n",
    "        table = pd.DataFrame(columns=['type', 'start', 'end', 'others', 'm_src', 'm_tgt', 's_src', 's_tgt', 'category', 'inter-eNB', 'inter-gNB', 'inter-Freq', 'band_cng', 'inter-RAT', '4G_5G', 'cause', 'near_before_RLF', *selected_cols])\n",
    "        print('Handover Not Found!!!')\n",
    "        return table, new_D\n",
    "    \n",
    "    # add Cell Identity & eNB ID\n",
    "    sc_info = df[df['type_id'] == 'LTE_RRC_Serv_Cell_Info'][['Timestamp', 'type_id', 'PCI', 'Cell Identity', 'Band ID', 'DL frequency', 'UL frequency', 'DL bandwidth', 'UL bandwidth', 'TAC', 'MCC', 'MNC']].reset_index(drop=True).rename(columns={'Timestamp': 'start', 'type_id': 'type'})\n",
    "    sc_info['eNB_ID'] = sc_info['Cell Identity'] // 256\n",
    "    # sc_info['Cell_ID'] = sc_info['Cell Identity'] % 256\n",
    "    sc_info = sc_info[['start', 'type', 'PCI', 'Cell Identity', 'eNB_ID', 'Band ID', 'DL frequency', 'UL frequency', 'DL bandwidth', 'UL bandwidth', 'TAC', 'MCC', 'MNC']]\n",
    "\n",
    "    table = pd.concat([table, sc_info], ignore_index=True).sort_values(by='start').reset_index(drop=True)\n",
    "\n",
    "    is_not_start = True\n",
    "    selected_cols = ['PCI', 'Cell Identity', 'eNB_ID', 'Band ID', 'DL frequency', 'UL frequency', 'DL bandwidth', 'UL bandwidth', 'TAC', 'MCC', 'MNC']\n",
    "    for i, row in table.iterrows():\n",
    "        if row['type'] == 'LTE_RRC_Serv_Cell_Info':\n",
    "            is_not_start = False\n",
    "            info_to_fill = row[selected_cols].to_list()\n",
    "            continue\n",
    "        if is_not_start:\n",
    "            continue\n",
    "        table.loc[i, selected_cols] = info_to_fill\n",
    "\n",
    "    table = table[table['type'] != 'LTE_RRC_Serv_Cell_Info'].reset_index(drop=True)\n",
    "    \n",
    "    # parse source & target cells\n",
    "    for i, row in table.iterrows():\n",
    "        table.loc[i, ['m_src', 'm_tgt', 's_src', 's_tgt']] = parse_trans(row['trans'])\n",
    "    \n",
    "    # distinguish intra/inter-eNB HO\n",
    "    table1 = table[np.in1d(table['type'], ['SCLA', 'SCGA', 'SCGR-I', 'SCGF'])]\n",
    "    table = table[~np.in1d(table['type'], ['SCLA', 'SCGA', 'SCGR-I', 'SCGF'])].reset_index(drop=True)\n",
    "    \n",
    "    table['next_eNB'] = table['eNB_ID'].shift(-1)\n",
    "    for i, row in table.iloc[:-1].iterrows():\n",
    "        if row['eNB_ID'] != row['next_eNB'] and row['type'] not in ['CXNS', 'CXNR']:\n",
    "            if row['others'] == '':\n",
    "                table.at[i, 'others'] = 'Inter eNB HO.'\n",
    "            else:\n",
    "                table.at[i, 'others'] += ' Inter eNB HO.'\n",
    "    \n",
    "    table = pd.concat([table, table1], ignore_index=True).sort_values(by='start').reset_index(drop=True)\n",
    "    \n",
    "    # label SCG Addition near after SCG Failure\n",
    "    table1 = table[~np.in1d(table['type'], ['SCGA', 'SCGR-I', 'SCGR-II'])]\n",
    "    table = table[np.in1d(table['type'], ['SCGA', 'SCGR-I', 'SCGR-II'])].reset_index(drop=True)\n",
    "    \n",
    "    table['prev_cmt'] = table['others'].shift(1)\n",
    "    for i, row in table.iloc[1:].iterrows():\n",
    "        if row['type'] == 'SCGA':\n",
    "            if 'Near after SN_Rel' in row['others'] and 'Caused by scg-failure' in row['prev_cmt']:\n",
    "                table.at[i, 'others'] += ' Caused by scg-failure.'\n",
    "    \n",
    "    # with pd.option_context('display.max_rows', None): \n",
    "    #     display(table)\n",
    "    \n",
    "    # combine closed SCG Addition & Release pair (which are not caused by scg-failure or RLF) into SCG Change\n",
    "    table['next_end'] = table['end'].shift(-1)\n",
    "    table['next_cmt'] = table['others'].shift(-1)\n",
    "    table['next_s_tgt'] = table['s_tgt'].shift(-1)\n",
    "    indices_to_remove = []\n",
    "    for i, row in table.iloc[:-1].iterrows():\n",
    "        if row['type'] == 'SCGR-I' and 'Near after SN_Rel' in row['next_cmt'] and row['s_src'] != row['next_s_tgt']:\n",
    "            table.at[i, 'end'] = row['next_end']\n",
    "            table.at[i, 's_tgt'] = row['next_s_tgt']\n",
    "            table.at[i, 'type'] = 'SCGC-I'\n",
    "            indices_to_remove.append(i+1)\n",
    "        if row['type'] == 'SCGR-II' and 'Near after MN_HO_to_eNB' in row['next_cmt'] and row['s_src'] != row['next_s_tgt']:\n",
    "            table.at[i, 'end'] = row['next_end']\n",
    "            table.at[i, 's_tgt'] = row['next_s_tgt']\n",
    "            table.at[i, 'type'] = 'SCGC-II'\n",
    "            indices_to_remove.append(i+1)\n",
    "    table = table.drop(indices_to_remove)\n",
    "    \n",
    "    # with pd.option_context('display.max_rows', None): \n",
    "    #     display(table)\n",
    "    \n",
    "    table = pd.concat([table, table1], ignore_index=True).sort_values(by='start').reset_index(drop=True)\n",
    "    \n",
    "    # re-classify eNB HO & MeNB HO\n",
    "    table.loc[np.in1d(table['type'], ['LTEH']) & table['others'].str.contains('Inter eNB HO'), 'type'] = 'ENBH'\n",
    "    table.loc[np.in1d(table['type'], ['MCGH']) & table['others'].str.contains('Inter eNB HO'), 'type'] = 'MNBH'\n",
    "    \n",
    "    # add the next eNB ID when meeting inter-eNB HO\n",
    "    table1 = table[~table['others'].str.contains('Inter eNB HO')]\n",
    "    table = table[table['others'].str.contains('Inter eNB HO')].reset_index(drop=True)\n",
    "    \n",
    "    table['next_eNB_ID'] = table['eNB_ID'].shift(-1)\n",
    "    \n",
    "    table = pd.concat([table, table1], ignore_index=True).sort_values(by='start').reset_index(drop=True)\n",
    "    \n",
    "    # detect band change and add the next Band ID when meeting inter-Freq HO\n",
    "    band_mapping = {}\n",
    "    for i, row in table[~table.duplicated(subset=['DL frequency'])].dropna(subset=['DL frequency']).iterrows():\n",
    "        band_mapping[int(row['DL frequency'])] = row['Band ID']\n",
    "    \n",
    "    print(band_mapping)\n",
    "    \n",
    "    table1 = table[~table['others'].str.contains('Inter frequency HO')]\n",
    "    table = table[table['others'].str.contains('Inter frequency HO')].reset_index(drop=True)\n",
    "    \n",
    "    table['next_BID'] = table['Band ID'].shift(-1)\n",
    "    try:\n",
    "        table.at[len(table)-1, 'next_BID'] = band_mapping[ast.literal_eval(table.iloc[-1]['m_tgt'])[1]]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    table = pd.concat([table, table1], ignore_index=True).sort_values(by='start').reset_index(drop=True)\n",
    "    \n",
    "    # check whether RLF is near after an HO event\n",
    "    table1 = table[np.in1d(table['type'], ['CXNS', 'CXNR', 'SCLA'])]\n",
    "    table = table[~np.in1d(table['type'], ['CXNS', 'CXNR', 'SCLA'])].reset_index(drop=True)\n",
    "    \n",
    "    table['next_start'] = table['start'].shift(-1)\n",
    "    table['next_type'] = table['type'].shift(-1)\n",
    "    table['near_before_RLF'] = False\n",
    "    for i, row in table.iloc[:-1].iterrows():\n",
    "        # check whether an HO event is near before an RLF (in 3 seconds)\n",
    "        if row['type'] in ['LTEH', 'ENBH', 'MCGH', 'MNBH', 'SCGM', 'SCGA', 'SCGR-I', 'SCGR-II', 'SCGC-I', 'SCGC-II'] and \\\n",
    "            row['next_type'] in ['NASR', 'MCGF', 'SCGF']:\n",
    "                if row['end'] > row['next_start'] - pd.Timedelta(seconds=1):\n",
    "                    table.at[i, 'near_before_RLF'] = True\n",
    "                    next_type = row['next_type']\n",
    "                    if row['others'] == '':\n",
    "                        table.at[i, 'others'] = f'Near before {next_type} 1 sec.'\n",
    "                    else:\n",
    "                        table.at[i, 'others'] += f' Near before {next_type} 1 sec.'\n",
    "                        \n",
    "                elif row['end'] > row['next_start'] - pd.Timedelta(seconds=2):\n",
    "                    table.at[i, 'near_before_RLF'] = True\n",
    "                    next_type = row['next_type']\n",
    "                    if row['others'] == '':\n",
    "                        table.at[i, 'others'] = f'Near before {next_type} 2 sec.'\n",
    "                    else:\n",
    "                        table.at[i, 'others'] += f' Near before {next_type} 2 sec.'\n",
    "                        \n",
    "                elif row['end'] > row['next_start'] - pd.Timedelta(seconds=3):\n",
    "                    table.at[i, 'near_before_RLF'] = True\n",
    "                    next_type = row['next_type']\n",
    "                    if row['others'] == '':\n",
    "                        table.at[i, 'others'] = f'Near before {next_type} 3 sec.'\n",
    "                    else:\n",
    "                        table.at[i, 'others'] += f' Near before {next_type} 3 sec.'\n",
    "                    \n",
    "    table = pd.concat([table, table1], ignore_index=True).sort_values(by='start').reset_index(drop=True)\n",
    "    \n",
    "    # label (PCI, BSID)\n",
    "    # table[['eNB_ID', 'next_eNB_ID',]] = table[['eNB_ID', 'next_eNB_ID']].astype('Int64')\n",
    "    table[['PCI', 'Cell Identity', 'eNB_ID', 'next_eNB_ID', 'Band ID', 'next_BID', 'DL frequency', 'UL frequency', 'TAC', 'MCC', 'MNC']] = \\\n",
    "        table[['PCI', 'Cell Identity', 'eNB_ID', 'next_eNB_ID', 'Band ID', 'next_BID', 'DL frequency', 'UL frequency', 'TAC', 'MCC', 'MNC']].astype('Int64')\n",
    "    \n",
    "    table1 = table[np.in1d(table['type'], ['CXNS', 'CXNR', 'SCLA'])]\n",
    "    table = table[~np.in1d(table['type'], ['CXNS', 'CXNR', 'SCLA'])].reset_index(drop=True)\n",
    "    \n",
    "    for col in ['m_src', 's_src']:\n",
    "        new_col = f'{col}1'\n",
    "        filtered_table = table[table[col].isna()].copy()\n",
    "        filtered_table[new_col] = np.nan\n",
    "        \n",
    "        # add BSID (eNB)\n",
    "        filtered_table1 = table[table[col].notna()].copy()\n",
    "        filtered_table1[new_col] = filtered_table1[col].astype(str) + ', ' + filtered_table1['eNB_ID'].astype(str)\n",
    "        \n",
    "        table = pd.concat([filtered_table, filtered_table1], ignore_index=True).sort_values(by='start').reset_index(drop=True)\n",
    "    \n",
    "    for col in ['m_tgt', 's_tgt']:\n",
    "        new_col = f'{col}1'\n",
    "        filtered_table = table[table[col].isna()].copy()\n",
    "        filtered_table[new_col] = np.nan\n",
    "        \n",
    "        # add BSID (eNB)\n",
    "        filtered_table1 = table[table[col].notna() & table['next_eNB_ID'].notna()].copy()\n",
    "        filtered_table1[new_col] = filtered_table1[col].astype(str) + ', ' + filtered_table1['next_eNB_ID'].astype(str)\n",
    "        filtered_table2 = table[table[col].notna() & table['next_eNB_ID'].isna()].copy()\n",
    "        filtered_table2[new_col] = filtered_table2[col].astype(str) + ', ' + filtered_table2['eNB_ID'].astype(str)\n",
    "        \n",
    "        table = pd.concat([filtered_table, filtered_table1, filtered_table2], ignore_index=True).sort_values(by='start').reset_index(drop=True)\n",
    "    \n",
    "    table = pd.concat([table, table1], ignore_index=True).sort_values(by='start').reset_index(drop=True)\n",
    "    \n",
    "    # add category\n",
    "    table['category'] = 'Others'\n",
    "    table.loc[np.in1d(table['type'], ['LTEH', 'ENBH', 'MCGH', 'MNBH', 'SCGM', 'SCGA', 'SCGR-I', 'SCGC-I', 'SCGR-II', 'SCGC-II']), 'category'] = 'HO'\n",
    "    table.loc[np.in1d(table['type'], ['MCGF', 'NASR', 'SCGF']), 'category'] = 'RLF'\n",
    "\n",
    "    # add failure cause\n",
    "    failure_cause = [\n",
    "        'reconfigurationFailure (0)', 'handoverFailure (1)', 'otherFailure (2)',\n",
    "        't310-Expiry (0)', 'randomAccessProblem (1)', 'rlc-MaxNumRetx (2)', 'synchReconfigFailureSCG (3)', 'scg-ReconfigFailure (4)', 'srb3-IntegrityFailure (5)', 'other-r16 (6)'\n",
    "    ]\n",
    "    \n",
    "    for tag in failure_cause:\n",
    "        table.loc[table['others'].str.contains(tag, regex=False), 'cause'] = tag\n",
    "        table['others'] = table['others'].str.replace(f\" {tag}.\", \"\", regex=False)\n",
    "        table['others'] = table['others'].str.replace(f\"{tag}.\", \"\", regex=False)\n",
    "    \n",
    "    # add Access Technology type\n",
    "    table['4G_5G'] = '4G'\n",
    "    table.loc[np.in1d(table['type'], ['SCGM', 'SCGA', 'SCGR-I', 'SCGC-I', 'SCGF']), '4G_5G'] = '5G'\n",
    "    table.loc[np.in1d(table['type'], ['SCGR-II', 'SCGC-II']), '4G_5G'] = '4G_5G'\n",
    "    \n",
    "    # add more boolean columns\n",
    "    table['inter-eNB'] = False\n",
    "    table.loc[table['others'].str.contains('Inter eNB HO'), 'inter-eNB'] = True\n",
    "    table['others'] = table['others'].str.replace(\" Inter eNB HO.\", \"\")\n",
    "    table['others'] = table['others'].str.replace(\"Inter eNB HO.\", \"\")\n",
    "    \n",
    "    table['inter-Freq'] = False\n",
    "    table.loc[table['others'].str.contains('Inter frequency HO'), 'inter-Freq'] = True\n",
    "    table['others'] = table['others'].str.replace(\" Inter frequency HO.\", \"\")\n",
    "    table['others'] = table['others'].str.replace(\"Inter frequency HO.\", \"\")\n",
    "    \n",
    "    table['band_cng'] = False\n",
    "    table.loc[table['inter-Freq'] & (table['Band ID'] != table['next_BID']), 'band_cng'] = True\n",
    "    \n",
    "    table['inter-RAT'] = False\n",
    "    table.loc[np.in1d(table['type'], ['SCGA', 'SCGR-I', 'SCGC-I', 'SCGR-II', 'SCGC-II']), 'inter-RAT'] = True\n",
    "    \n",
    "    table['inter-gNB'] = False\n",
    "    table.loc[np.in1d(table['type'], ['SCGC-I', 'SCGC-II']), 'inter-gNB'] = True\n",
    "    \n",
    "    # find row na-\"end\" & fill with \"start\"\n",
    "    if endfill:\n",
    "        nan_end_rows = table[table['end'].isnull()]\n",
    "        table.loc[nan_end_rows.index, 'end'] = nan_end_rows['start']\n",
    "    \n",
    "    # ignore CXNS, CXNR, SCLA\n",
    "    table = table[~np.in1d(table['type'], ['CXNS', 'CXNR', 'SCLA'])].reset_index(drop=True)\n",
    "    \n",
    "    # remove SCG Addition, Release caused by SCG Failure or any other RLFs if needed (default: True)\n",
    "    if radical:\n",
    "        table = table[~((table['others'].str.contains('Caused by scg-failure') | table['others'].str.contains('Near after RLF')))].reset_index(drop=True)\n",
    "    \n",
    "    # select columns\n",
    "    selected_cols = ['m_src1', 'm_tgt1', 's_src1', 's_tgt1', 'PCI', 'Cell Identity', 'eNB_ID', 'next_eNB_ID', 'Band ID', 'next_BID', 'DL frequency', 'UL frequency', 'DL bandwidth', 'UL bandwidth', 'TAC', 'MCC', 'MNC']\n",
    "    table = table[['type', 'start', 'end', 'others', 'm_src', 'm_tgt', 's_src', 's_tgt', 'category', 'inter-eNB', 'inter-gNB', 'inter-Freq', 'band_cng', 'inter-RAT', '4G_5G', 'cause', 'near_before_RLF', *selected_cols]]\n",
    "    \n",
    "    return table, new_D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Handover Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************************************* label ho *************************************************\n",
    "\n",
    "def handover_classify_labelling(df_ho, df_dl=None, df_ul=None, ratio=0.5, scope=None):\n",
    "    \n",
    "    if scope is None:\n",
    "        scope = {\n",
    "            'LTEH': (-4.0, 2.0, 0.018),\n",
    "            'ENBH': (-4.0, 2.6, 0.019),\n",
    "            'MCGH': (-3.0, 3.3, 0.019),\n",
    "            'MNBH': (-3.2, 3.3, 0.02),\n",
    "            'SCGM': (-4.2, 3.3, 0.017),\n",
    "            'SCGA': (-1.2, 2.6, 0.027),\n",
    "            'SCGR-I': (-3.1, 3.3, 0.04),\n",
    "            'SCGC-I': (-3.0, 2.9, 0.37),\n",
    "            'SCGR-II': (-2.0, 4.1, 0.034),\n",
    "            'SCGC-II': (-2.0, 3.2, 0.396),\n",
    "            'MCGF': (-5.8, 7.2, 0.078),\n",
    "            'NASR': (-4.0, 6.5, 0.394),\n",
    "            'SCGF': (-4.0, 4.6, 0.111)\n",
    "        }\n",
    "        \n",
    "    def interp(x, y, ratio):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x, y (datetime.datetime): x < y\n",
    "            ratio (float): a decimal numeral in a range [0, 1]; 0 means break at x, 1 means break at y.\n",
    "        Returns:\n",
    "            (datetime.datetime): breakpoint of interpolation\n",
    "        \"\"\"\n",
    "        return x + (y - x) * ratio\n",
    "\n",
    "    Metrics = namedtuple('Metrics', ['dl_pkt', 'dl_lost', 'dl_excl', 'ul_pkt', 'ul_lost', 'ul_excl'])\n",
    "    Duration = namedtuple('Duration', ['total', 'stable', 'unstable'])\n",
    "\n",
    "    mcgf = [f'MCGF_{suffix}' for suffix in ['reconfigurationFailure (0)', 'handoverFailure (1)', 'otherFailure (2)']]\n",
    "    nasr = [f'NASR_{suffix}' for suffix in ['reconfigurationFailure (0)', 'handoverFailure (1)', 'otherFailure (2)']]\n",
    "    scgf = [f'SCGF_{suffix}' for suffix in ['t310-Expiry (0)', 'randomAccessProblem (1)', 'rlc-MaxNumRetx (2)', 'synchReconfigFailureSCG (3)', 'scg-ReconfigFailure (4)', 'srb3-IntegrityFailure (5)', 'other-r16 (6)']]\n",
    "\n",
    "    E = { stage: { key: [] for key in [*scope.keys(), *mcgf, *nasr, *scgf] } for stage in ['before', 'during', 'after']}\n",
    "    E['overview'] = {}\n",
    "\n",
    "    selected_cols = ['stage', 'category', 'type', 'cause', 'index', 'inter-RAT', 'inter-eNB', 'inter-gNB', 'inter-Freq', 'band_cng']\n",
    "    reset_values = ['-', 'stable', 'stable', '-', -1, False, False, False, False, False]\n",
    "    \n",
    "    dl_start_time, dl_end_time = pd.Timestamp.max, pd.Timestamp.min\n",
    "    if df_dl is not None:\n",
    "        dl_start_time, dl_end_time = df_dl.iloc[0]['Timestamp'] - pd.Timedelta(seconds=0.1), df_dl.iloc[-1]['Timestamp'] + pd.Timedelta(seconds=0.1)\n",
    "        # df_dl = df_dl.reindex(columns=[*list(df_dl.columns), *selected_cols])\n",
    "        # Reset the specified columns using a loop\n",
    "        for col, reset_value in zip(selected_cols, reset_values):\n",
    "            df_dl[col] = reset_value\n",
    "    \n",
    "    ul_start_time, ul_end_time = pd.Timestamp.max, pd.Timestamp.min\n",
    "    if df_ul is not None:\n",
    "        ul_start_time, ul_end_time = df_ul.iloc[0]['Timestamp'] - pd.Timedelta(seconds=0.1), df_ul.iloc[-1]['Timestamp'] + pd.Timedelta(seconds=0.1)\n",
    "        # df_ul = df_ul.reindex(columns=[*list(df_ul.columns), *selected_cols])\n",
    "        # Reset the specified columns using a loop\n",
    "        for col, reset_value in zip(selected_cols, reset_values):\n",
    "            df_ul[col] = reset_value\n",
    "            \n",
    "    start_time, end_time = min(dl_start_time, ul_start_time), max(dl_end_time, ul_end_time)\n",
    "    stable_interval = P.open(start_time, end_time)\n",
    "    for i, row in df_ho.iterrows():\n",
    "        prior_row = df_ho.iloc[i-1] if i != 0 else None\n",
    "        post_row = df_ho.iloc[i+1] if i != len(df_ho) - 1 else None\n",
    "        \n",
    "        # Peek the next event to avoid HO overlapping with handoverFailure (skip and set the next prior event)\n",
    "        if i != len(df_ho) - 1 and pd.notna(row.end) and row.end > post_row.start:\n",
    "            print('Overlapping event occurs!!')\n",
    "            print(i, row['start'], row['end'], row['type'], row['cause'])\n",
    "            print(i+1, post_row['start'], post_row['end'], post_row['type'], post_row['cause'])\n",
    "            continue\n",
    "        if i != 0 and pd.notna(prior_row.end) and prior_row.end > row.start:\n",
    "            prior_row = df_ho.iloc[i-2] if i > 1 else None\n",
    "        \n",
    "        # Basic information\n",
    "        tag = row['type']  # specific column name\n",
    "        cause = row['cause']\n",
    "        start, end = row['start'], row['end']  # handover start/end time\n",
    "        intr = (end - start).total_seconds() if pd.notna(end) else 0  # handover interruption time\n",
    "        ho_info = [row['category'], row['type'], row['cause'], i, row['inter-RAT'], row['inter-eNB'], row['inter-gNB'], row['inter-Freq'], row['band_cng']]\n",
    "        \n",
    "        # peri_interval\n",
    "        if pd.isna(row.end):\n",
    "            peri_interval = P.singleton(row.start)\n",
    "        else:\n",
    "            peri_interval = P.closed(row.start, row.end)\n",
    "\n",
    "        # prior_interval\n",
    "        C = row.start + pd.Timedelta(seconds=scope[tag][0])\n",
    "        D = row.start\n",
    "        prior_interval = P.closedopen(C, D)\n",
    "        if ratio is not None and i != 0:\n",
    "            prior_tag = prior_row['type']\n",
    "            A = max(prior_row.start, prior_row.end)\n",
    "            B = max(prior_row.start, prior_row.end) + pd.Timedelta(seconds=scope[prior_tag][1]-scope[prior_tag][2])\n",
    "            if P.openclosed(A, B).overlaps(prior_interval):\n",
    "                # print(\"Overlaps with the previous!\")\n",
    "                bkp = interp(C, B, ratio)\n",
    "                bkp = max(bkp, A)  # avoid the breakpoint overlapping the previous event's duration\n",
    "                # bkp = min(max(bkp, A), D)  # 我不侵犯到其他任何人，代表其他人也不會侵犯到我！可不加！\n",
    "                prior_interval = P.closedopen(bkp, D)\n",
    "                if A in prior_interval:\n",
    "                    prior_interval = P.open(bkp, D)\n",
    "        \n",
    "        # post_interval\n",
    "        C = row.end\n",
    "        D = row.end + pd.Timedelta(seconds=scope[tag][1]-scope[tag][2])\n",
    "        post_interval = P.openclosed(C, D)\n",
    "        if ratio is not None and i != len(df_ho)-1:\n",
    "            post_tag = post_row['type']\n",
    "            A = min(post_row.start, post_row.end) + pd.Timedelta(seconds=scope[post_tag][0])\n",
    "            B = min(post_row.start, post_row.end)\n",
    "            if P.closedopen(A, B).overlaps(post_interval):\n",
    "                # print(\"Overlaps with the following!\")\n",
    "                bkp = interp(A, D, ratio)\n",
    "                bkp = min(bkp, B)  # avoid the breakpoint overlappint the following event's duration\n",
    "                # bkp = max(min(bkp, B), C)  # 我不侵犯到其他任何人，代表其他人也不會侵犯到我！可不加！\n",
    "                post_interval = P.open(C, bkp)\n",
    "        \n",
    "        # calculate lost & excl\n",
    "        dl_pkt, dl_lost, dl_excl = [0, 0, 0], [0, 0, 0], [0, 0, 0]\n",
    "        if df_dl is not None:\n",
    "            for i, (stage, intv) in enumerate(zip(['before', 'during', 'after'], [prior_interval, peri_interval, post_interval])):\n",
    "                if intv.empty:\n",
    "                    continue\n",
    "                filt = (df_dl['arr_time'] >= intv.lower) & (df_dl['arr_time'] < intv.upper)\n",
    "                tmp = df_dl[filt].copy()\n",
    "                dl_pkt[i] = len(tmp)\n",
    "                dl_lost[i] = sum(tmp['lost'])\n",
    "                dl_excl[i] = sum(~tmp['lost'] & tmp['excl'])\n",
    "                df_dl.loc[filt, selected_cols] = [stage, *ho_info]\n",
    "        \n",
    "        ul_pkt, ul_lost, ul_excl = [0, 0, 0], [0, 0, 0], [0, 0, 0]\n",
    "        if df_ul is not None:\n",
    "            for i, (stage, intv) in enumerate(zip(['before', 'during', 'after'], [prior_interval, peri_interval, post_interval])):\n",
    "                if intv.empty:\n",
    "                    continue\n",
    "                filt = (df_ul['xmit_time'] >= intv.lower) & (df_ul['xmit_time'] < intv.upper)\n",
    "                tmp = df_ul[filt].copy()\n",
    "                ul_pkt[i] = len(tmp)\n",
    "                ul_lost[i] = sum(tmp['lost'])\n",
    "                ul_excl[i] = sum(~tmp['lost'] & tmp['excl'])\n",
    "                df_ul.loc[filt, selected_cols] = [stage, *ho_info]\n",
    "        \n",
    "        # fill in the blank\n",
    "        for i, (stage, intv) in enumerate(zip(['before', 'during', 'after'], [prior_interval, peri_interval, post_interval])):\n",
    "            E[stage][tag].append((intv, Metrics(dl_pkt[i], dl_lost[i], dl_excl[i], ul_pkt[i], ul_lost[i], ul_excl[i])))\n",
    "            if tag in ['MCGF', 'NASR', 'SCGF']:\n",
    "                E[stage][f'{tag}_{cause}'].append((intv, Metrics(dl_pkt[i], dl_lost[i], dl_excl[i], ul_pkt[i], ul_lost[i], ul_excl[i])))\n",
    "        \n",
    "        # update stable interval\n",
    "        stable_interval = stable_interval - prior_interval - peri_interval - post_interval\n",
    "    \n",
    "    stable_dl_pkt, stable_dl_lost, stable_dl_excl = 0, 0, 0\n",
    "    stable_ul_pkt, stable_ul_lost, stable_ul_excl = 0, 0, 0\n",
    "    if df_dl is not None:\n",
    "        tmp = df_dl[df_dl['category'] == 'stable'].copy()\n",
    "        stable_dl_pkt = len(tmp)\n",
    "        stable_dl_lost = sum(tmp['lost'])\n",
    "        stable_dl_excl = sum(~tmp['lost'] & tmp['excl'])\n",
    "    \n",
    "    if df_ul is not None:\n",
    "        tmp = df_ul[df_ul['category'] == 'stable'].copy()\n",
    "        stable_ul_pkt = len(tmp)\n",
    "        stable_ul_lost = sum(tmp['lost'])\n",
    "        stable_ul_excl = sum(~tmp['lost'] & tmp['excl'])\n",
    "    \n",
    "    total_duration = (end_time - start_time).total_seconds()\n",
    "    stable_duration = 0\n",
    "    for intv in stable_interval:\n",
    "        if intv.empty:\n",
    "            continue\n",
    "        stable_duration += (intv.upper - intv.lower).total_seconds()\n",
    "    unstable_duration = total_duration - stable_duration\n",
    "    \n",
    "    E['overview']['stable_intv'] = (stable_interval, Metrics(stable_dl_pkt, stable_dl_lost, stable_dl_excl, stable_ul_pkt, stable_ul_lost, stable_ul_excl))\n",
    "    E['overview']['duration'] = Duration(total_duration, stable_duration, unstable_duration)\n",
    "\n",
    "    return E, df_dl, df_ul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_duration_into_dict(E):\n",
    "    mcgf = [f'MCGF_{suffix}' for suffix in ['reconfigurationFailure (0)', 'handoverFailure (1)', 'otherFailure (2)']]\n",
    "    nasr = [f'NASR_{suffix}' for suffix in ['reconfigurationFailure (0)', 'handoverFailure (1)', 'otherFailure (2)']]\n",
    "    scgf = [f'SCGF_{suffix}' for suffix in ['t310-Expiry (0)', 'randomAccessProblem (1)', 'rlc-MaxNumRetx (2)', 'synchReconfigFailureSCG (3)', 'scg-ReconfigFailure (4)', 'srb3-IntegrityFailure (5)', 'other-r16 (6)']]\n",
    "\n",
    "    A = { stage: { key: [] for key in ['LTEH', 'ENBH', 'MCGH', 'MNBH', 'SCGM', 'SCGA', 'SCGR-I', 'SCGC-I', 'SCGR-II', 'SCGC-II',\n",
    "                                    'MCGF', 'NASR', 'SCGF', *mcgf, *nasr, *scgf] } for stage in ['before', 'during', 'after'] }\n",
    "    A['overview'] = {}\n",
    "\n",
    "    for stage in ['before', 'during', 'after']:\n",
    "        for tag, lst in E[stage].items():\n",
    "            for item in lst:\n",
    "                A[stage][tag].append((item[0], item[1]._asdict()))\n",
    "    A['overview']['stable_intv'] = (E['overview']['stable_intv'][0], E['overview']['stable_intv'][1]._asdict())\n",
    "    A['overview']['duration'] = E['overview']['duration']._asdict()\n",
    "    \n",
    "    return A\n",
    "\n",
    "def dict_into_metrics_duration(A):\n",
    "    Metrics = namedtuple('Metrics', ['dl_pkt', 'dl_lost', 'dl_excl', 'ul_pkt', 'ul_lost', 'ul_excl'])\n",
    "    Duration = namedtuple('Duration', ['total', 'stable', 'unstable'])\n",
    "\n",
    "    mcgf = [f'MCGF_{suffix}' for suffix in ['reconfigurationFailure (0)', 'handoverFailure (1)', 'otherFailure (2)']]\n",
    "    nasr = [f'NASR_{suffix}' for suffix in ['reconfigurationFailure (0)', 'handoverFailure (1)', 'otherFailure (2)']]\n",
    "    scgf = [f'SCGF_{suffix}' for suffix in ['t310-Expiry (0)', 'randomAccessProblem (1)', 'rlc-MaxNumRetx (2)', 'synchReconfigFailureSCG (3)', 'scg-ReconfigFailure (4)', 'srb3-IntegrityFailure (5)', 'other-r16 (6)']]\n",
    "\n",
    "    B = { stage: { key: [] for key in ['LTEH', 'ENBH', 'MCGH', 'MNBH', 'SCGM', 'SCGA', 'SCGR-I', 'SCGC-I', 'SCGR-II', 'SCGC-II',\n",
    "                                    'MCGF', 'NASR', 'SCGF', *mcgf, *nasr, *scgf] } for stage in ['before', 'during', 'after'] }\n",
    "    B['overview'] = {}\n",
    "\n",
    "    for stage in ['before', 'during', 'after']:\n",
    "        for tag, lst in A[stage].items():\n",
    "            for item in lst:\n",
    "                B[stage][tag].append((item[0], Metrics(**item[1])))\n",
    "    B['overview']['stable_intv'] = (A['overview']['stable_intv'][0], Metrics(**A['overview']['stable_intv'][1]))\n",
    "    B['overview']['duration'] = Duration(**A['overview']['duration'])\n",
    "    \n",
    "    return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Profiling\n",
    "- Category: HO, RLF\n",
    "- Type: LTEH, ENBH, ..., MCGF, NASR, ...\n",
    "- Cause: handoverFailure (1), otherFailure (2), ...\n",
    "\n",
    "### Tools: interval operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_disjoint(set1, set2):\n",
    "    \"\"\"\n",
    "    Check if two sets are disjoint.\n",
    "    \"\"\"\n",
    "    return (set1 & set2).empty\n",
    "\n",
    "def interp(x, y, ratio):\n",
    "    \"\"\"\n",
    "    Interpolation (線性內插法)\n",
    "\n",
    "    Args:\n",
    "        x, y (datetime.datetime): x < y\n",
    "        ratio (float): a decimal numeral in a range [0, 1]; 0 means break at x, 1 means break at y.\n",
    "    Returns:\n",
    "        (datetime.datetime): breakpoint of interpolation\n",
    "    \"\"\"\n",
    "    return x + (y - x) * ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: setup an instance of profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_profile_instance(df, tag, start, end, mode='dl', mets='lost', left=pd.Timestamp.min, right=pd.Timestamp.max, fix=False, scope=(-3, 3)):\n",
    "    \n",
    "    if pd.notna(end):\n",
    "        if fix:\n",
    "            intv = P.closed(max(start+pd.Timedelta(seconds=scope[0]), left), min(start+pd.Timedelta(seconds=scope[1]), right))\n",
    "        else:\n",
    "            intv = P.closed(max(start+pd.Timedelta(seconds=scope[tag][0]), left), min(start+pd.Timedelta(seconds=scope[tag][1]), right))\n",
    "    else:\n",
    "        if fix:\n",
    "            intv = P.closed(max(start+pd.Timedelta(seconds=scope[0]), left), min(start+pd.Timedelta(seconds=scope[1]), right))\n",
    "        else:\n",
    "            intv = P.closed(max(start+pd.Timedelta(seconds=scope[tag][0]), left), min(start+pd.Timedelta(seconds=scope[tag][1]), right))\n",
    "    \n",
    "    if mode == 'ul':\n",
    "        df_plot = df.query('xmit_time >= @intv.lower & xmit_time <= @intv.upper').copy().reset_index(drop=True)\n",
    "        df_plot['rel_time'] = (df_plot['xmit_time'] - start).dt.total_seconds()\n",
    "    elif mode == 'dl':\n",
    "        df_plot = df.query('arr_time >= @intv.lower & arr_time <= @intv.upper').copy().reset_index(drop=True)\n",
    "        df_plot['rel_time'] = (df_plot['arr_time'] - start).dt.total_seconds()\n",
    "    else: # 'pyl'\n",
    "        df_plot = df.query('Timestamp >= @intv.lower & Timestamp <= @intv.upper').copy().reset_index(drop=True)\n",
    "        df_plot['rel_time'] = (df_plot['Timestamp'] - start).dt.total_seconds()\n",
    "    df_plot['cat_id'] = ((df_plot['rel_time'] + 0.005) // 0.01) * 0.01\n",
    "    \n",
    "    if mets == 'lost':\n",
    "        # calculate PLR\n",
    "        ts_group = df_plot.groupby(['cat_id'])\n",
    "        table = ts_group['lost'].agg(['count','sum','mean']).copy().reset_index()\n",
    "        table = table.rename(columns={'count':'tx_count', 'sum':'lost', 'mean':'PLR'})\n",
    "        table['PLR'] = table['PLR'] * 100\n",
    "    elif mets == 'excl':\n",
    "        # calculate ELR\n",
    "        df_plot['excl_new'] = df_plot['excl'] & ~df_plot['lost']\n",
    "        ts_group = df_plot.groupby(['cat_id'])\n",
    "        table = ts_group['excl_new'].agg(['count','sum','mean']).copy().reset_index()\n",
    "        table = table.rename(columns={'count':'tx_count', 'sum':'excl', 'mean':'ELR'})\n",
    "        table['ELR'] = table['ELR'] * 100\n",
    "    else:\n",
    "        print(f'No metrics: {mets}!!')\n",
    "        raise\n",
    "    \n",
    "    return table, intv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: setup profiles with one-trip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_profile(df, df_ho, mode='dl', mets='lost', sp_col=None, scope=None):\n",
    "    # Initialize\n",
    "    A = {tag: {'tables': [], 'intrs': []} for tag in scope.keys()}\n",
    "    stable_intv = P.open(pd.Timestamp.min, pd.Timestamp.max)\n",
    "    stable_pkt = 0\n",
    "    stable_miss = 0\n",
    "    stable_miss_rate = 0\n",
    "\n",
    "    for i, row in df_ho.iterrows():\n",
    "        prior_row = df_ho.iloc[i-1] if i != 0 else None\n",
    "        post_row = df_ho.iloc[i+1] if i != len(df_ho) - 1 else None\n",
    "        \n",
    "        # Peek the next event to avoid HO overlapping with handoverFailure (skip and set the next prior event)\n",
    "        if i != len(df_ho) - 1 and pd.notna(row.end) and row.end > post_row.start:\n",
    "            print('Overlapping event occurs!!')\n",
    "            print(i, row['start'], row['end'], row['type'], row['cause'])\n",
    "            print(i+1, post_row['start'], post_row['end'], post_row['type'], post_row['cause'])\n",
    "            continue\n",
    "        if i != 0 and pd.notna(prior_row.end) and prior_row.end > row.start:\n",
    "            prior_row = df_ho.iloc[i-2] if i > 1 else None\n",
    "        \n",
    "        # Basic information\n",
    "        tag = row[sp_col]  # specific column name\n",
    "        start, end = row['start'], row['end']  # handover start/end time\n",
    "        intr = (end - start).total_seconds() if pd.notna(end) else 0  # handover interruption time\n",
    "        \n",
    "        # Set left/right bounds to avoid event overlapping with each other\n",
    "        if prior_row is not None:\n",
    "            if pd.notna(prior_row['end']):\n",
    "                left = prior_row['end'] + (start - prior_row['end']) / 2\n",
    "            else:\n",
    "                left = prior_row['start'] + (start - prior_row['start']) / 2\n",
    "        else:\n",
    "            left = pd.Timestamp.min\n",
    "        if post_row is not None:\n",
    "            if pd.notna(end):\n",
    "                right = end + (post_row['start'] - end) / 2\n",
    "            else:\n",
    "                right = start + (post_row['start'] - start) / 2\n",
    "        else:\n",
    "            right = pd.Timestamp.max\n",
    "        \n",
    "        # Setup a profile instance\n",
    "        table, intv = setup_profile_instance(df, tag=tag, start=start, end=end, mode=mode, mets=mets, left=left, right=right, scope=scope)\n",
    "        \n",
    "        A[tag]['tables'].append(table)  # profile table instance\n",
    "        A[tag]['intrs'].append(intr)  # handover interruption time\n",
    "        \n",
    "        # Update stable interval\n",
    "        stable_intv = stable_intv - intv\n",
    "\n",
    "    # Count lost and transferred packets under stable state\n",
    "    for intv in stable_intv:\n",
    "        lower = intv.lower; upper = intv.upper\n",
    "        if mode == 'dl':\n",
    "            df_tmp = df[(df['arr_time'] > lower) & (df['arr_time'] <= upper)].copy().reset_index(drop=True)\n",
    "        elif mode == 'ul':\n",
    "            df_tmp = df[(df['xmit_time'] > lower) & (df['xmit_time'] <= upper)].copy().reset_index(drop=True)\n",
    "        else:\n",
    "            print(f'No mode: {mode}!!')\n",
    "            raise\n",
    "        \n",
    "        stable_pkt += len(df_tmp)\n",
    "        if mets == 'lost':\n",
    "            stable_miss += sum(df_tmp['lost'])\n",
    "        elif mets == 'excl':\n",
    "            stable_miss += sum(df_tmp['excl'] & ~df_tmp['lost'])\n",
    "    \n",
    "    stable_miss_rate = round(stable_miss / (stable_pkt + 1e-9) * 100, 3)\n",
    "    \n",
    "    # Calculate stable duration, proportion\n",
    "    stable_duration = 0\n",
    "    stable_intv = stable_intv & P.closed(df.iloc[0]['xmit_time'], df.iloc[-1]['xmit_time'])\n",
    "    for intv in stable_intv:\n",
    "        stable_duration += (intv.upper - intv.lower).total_seconds()\n",
    "        \n",
    "    total_duration = (df.iloc[-1]['xmit_time'] - df.iloc[0]['xmit_time']).total_seconds()\n",
    "    stable_proportion = round(stable_duration / (total_duration + 1e-9) * 100, 3)\n",
    "    \n",
    "    return A, stable_miss, stable_pkt, stable_miss_rate, stable_duration, total_duration, stable_proportion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3: merge profiles from all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_profile(tables, intrs, mets='lost'):\n",
    "    \n",
    "    intr = np.mean(intrs)\n",
    "    if mets == 'lost':\n",
    "        table = pd.DataFrame(columns=['cat_id','tx_count','lost','PLR'])\n",
    "        table['lost'] = table['lost'].astype('Int32')\n",
    "        table['PLR'] = table['PLR'].astype('float32')\n",
    "    elif mets == 'excl':\n",
    "        table = pd.DataFrame(columns=['cat_id','tx_count','excl','ELR'])\n",
    "        table['excl'] = table['excl'].astype('Int32')\n",
    "        table['ELR'] = table['ELR'].astype('float32')\n",
    "    \n",
    "    table['cat_id'] = table['cat_id'].astype('float32')\n",
    "    table['tx_count'] = table['tx_count'].astype('Int32')\n",
    "    \n",
    "    for i in range(len(tables)):\n",
    "        _table = tables[i].copy()\n",
    "        table = table.merge(_table, on=['cat_id'], how='outer').fillna(0)\n",
    "        table['tx_count'] = table['tx_count_x'] + table['tx_count_y']\n",
    "        \n",
    "        if mets == 'lost':\n",
    "            table['lost'] = table['lost_x'] + table['lost_y']\n",
    "            table['PLR'] = 0\n",
    "            table = table[['cat_id','tx_count','lost','PLR']]\n",
    "        elif mets == 'excl':\n",
    "            table['excl'] = table['excl_x'] + table['excl_y']\n",
    "            table['ELR'] = 0\n",
    "            table = table[['cat_id','tx_count','excl','ELR']]\n",
    "    \n",
    "    if mets == 'lost':    \n",
    "        table['PLR'] = table['lost'] / (table['tx_count'] + 1e-9) * 100\n",
    "        table = table[['cat_id','tx_count','lost','PLR']].copy().sort_values(by=['cat_id']).reset_index(drop=True)\n",
    "    elif mets == 'excl':\n",
    "        table['ELR'] = table['excl'] / (table['tx_count'] + 1e-9) * 100\n",
    "        table = table[['cat_id','tx_count','excl','ELR']].copy().sort_values(by=['cat_id']).reset_index(drop=True)\n",
    "    \n",
    "    return table, intr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 4: approximate the scopes of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_scope(table, mets='lost', evt_type='RLF', stable_miss_rate=0):\n",
    "    table['new'] = (table[mets] - table['tx_count'] * stable_miss_rate / 1e3).round()\n",
    "    table_tmp = table[table['new'] > 0]\n",
    "    \n",
    "    if len(table_tmp) == 0:\n",
    "        return 0\n",
    "    \n",
    "    lower = min(math.floor(table.iloc[0]['cat_id'] * 10), -1)\n",
    "    upper = max(math.ceil(table.iloc[-1]['cat_id'] * 10), 1)\n",
    "\n",
    "    # total = sum(table_tmp[mets])\n",
    "    total = sum(table_tmp['new'])\n",
    "    if evt_type == 'RLF':\n",
    "        thr = round(total * 0.9973)\n",
    "    elif evt_type == 'HO':\n",
    "        # thr = round(total * 0.9876)\n",
    "        thr = round(total * 0.9545)\n",
    "        # thr = round(total * 0.8664)\n",
    "        # thr = round(total * 0.6827)\n",
    "    else:\n",
    "        print(f'No category: {evt_type}!!')\n",
    "        raise\n",
    "\n",
    "    scope_candidate = []\n",
    "    scope_duration = []\n",
    "    for i in range(lower, 0):\n",
    "        for j in range(1, upper+1):\n",
    "            k = i/10; l = j/10\n",
    "            _table_tmp = table_tmp[(table_tmp['cat_id'] >= k) & (table_tmp['cat_id'] < l)]\n",
    "            # miss = sum(_table_tmp[mets])\n",
    "            miss = sum(_table_tmp['new'])\n",
    "            # print(miss, thr)\n",
    "            if miss >= thr:\n",
    "                scope_candidate.append((k, l))\n",
    "                scope_duration.append(l-k)\n",
    "    \n",
    "    if len(scope_duration) == 0:\n",
    "        return 0\n",
    "    \n",
    "    min_value = min(scope_duration)\n",
    "    min_scope = [scp for scp, value in zip(scope_candidate, scope_duration) if value == min_value]\n",
    "    print(min_scope)\n",
    "\n",
    "    return min_scope[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heaviside(x, left, right):\n",
    "    if x < left:\n",
    "        return 0\n",
    "    elif x > right:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def poly_approx(coef_table, x_list, type, center=0):\n",
    "    _coef = list(coef_table.loc[type])\n",
    "    \n",
    "    if center != 0:\n",
    "        x_list = [x - center for x in x_list]\n",
    "        \n",
    "    lower_bd = _coef[2]\n",
    "    upper_bd = _coef[3]\n",
    "    coef = _coef[5:]\n",
    "    p = np.poly1d(coef)\n",
    "    \n",
    "    return np.clip(p(x_list)*np.vectorize(heaviside)(x_list, lower_bd, upper_bd), a_min=0, a_max=100)\n",
    "\n",
    "def generate_random_boolean(probability_true):\n",
    "    return random.random() < probability_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Profile for a specific dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['/Users/jackbedford/Desktop/MOXA/Code/data/2023-09-12-2/UDP_Bandlock_9S_Phone_Brown/sm01/#01/data/handover_info_log.csv',\n",
       "  '/Users/jackbedford/Desktop/MOXA/Code/data/2023-09-12-2/UDP_Bandlock_9S_Phone_Brown/sm01/#01/data/udp_dnlk_loss_latency.csv',\n",
       "  '/Users/jackbedford/Desktop/MOXA/Code/data/2023-09-12-2/UDP_Bandlock_9S_Phone_Brown/sm01/#01/data/udp_uplk_loss_latency.csv']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_dir = \"/Users/jackbedford/Desktop/MOXA/Code/data/2023-03-26/UDP_Bandlock_All_RM500Q\"\n",
    "# devices = ['qc00', 'qc02', 'qc03']\n",
    "# trips = ['#01', '#02', '#03', '#04']\n",
    "\n",
    "# filepaths = []\n",
    "# for tr in trips:\n",
    "#     for dev in devices:\n",
    "#         filepaths.append([os.path.join(data_dir, dev, tr, 'data', 'handover_info_log.csv'), \\\n",
    "#             os.path.join(data_dir, dev, tr, 'data', 'udp_dnlk_loss_latency.csv'), \\\n",
    "#             os.path.join(data_dir, dev, tr, 'data', 'udp_uplk_loss_latency.csv'),])\n",
    "#     break\n",
    "\n",
    "# filepaths\n",
    "\n",
    "data_dir = \"/Users/jackbedford/Desktop/MOXA/Code/data/2023-09-12-2/UDP_Bandlock_9S_Phone_Brown\"\n",
    "devices = ['sm01',]\n",
    "trips = ['#01',]\n",
    "\n",
    "filepaths = []\n",
    "for tr in trips:\n",
    "    for dev in devices:\n",
    "        filepaths.append([os.path.join(data_dir, dev, tr, 'data', 'handover_info_log.csv'), \\\n",
    "            os.path.join(data_dir, dev, tr, 'data', 'udp_dnlk_loss_latency.csv'), \\\n",
    "            os.path.join(data_dir, dev, tr, 'data', 'udp_uplk_loss_latency.csv'),])\n",
    "    break\n",
    "\n",
    "filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>rpkg</th>\n",
       "      <th>frame_id</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>lost</th>\n",
       "      <th>excl</th>\n",
       "      <th>latency</th>\n",
       "      <th>xmit_time</th>\n",
       "      <th>arr_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2501</td>\n",
       "      <td>4</td>\n",
       "      <td>3509</td>\n",
       "      <td>2023-09-12 13:34:16.245625</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.006932</td>\n",
       "      <td>2023-09-12 13:34:16.245656</td>\n",
       "      <td>2023-09-12 13:34:16.252557443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2502</td>\n",
       "      <td>3</td>\n",
       "      <td>3513</td>\n",
       "      <td>2023-09-12 13:34:16.247625</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.010498</td>\n",
       "      <td>2023-09-12 13:34:16.247636</td>\n",
       "      <td>2023-09-12 13:34:16.258123443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2503</td>\n",
       "      <td>3</td>\n",
       "      <td>3513</td>\n",
       "      <td>2023-09-12 13:34:16.249625</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.008498</td>\n",
       "      <td>2023-09-12 13:34:16.249636</td>\n",
       "      <td>2023-09-12 13:34:16.258123443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2504</td>\n",
       "      <td>3</td>\n",
       "      <td>3513</td>\n",
       "      <td>2023-09-12 13:34:16.251626</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.006497</td>\n",
       "      <td>2023-09-12 13:34:16.251636</td>\n",
       "      <td>2023-09-12 13:34:16.258123443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2505</td>\n",
       "      <td>3</td>\n",
       "      <td>3517</td>\n",
       "      <td>2023-09-12 13:34:16.253626</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.010151</td>\n",
       "      <td>2023-09-12 13:34:16.253638</td>\n",
       "      <td>2023-09-12 13:34:16.263777443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417486</th>\n",
       "      <td>1419987</td>\n",
       "      <td>6</td>\n",
       "      <td>1865425</td>\n",
       "      <td>2023-09-12 14:21:31.350104</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.003973</td>\n",
       "      <td>2023-09-12 14:21:31.350112</td>\n",
       "      <td>2023-09-12 14:21:31.354077443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417487</th>\n",
       "      <td>1419988</td>\n",
       "      <td>6</td>\n",
       "      <td>1865425</td>\n",
       "      <td>2023-09-12 14:21:31.352104</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>2023-09-12 14:21:31.352112</td>\n",
       "      <td>2023-09-12 14:21:31.354077443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417488</th>\n",
       "      <td>1419989</td>\n",
       "      <td>6</td>\n",
       "      <td>1865425</td>\n",
       "      <td>2023-09-12 14:21:31.354104</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>2023-09-12 14:21:31.354112</td>\n",
       "      <td>2023-09-12 14:21:31.354077443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417489</th>\n",
       "      <td>1419990</td>\n",
       "      <td>6</td>\n",
       "      <td>1865425</td>\n",
       "      <td>2023-09-12 14:21:31.356104</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.002027</td>\n",
       "      <td>2023-09-12 14:21:31.356112</td>\n",
       "      <td>2023-09-12 14:21:31.354077443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417490</th>\n",
       "      <td>1419991</td>\n",
       "      <td>6</td>\n",
       "      <td>1865425</td>\n",
       "      <td>2023-09-12 14:21:31.358104</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.004027</td>\n",
       "      <td>2023-09-12 14:21:31.358113</td>\n",
       "      <td>2023-09-12 14:21:31.354077443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1417491 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             seq  rpkg  frame_id                  Timestamp   lost   excl  \\\n",
       "0           2501     4      3509 2023-09-12 13:34:16.245625  False  False   \n",
       "1           2502     3      3513 2023-09-12 13:34:16.247625  False  False   \n",
       "2           2503     3      3513 2023-09-12 13:34:16.249625  False  False   \n",
       "3           2504     3      3513 2023-09-12 13:34:16.251626  False  False   \n",
       "4           2505     3      3517 2023-09-12 13:34:16.253626  False  False   \n",
       "...          ...   ...       ...                        ...    ...    ...   \n",
       "1417486  1419987     6   1865425 2023-09-12 14:21:31.350104  False  False   \n",
       "1417487  1419988     6   1865425 2023-09-12 14:21:31.352104  False  False   \n",
       "1417488  1419989     6   1865425 2023-09-12 14:21:31.354104  False  False   \n",
       "1417489  1419990     6   1865425 2023-09-12 14:21:31.356104  False  False   \n",
       "1417490  1419991     6   1865425 2023-09-12 14:21:31.358104  False  False   \n",
       "\n",
       "          latency                  xmit_time                      arr_time  \n",
       "0        0.006932 2023-09-12 13:34:16.245656 2023-09-12 13:34:16.252557443  \n",
       "1        0.010498 2023-09-12 13:34:16.247636 2023-09-12 13:34:16.258123443  \n",
       "2        0.008498 2023-09-12 13:34:16.249636 2023-09-12 13:34:16.258123443  \n",
       "3        0.006497 2023-09-12 13:34:16.251636 2023-09-12 13:34:16.258123443  \n",
       "4        0.010151 2023-09-12 13:34:16.253638 2023-09-12 13:34:16.263777443  \n",
       "...           ...                        ...                           ...  \n",
       "1417486  0.003973 2023-09-12 14:21:31.350112 2023-09-12 14:21:31.354077443  \n",
       "1417487  0.001973 2023-09-12 14:21:31.352112 2023-09-12 14:21:31.354077443  \n",
       "1417488 -0.000027 2023-09-12 14:21:31.354112 2023-09-12 14:21:31.354077443  \n",
       "1417489 -0.002027 2023-09-12 14:21:31.356112 2023-09-12 14:21:31.354077443  \n",
       "1417490 -0.004027 2023-09-12 14:21:31.358113 2023-09-12 14:21:31.354077443  \n",
       "\n",
       "[1417491 rows x 9 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_filepath = filepaths[0][1]\n",
    "dl_filepath\n",
    "\n",
    "df = set_data(pd.read_csv(dl_filepath))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['xmit_time'].to_numpy()\n",
    "Xt = np.array([np.datetime_as_string(dt, unit='ms')[:-1] for dt in X + np.timedelta64(5, 'ms')], dtype='datetime64')\n",
    "df = df.assign(xmit_time_group_10ms=Xt)\n",
    "\n",
    "X = df['xmit_time'].to_numpy()\n",
    "Xt = np.array([np.datetime_as_string(dt, unit='ms')[:-2] for dt in X + np.timedelta64(50, 'ms')], dtype='datetime64')\n",
    "df = df.assign(xmit_time_group_100ms=Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>282257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Timestamp\n",
       "Timestamp           \n",
       "2                  1\n",
       "4                660\n",
       "5             282257\n",
       "6                594"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('xmit_time_group_10ms').agg({'Timestamp': 'size', 'lost': 'sum'}).groupby('Timestamp').agg({'Timestamp': 'size'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>27488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Timestamp\n",
       "Timestamp           \n",
       "3                  1\n",
       "5                  1\n",
       "49               465\n",
       "50             27488\n",
       "51               398"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('xmit_time_group_100ms').agg({'Timestamp': 'size', 'lost': 'sum'}).groupby('Timestamp').agg({'Timestamp': 'size'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Timestamp'].to_numpy()\n",
    "Xt = np.array([np.datetime_as_string(dt, unit='ms')[:-1] for dt in X + np.timedelta64(5, 'ms')], dtype='datetime64')\n",
    "df = df.assign(Timestamp_group_10ms=Xt)\n",
    "\n",
    "X = df['Timestamp'].to_numpy()\n",
    "Xt = np.array([np.datetime_as_string(dt, unit='ms')[:-2] for dt in X + np.timedelta64(50, 'ms')], dtype='datetime64')\n",
    "df = df.assign(Timestamp_group_100ms=Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>283419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Timestamp\n",
       "Timestamp           \n",
       "2                  1\n",
       "4                 79\n",
       "5             283419\n",
       "6                 13"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Timestamp_group_10ms').agg({'Timestamp': 'size', 'lost': 'sum'}).groupby('Timestamp').agg({'Timestamp': 'size'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>28268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Timestamp\n",
       "Timestamp           \n",
       "3                  1\n",
       "5                  1\n",
       "49                75\n",
       "50             28268\n",
       "51                 8"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Timestamp_group_100ms').agg({'Timestamp': 'size', 'lost': 'sum'}).groupby('Timestamp').agg({'Timestamp': 'size'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moxa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
